{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Run a Self-Hosted LLM on Your Server: Practical Guide [2025]\n\n**Description:** Ship a secure self-hosted LLM on Ubuntu. Size hardware, pick models, run vLLM, serve via FastAPI endpoints, choose adaptation confidently.\n\n**ðŸ“– Read the full article:** [How to Run a Self-Hosted LLM on Your Server: Practical Guide [2025]](https://blog.thegenairevolution.com/article/how-to-run-your-own-self-hosted-llm-on-a-server-a-practical-guide)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes you have perfectly good reasons to run language models on your own hardware. Maybe your data canâ€™t leave your network. Maybe youâ€™re tired of API caps. Or maybe you just want full control over how your model behaves.\n\nAs noted in the article on [9 principles for reliable, scalable AI agent systems](https://github.com/thegenairevolution/cookbooks/blob/bb45327c52f1266ff0bc3b170fdb17f18b7106da//article/ai-agent-design-9-principles-for-reliable-scalable-systems-5), â€œfor critical agentsâ€¦ you need three backups. Two cloud, one on\\-premises.â€ Iâ€™ve been running models locally for a while now, and the benefits are real. No rate limits, no surprise bills, and no wondering where your data is going.\n\nToday I want to show you exactly how to get **FLAN\\-T5** running on your own Ubuntu server. This is the same setup Iâ€™ve used in personal projects where data privacy was non\\-negotiable.\n\n![Uploaded image](/public-objects/user_insert_44830763_1764171049480.png \"Uploaded image\")Here's what we're going to build together:\n\n* Install PyTorch, Transformers, and NVIDIA drivers on Ubuntu\n* Load and run FLAN\\-T5\\-base for text generation\n* Measure latency and throughput for CPU and GPU\n* Improve outputs with zero\\-shot, one\\-shot, and few\\-shot prompting\n* Validate your setup with reproducible acceptance tests\n\n**Prerequisites**: You should be comfortable with Ubuntu, SSH, and Python virtual environments. You'll need access to an Ubuntu 22\\.04 server with 8 to 16 GB RAM. If you have an NVIDIA GPU with 4\\+ GB VRAM, even better.\n\n**Out of scope**: We won't cover API serving, fine\\-tuning, or production deployment. This guide focuses on getting a single, working inference pipeline up and running.\n\n## What We're Building\n\nA command\\-line inference script that accepts a prompt, tokenizes it, runs it through FLAN\\-T5\\-base, and returns the generated text. The script logs latency and token counts for performance monitoring.\n\nSystem flow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Prompt â†’ Tokenizer â†’ Model (CPU/GPU) â†’ Decode â†’ Output + Logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Deliverable**: A Python script (run\\_[flan.py](http://flan.py)) that runs inference on any prompt and exits with code 0 on success.\n\n**Success criteria**:\n\n* FLAN\\-T5\\-base generates 32 tokens in less than 2 seconds on g5\\.xlarge, less than 15 seconds on t3\\.xlarge\n* Script returns exit code 0 on 5 predefined prompts\n* Logs capture prompt length, output length, and latency\n\n## Choosing the Right Hardware\n\nFLAN\\-T5\\-base has 270M parameters. In fp32, the model needs about 1\\.08 GB of memory. In fp16, it needs about 0\\.54 GB. But here's the thing, you need to budget 2 to 3 times that for KV cache and activations. So plan for about 1\\.5 to 2 GB VRAM or RAM per concurrent sequence.\n\n**CPU baseline**:\n\n* AWS t3\\.large (2 vCPU, 8 GB RAM): About 10 to 15 seconds per 32\\-token generation\n* AWS t3\\.xlarge (4 vCPU, 16 GB RAM): About 5 to 10 seconds per 32\\-token generation\n\n**GPU acceleration**:\n\n* AWS g5\\.xlarge (1x A10G, 24 GB VRAM): Less than 2 seconds per 32\\-token generation\n* Supports fp16 and batch inference for higher throughput\n\nI'd recommend starting with CPU to validate the pipeline, then moving to GPU if latency becomes critical. Actually, when I first experimented with this setup on a personal project, I ran everything on CPU for weeks before realizing I needed the speed boost.\n\n## Setup \\& Installation\n\n### 1\\. Access Your Server\n\nSSH into your Ubuntu 22\\.04 server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ssh -i ~/.ssh/id_rsa ubuntu@SERVER_IP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2\\. Update System Packages\n\nUpdate Ubuntu and install build essentials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo apt update && sudo apt upgrade -y\nsudo apt install -y build-essential git curl wget ca-certificates\nsudo apt install -y python3 python3-venv python3-pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3\\. Create a Python Virtual Environment\n\nCreate and activate a virtual environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python3 -m venv ~/llm-venv\nsource ~/llm-venv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4\\. Upgrade pip and wheel\n\nEnsure clean installs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python -m pip install --upgrade pip wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5\\. Install PyTorch\n\nFor CPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For GPU (CUDA 12\\.1\\):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6\\. Install NVIDIA Drivers (GPU Only)\n\nIf you're using a GPU, install NVIDIA drivers and CUDA toolkit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo ubuntu-drivers autoinstall\nsudo apt install -y nvidia-cuda-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify installation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see GPU details and driver version.\n\n### 7\\. Install Hugging Face Transformers\n\nInstall Transformers and dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install transformers accelerate sentencepiece safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8\\. Install Jupyter Notebook (Optional)\n\nIf you prefer interactive development:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9\\. Pin Dependencies for Reproducibility\n\nCreate a requirements.txt with pinned versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch==2.4.0+cu121\ntransformers==4.44.2\naccelerate==0.34.2\nsentencepiece==0.2.0\nsafetensors==0.4.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install from the file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10\\. Verify Installation\n\nCheck PyTorch and CUDA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"CUDA device count:\", torch.cuda.device_count())\n    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validate tokenizer download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\nprint(\"Tokenizer loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stop here if these checks fail. You need to resolve installation issues before proceeding. Trust me on this one, I've wasted hours debugging model issues that were actually installation problems.\n\n### 11\\. Network Configuration\n\nSecure your server with ufw:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo ufw allow 22/tcp\nsudo ufw enable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're using Jupyter, bind to localhost only and access via SSH tunnel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jupyter notebook --no-browser --port=8888"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On your local machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ssh -L 8888:localhost:8888 ubuntu@SERVER_IP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Never bind Jupyter to 0\\.0\\.0\\.0 in production. Seriously, don't do it. I learned this the hard way in a previous role when someone found our unsecured notebook server.\n\n## How It Works: High\\-Level System Overview\n\nFLAN\\-T5 is a sequence\\-to\\-sequence model. It takes a text prompt, encodes it into token IDs, generates output token IDs, and decodes them back to text.\n\n**Key integration points**:\n\n* **Tokenizer**: Converts text to token IDs and back\n* **Model**: Runs inference on token IDs\n* **GenerationConfig**: Controls output length, sampling, and repetition\n* **Device placement**: Moves tensors to CPU or GPU\n\n**Why FLAN\\-T5\\-base?**\n\n* Instruction\\-tuned for zero\\-shot tasks (summarization, Q\\&A, translation)\n* 270M parameters fit on modest hardware\n* Apache\\-2\\.0 license allows commercial use\n\n## Downloading and Running Your First Model\n\nLet's start with a minimal inference pipeline. This script loads FLAN\\-T5\\-base, tokenizes a prompt, generates output, and decodes it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\nimport torch\n\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenize a prompt and inspect tensor shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Question. What is the capital of Italy?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\nprint(\"Input IDs shape:\", inputs[\"input_ids\"].shape)\nprint(\"Attention mask shape:\", inputs[\"attention_mask\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure generation parameters for controlled output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_cfg = GenerationConfig(\n    max_new_tokens=64,\n    temperature=0.7,\n    top_p=0.9,\n    num_beams=1,\n    repetition_penalty=1.05\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run inference and decode the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n    output_ids = model.generate(**inputs, generation_config=gen_cfg)\n    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    print(\"Model output:\", text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Design choices**:\n\n* **max\\_new\\_tokens\\=64** limits latency and cost\n* **temperature\\=0\\.7** balances diversity and coherence\n* **top\\_p\\=0\\.9** uses nucleus sampling for natural output\n* **repetition\\_penalty\\=1\\.05** reduces repetitive phrases\n\n## Understanding Model Behavior: Base vs Instruction\\-Tuned\n\nFLAN\\-T5\\-base is instruction\\-tuned. It follows explicit instructions like \"Summarize\" or \"Translate.\" Base models like T5\\-base aren't instruction\\-tuned and often fail on zero\\-shot tasks.\n\nCompare outputs on the same prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\ndef run(model_id: str, prompt: str) -> str:\n    tok = AutoTokenizer.from_pretrained(model_id)\n    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cpu\")\n    inputs = tok(prompt, return_tensors=\"pt\")\n    with torch.inference_mode():\n        out = mdl.generate(**inputs, max_new_tokens=32)\n    return tok.decode(out[0], skip_special_tokens=True)\n\nprompt = \"Instruction. List three benefits of version control in software projects.\"\nprint(\"t5-base:\", run(\"t5-base\", prompt))\nprint(\"flan-t5-base:\", run(\"google/flan-t5-base\", prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output**:\n\n* **t5\\-base**: Gibberish or incomplete fragments\n* **flan\\-t5\\-base**: \"1\\. Track changes 2\\. Collaborate 3\\. Revert errors\"\n\nInstruction\\-tuned models save you from fine\\-tuning for common tasks. This is a huge time\\-saver. When I was working on a text summarization project last year, switching from T5 to FLAN\\-T5 literally saved me weeks of fine\\-tuning work.\n\n## Improving Outputs with In\\-Context Learning\n\nYou'll run a prompt through a fast inference pipeline, then iterate on prompt quality with zero\\-shot, one\\-shot, and few\\-shot patterns. For a deeper dive into how in\\-context learning can dramatically improve your model's accuracy and flexibility, check out our [in\\-context learning guide](https://github.com/thegenairevolution/cookbooks/blob/bb45327c52f1266ff0bc3b170fdb17f18b7106da//article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3).\n\n### Zero\\-Shot Prompting\n\nProvide a task with no examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Instruction. Summarize the following review in one sentence. Review. The coffee was strong, the staff friendly, but the place was a bit noisy.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output: \"Strong coffee, friendly staff, noisy environment.\"\n\nToken count: About 30 input, about 10 output\n\n### One\\-Shot Prompting\n\nProvide one example to teach the model a format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "one_shot = \"\"\"Task. Write a JSON object with keys title and summary.\nInput. 'Deploy a Python app on Ubuntu.'\nOutput. {\"title\": \"Deploy a Python App\", \"summary\": \"Install Python, set up a venv, configure service, then monitor.\"}\nInput. 'Set up a Redis cache for Django.'\nOutput.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output: {\"title\": \"Set Up Redis Cache\", \"summary\": \"Install Redis, configure Django, test cache.\"}\n\nToken count: About 60 input, about 20 output\n\n### Few\\-Shot Prompting\n\nProvide multiple examples to reinforce patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "few_shot = \"\"\"Task. Convert a sentence to a short title.\nInput. 'Troubleshoot slow PostgreSQL queries.'\nOutput. 'Fix Slow PostgreSQL Queries'\nInput. 'Implement OAuth2 login with FastAPI.'\nOutput. 'FastAPI OAuth2 Login'\nInput. 'Harden Ubuntu SSH for production.'\nOutput. 'Harden SSH on Ubuntu'\nInput. 'Automate backups with S3 lifecycle rules.'\nOutput. 'Automate S3 Backups'\nInput. 'Audit API calls with structured logs.'\nOutput.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output: \"Audit API Calls\"\n\nToken count: About 120 input, about 5 output\n\n**Trade\\-offs**:\n\n* **Zero\\-shot**: Fast, low token cost, less control\n* **One\\-shot**: Moderate cost, teaches format\n* **Few\\-shot**: High token cost, strongest control\n\nActually, let me share something interesting. In a personal project where I was categorizing support tickets, I found that two well\\-chosen examples worked better than five mediocre ones. Quality beats quantity every time.\n\nIf you want to refine prompts further and boost reliability, explore these [prompt engineering techniques for reliable LLM outputs](https://github.com/thegenairevolution/cookbooks/blob/bb45327c52f1266ff0bc3b170fdb17f18b7106da//article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4). If outputs still fall short, evaluate fine\\-tuning or parameter\\-efficient fine\\-tuning. Choose the least invasive method that meets your quality and cost goals. For a [step\\-by\\-step walkthrough of full fine\\-tuning with Hugging Face](https://github.com/thegenairevolution/cookbooks/blob/bb45327c52f1266ff0bc3b170fdb17f18b7106da//article/fine-tuning-large-language-models-a-step-by-step-guide-2025-5), follow our detailed guide.\n\n## Run and Validate Your Self\\-Hosted LLM\n\n### End\\-to\\-End CLI Script\n\nThis script runs inference from the command line and logs performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndef main():\n    model_id = \"google/flan-t5-base\"\n    tok = AutoTokenizer.from_pretrained(model_id)\n    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    inp = \" \".join(sys.argv[1:]) or \"Question. What is the capital of Italy?\"\n    inputs = tok(inp, return_tensors=\"pt\").to(mdl.device)\n    \n    with torch.inference_mode():\n        out = mdl.generate(**inputs, max_new_tokens=64)\n    \n    print(tok.decode(out[0], skip_special_tokens=True))\n\nif __name__ == \"__main__\":\n    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save as run\\_flan.py and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python run_flan.py \"Question. What is the capital of France?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Measure Latency\n\nTrack inference time for performance monitoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\nt0 = time.time()\nwith torch.inference_mode():\n    output_ids = model.generate(**inputs, max_new_tokens=64)\nlatency = time.time() - t0\nprint(\"Latency (seconds):\", latency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Inference\n\nProcess multiple prompts in one pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = [\n    \"Question. Who wrote The Pragmatic Programmer?\",\n    \"Instruction. Translate to French. How are you today?\",\n    \"Instruction. Summarize. Kubernetes manages containers across nodes.\"\n]\n\nenc = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\nwith torch.inference_mode():\n    out = model.generate(**enc, max_new_tokens=64)\n\nfor i, o in enumerate(out):\n    print(f\"Prompt {i} output:\", tokenizer.decode(o, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enable Logging\n\nLog prompt and output lengths for debugging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nfrom transformers.utils import logging as hf_logging\n\nlogging.basicConfig(level=logging.INFO)\nhf_logging.set_verbosity_info()\n\nlogging.info(f\"Prompt length: {inputs['input_ids'].shape[1]}\")\nlogging.info(f\"Output length: {output_ids.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Security note: Redact sensitive data in logs. Use log rotation (logrotate) and set retention policies. I once had a script that logged customer prompts to debug an issue. Bad idea. Really bad idea. Always sanitize your logs.\n\n### Acceptance Tests\n\nValidate your setup with 5 canonical prompts:\n\n1. \"Question. What is the capital of Italy?\" should return \"Rome\"\n2. \"Instruction. Translate to Spanish. Hello.\" should return \"Hola\"\n3. \"Instruction. Summarize. AI is transforming industries.\" should return \"AI transforms industries\"\n4. \"Task. List two benefits of Docker.\" should return \"1\\. Portability 2\\. Isolation\"\n5. \"Question. Who invented Python?\" should return \"Guido van Rossum\"\n\nRun each prompt and verify:\n\n* Exit code 0\n* Output matches expected pattern\n* Latency within thresholds (CPU: less than 15s, GPU: less than 2s)\n\n## Scaling Considerations\n\n### GPU Optimization\n\nUse fp16 for faster inference and lower memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enable TF32 on Ampere GPUs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Warning:** Don't use fp16 on CPU. It will slow down inference. Found this out the hard way when I tried to optimize a CPU deployment and made it 3x slower.\n\n### Batch Processing\n\nIncrease throughput by processing multiple prompts at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = tokenizer(\n    prompts,\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True,\n    max_length=512\n).to(device)\n\nwith torch.inference_mode():\n    outputs = model.generate(**batch, max_new_tokens=64)\n\ndecoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(\"Batch outputs:\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Trade\\-off:** Larger batches increase throughput but also increase latency per prompt.\n\nIf you plan to scale beyond prompts and need retrieval to ground generations, consider [implementing vector store retrieval for RAG systems](https://github.com/thegenairevolution/cookbooks/blob/bb45327c52f1266ff0bc3b170fdb17f18b7106da//article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it). This helps reduce hallucinations and improves factual accuracy at scale.\n\n### Offline and Air\\-Gapped Environments\n\nDownload models once and cache them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export HF_HOME=/path/to/cache\nexport TRANSFORMERS_CACHE=/path/to/cache\nexport HF_HUB_OFFLINE=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For proxy environments, set HTTP\\_PROXY and HTTPS\\_PROXY before downloading models.\n\n## Advanced Topics\n\n### Deterministic Inference\n\nSet seeds for reproducible outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Limitation: GPU inference isn't fully deterministic. Use torch.use\\_deterministic\\_algorithms(True) for stricter control, but expect slower performance.\n\n### Parameter\\-Efficient Fine\\-Tuning\n\nIf you need consistent schemas, domain\\-specific style, or complex transformations, in\\-context learning might not be enough. Actually, let me put it this way. When you hit the limits of prompting, evaluate parameter\\-efficient fine\\-tuning like LoRA using PEFT. Check out [our hands\\-on PEFT and LoRA guide](https://github.com/thegenairevolution/cookbooks/blob/bb45327c52f1266ff0bc3b170fdb17f18b7106da//article/parameter-efficient-fine-tuning-peft-with-lora-2025-hands-on-guide-2). This reduces training cost and memory while giving strong control over output behavior.\n\n### License and Compliance\n\nFLAN\\-T5\\-base is licensed under Apache\\-2\\.0, which allows commercial use. Review the model card on Hugging Face for restrictions. For production deployments, confirm internal approval and audit requirements.\n\n## Conclusion and Next Steps\n\nSo there you have it. You've deployed FLAN\\-T5\\-base on Ubuntu, run inference, and validated outputs. You learned to measure latency, batch prompts, and improve quality with in\\-context learning.\n\n**Key architectural decisions:**\n\n* Seq2seq model for instruction\\-following tasks\n* CPU\\-first baseline for cost control\n* Instruction\\-tuned model to avoid fine\\-tuning\n\n**Next steps:**\n\n* Target 5x latency reduction with GPU and fp16\n* Wrap the script with FastAPI and enforce JWT auth\n* Explore [building reliable LLM workflows with LangChain](https://github.com/thegenairevolution/cookbooks/blob/bb45327c52f1266ff0bc3b170fdb17f18b7106da//article/langchain-101-build-your-first-real-llm-application-step-by-step)\n* Implement log rotation and PII redaction for production\n* Evaluate LoRA fine\\-tuning for domain\\-specific tasks\n\nYou now have a working, self\\-hosted LLM pipeline. And honestly, that's no small achievement. The first time I got a model running locally, generating coherent text on my own hardware, it felt like magic. Now it's your turn to scale it, secure it, and deploy it with confidence."
      ]
    }
  ],
  "metadata": {
    "title": "How to Run a Self-Hosted LLM on Your Server: Practical Guide [2025]",
    "description": "Ship a secure self-hosted LLM on Ubuntu. Size hardware, pick models, run vLLM, serve via FastAPI endpoints, choose adaptation confidently.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}