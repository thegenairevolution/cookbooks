{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** LangGraph Agent: How to Build a Deterministic Plan-Execute with Memory\n\n**Description:** Build a production-ready LangGraph agent that plans, executes, validates tools, persists state, remembers context, and serves a deterministic JSON /agent.\n\n**ðŸ“– Read the full article:** [LangGraph Agent: How to Build a Deterministic Plan-Execute with Memory](https://blog.thegenairevolution.com/article/langgraph-agent-how-to-build-a-deterministic-plan-execute-with-memory)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you separate planning from execution, you get to lock down an explicit sequence of steps before any tool actually fires. This cuts down on all that unpredictable back\\-and\\-forth you usually see with LLMs. What you'll do is ask the model to output a structured, multi\\-step plan first. Then your executor runs tools deterministically against that plan, complete with validation and guardrails. The result? Behavior you can actually repeat. If you want to dig deeper into how prompt structure and where you place information affects model performance, check out our analysis of [position bias in long prompts](/article/lost-in-the-middle-placing-critical-info-in-long-prompts).\n\nHere's what this pattern gives you:\n\n* **Determinism.** Your plan is fixed before execution starts, so you can log it, audit it, and replay it whenever you need.\n* **Safety.** Every single tool call gets validated with Pydantic schemas both before and after execution.\n* **Recovery.** When a step fails (and they will), the agent can replan and keep going instead of just crashing.\n* **Memory.** LangGraph checkpointers persist state across turns, which makes multi\\-turn workflows actually possible.\n\n\nYou're going to build a FastAPI `/agent` endpoint backed by a LangGraph state graph that plans, executes, and replans on error when needed. What you end up with is a working, production\\-ready agent that you can extend with new tools, memory backends, and observability features. If you're just getting started with LangChain or want something hands\\-on, our guide to [building reliable LangChain LLM workflows](/article/langchain-101-build-your-first-real-llm-application-step-by-step) walks through the whole setup, from prompt\\-driven chains to structured outputs that actually work in production.\n\n## How It Works\n\nLet me walk you through the high\\-level flow:\n\n1. User sends `thread_id` and query to `/agent`\n2. Planner node invokes the LLM with structured output to generate a PlanModel (basically a list of steps)\n3. Executor node runs each step:\n    * For tool steps: validate input, call the tool, validate output, store result\n    * For respond step: synthesize final answer from step results using the LLM\n4. On error: route to Replan node, which generates a revised plan and re\\-enters execution\n5. Checkpointer persists state per `thread_id` for conversation memory\n6. API returns plan, step results, final answer, error (if any), and trace\n\n\nThis architecture keeps planning and execution separate. That's what makes the system auditable, testable, and honestly, pretty easy to extend.\n\n## Setup \\& Installation\n\nYou can run this in a Colab notebook or any local Python 3\\.10\\+ environment. First, install your dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langgraph langchain-openai pydantic httpx fastapi uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key. If you're in Colab, store it in Secrets as `OPENAI_API_KEY`. For a local environment, just export it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\ntry:\n    from google.colab import userdata\n    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\nexcept ImportError:\n    pass  # Not in Colab; ensure OPENAI_API_KEY is set in your shell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick verification that everything's working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert \"OPENAI_API_KEY\" in os.environ, \"Set OPENAI_API_KEY in environment or Colab Secrets\"\nprint(\"âœ“ Environment ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Define the agent's shared state and plan models\n\nWe're using TypedDict for the state and Pydantic for structured plan output. This gives us type safety and validation at every single step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any, Optional\nfrom pydantic import BaseModel, Field\n\nclass PlanStep(BaseModel):\n    \"\"\"\n    Represents a single step in the agent's plan.\n    \"\"\"\n    id: int = Field(..., description=\"Step index starting at 1\")\n    action: str = Field(..., description=\"Either 'tool' or 'respond'\")\n    name: Optional[str] = Field(None, description=\"Tool name if action is 'tool'\")\n    args: Optional[Dict[str, Any]] = Field(None, description=\"Arguments for the tool\")\n    description: str = Field(..., description=\"Short description of the step\")\n\nclass PlanModel(BaseModel):\n    \"\"\"\n    Represents the overall plan, including rationale and steps.\n    \"\"\"\n    rationale: str\n    steps: List[PlanStep]\n\nclass AgentState(TypedDict, total=False):\n    \"\"\"\n    Shared state for the agent, passed between nodes.\n    \"\"\"\n    user_input: str\n    plan: List[PlanStep]\n    step_results: List[Dict[str, Any]]\n    final_answer: Optional[str]\n    error: Optional[str]\n    trace: List[Dict[str, Any]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define safe tools with explicit schemas\n\nEach tool gets Pydantic input/output models for validation. This is crucial. It prevents malformed data from propagating through your system. If your agents need robust retrieval capabilities, take a look at our comprehensive guide on [implementing vector store retrieval for RAG systems](/article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it). It covers semantic search, chunking, and how to reduce those pesky hallucinations in LLM outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\nimport httpx\nimport math\n\nclass SumInput(BaseModel):\n    numbers: list[float] = Field(..., min_items=1)\n\nclass SumOutput(BaseModel):\n    total: float\n\ndef sum_numbers_tool(inp: SumInput) -> SumOutput:\n    \"\"\"Sums a list of numbers.\"\"\"\n    total = float(math.fsum(inp.numbers))\n    return SumOutput(total=total)\n\nclass KBQueryInput(BaseModel):\n    topic: str = Field(..., min_length=1)\n\nclass KBQueryOutput(BaseModel):\n    topic: str\n    content: str\n\nKB = {\n    \"refund_policy\": \"Refunds available within 30 days with receipt.\",\n    \"sla\": \"Standard support SLA is 24 hours response time.\",\n}\n\ndef kb_retrieve_tool(inp: KBQueryInput) -> KBQueryOutput:\n    \"\"\"Retrieves a KB article by topic.\"\"\"\n    topic = inp.topic.strip().lower()\n    if topic not in KB:\n        raise ValueError(f\"Topic '{topic}' not found\")\n    return KBQueryOutput(topic=topic, content=KB[topic])\n\nclass HttpGetInput(BaseModel):\n    url: str = Field(..., pattern=r\"^https://httpbin.org/.*\")\n\nclass HttpGetOutput(BaseModel):\n    status_code: int\n    json: dict\n\ndef http_get_json_tool(inp: HttpGetInput) -> HttpGetOutput:\n    \"\"\"Fetches JSON from a safe endpoint.\"\"\"\n    with httpx.Client(timeout=10.0) as client:\n        resp = client.get(inp.url)\n        data = resp.json() if \"application/json\" in resp.headers.get(\"content-type\", \"\") else {}\n        return HttpGetOutput(status_code=resp.status_code, json=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrap tools in a registry with validation\n\nThe registry validates inputs and outputs, catching errors before they can spread. This is absolutely critical for determinism and safety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable, Type, Any\nfrom pydantic import ValidationError\n\nclass ToolError(Exception):\n    \"\"\"Custom exception for tool validation or execution errors.\"\"\"\n    pass\n\nclass Tool:\n    \"\"\"\n    Registry entry for a tool, including validation and execution.\n    \"\"\"\n    def __init__(self, name: str, description: str, input_model: Type[BaseModel], output_model: Type[BaseModel], fn: Callable[[Any], Any]):\n        self.name = name\n        self.description = description\n        self.input_model = input_model\n        self.output_model = output_model\n        self.fn = fn\n\n    def run(self, args: dict) -> dict:\n        \"\"\"Validates input, runs the tool, and validates output.\"\"\"\n        try:\n            validated_in = self.input_model(**args)\n        except ValidationError as ve:\n            raise ToolError(f\"Input validation failed for {self.name}: {ve}\") from ve\n        try:\n            raw_out = self.fn(validated_in)\n        except Exception as e:\n            raise ToolError(f\"Tool {self.name} execution failed: {e}\") from e\n        try:\n            validated_out = self.output_model.model_validate(raw_out)\n        except ValidationError as ve:\n            raise ToolError(f\"Output validation failed for {self.name}: {ve}\") from ve\n        return validated_out.model_dump()\n\nTOOL_REGISTRY: dict[str, Tool] = {\n    \"sum_numbers\": Tool(\n        name=\"sum_numbers\",\n        description=\"Return the sum of an array of numbers\",\n        input_model=SumInput,\n        output_model=SumOutput,\n        fn=sum_numbers_tool,\n    ),\n    \"kb_retrieve\": Tool(\n        name=\"kb_retrieve\",\n        description=\"Retrieve a short KB article by topic\",\n        input_model=KBQueryInput,\n        output_model=KBQueryOutput,\n        fn=kb_retrieve_tool,\n    ),\n    \"http_get_json\": Tool(\n        name=\"http_get_json\",\n        description=\"GET JSON from https://httpbin.org endpoints only\",\n        input_model=HttpGetInput,\n        output_model=HttpGetOutput,\n        fn=http_get_json_tool,\n    ),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the LLM with structured output\n\nWe use LangChain's OpenAI wrapper with temperature\\=0 for deterministic planning. If you want to further improve the reliability of your agent's responses, our article on [prompt engineering strategies for reliable LLM outputs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4) goes deep into step\\-by\\-step prompt design and output formatting. The `with_structured_output` method ensures the LLM returns a valid PlanModel. And if you're still deciding which language model to use for your agent, our guide on [how to pick an LLM for your application](/article/how-to-choose-an-ai-model-for-your-app-speed-cost-reliability) breaks down all the tradeoffs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\nplanner_llm = llm.with_structured_output(PlanModel)\n\nPLAN_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a planning assistant. You must output a feasible, minimal plan.\"),\n    (\"system\", \"Available tools:\\n{tool_summaries}\\nOnly call tools listed above.\"),\n    (\"user\", \"User request: {user_input}\\nProduce a plan with one or more steps. Use 'respond' as the last step.\"),\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement the planner node\n\nThe planner generates a structured plan from the user's input. It validates that all tool steps actually refer to known tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tool_summaries() -> str:\n    \"\"\"Returns a summary of available tools and their input schemas.\"\"\"\n    lines = []\n    for t in TOOL_REGISTRY.values():\n        lines.append(f\"- {t.name}: {t.description}; input={t.input_model.model_json_schema()['properties']}\")\n    return \"\\n\".join(lines)\n\ndef plan_node(state: AgentState) -> AgentState:\n    \"\"\"Planner node: generates a plan from user input using the LLM.\"\"\"\n    ui = state[\"user_input\"]\n    result = planner_llm.invoke(PLAN_PROMPT.format_messages(\n        tool_summaries=tool_summaries(),\n        user_input=ui,\n    ))\n    plan: PlanModel = result\n    steps = []\n    for s in plan.steps:\n        if s.action == \"tool\" and (not s.name or s.name not in TOOL_REGISTRY):\n            raise ValueError(f\"Planner proposed unknown tool: {s.name}\")\n        steps.append(s)\n    return {\n        \"plan\": steps,\n        \"trace\": (state.get(\"trace\") or []) + [{\"event\": \"plan\", \"plan\": [s.model_dump() for s in steps]}],\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement the executor node\n\nThe executor runs each step in the plan. Tool steps get validated and executed via the registry. The final respond step synthesizes an answer from all the step results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n\nANSWER_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a precise assistant. Use the provided step results to answer.\"),\n    (\"user\", \"Question: {user_input}\\nStep results:\\n{step_results}\\nCompose a concise answer.\"),\n])\nanswer_llm = llm\n\ndef execute_node(state: AgentState) -> AgentState:\n    \"\"\"Executor node: runs the plan step by step, validates tool calls, and builds the final answer.\"\"\"\n    plan = state.get(\"plan\") or []\n    step_results = state.get(\"step_results\") or []\n    trace = state.get(\"trace\") or []\n    error = None\n    for step in plan:\n        if step.action == \"tool\":\n            if step.name not in TOOL_REGISTRY:\n                error = f\"Unknown tool {step.name}\"\n                trace.append({\"event\": \"tool_error\", \"step_id\": step.id, \"error\": error})\n                break\n            tool = TOOL_REGISTRY[step.name]\n            try:\n                result = tool.run(step.args or {})\n                step_results.append({\"step_id\": step.id, \"tool\": step.name, \"args\": step.args, \"output\": result})\n                trace.append({\"event\": \"tool_ok\", \"step_id\": step.id, \"tool\": step.name, \"output\": result})\n            except ToolError as te:\n                error = str(te)\n                trace.append({\"event\": \"tool_error\", \"step_id\": step.id, \"tool\": step.name, \"error\": error})\n                break\n        elif step.action == \"respond\":\n            sr_str = \"\\n\".join([f\"- Step {r['step_id']} ({r['tool']}): {r['output']}\" for r in step_results])\n            msg = ANSWER_PROMPT.format_messages(user_input=state[\"user_input\"], step_results=sr_str)\n            final = answer_llm.invoke(msg).content\n            trace.append({\"event\": \"respond\", \"text\": final})\n            return {\"step_results\": step_results, \"final_answer\": final, \"trace\": trace}\n        else:\n            error = f\"Unknown action {step.action}\"\n            trace.append({\"event\": \"plan_error\", \"error\": error})\n            break\n    if error:\n        return {\"step_results\": step_results, \"error\": error, \"trace\": trace}\n    return {\"step_results\": step_results, \"error\": \"Plan missing 'respond' step\", \"trace\": trace}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add a replan node for recovery\n\nWhen execution fails, the replan node generates a revised plan based on what's been completed and what went wrong. This is how you get graceful recovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPLAN_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a repair planner. Create a minimal revised plan to complete the task.\"),\n    (\"system\", \"Available tools:\\n{tool_summaries}\"),\n    (\"user\", \"Original request: {user_input}\\nCompleted steps:\\n{done}\\nError: {error}\\nPropose a revised plan (include 'respond' as last step).\"),\n])\nreplanner_llm = llm.with_structured_output(PlanModel)\n\ndef replan_node(state: AgentState) -> AgentState:\n    \"\"\"Replanner node: generates a revised plan after an error.\"\"\"\n    done_lines = []\n    for r in state.get(\"step_results\") or []:\n        done_lines.append(f\"Step {r['step_id']} {r['tool']} -> OK\")\n    msgs = REPLAN_PROMPT.format_messages(\n        tool_summaries=tool_summaries(),\n        user_input=state[\"user_input\"],\n        done=\"\\n\".join(done_lines) or \"None\",\n        error=state.get(\"error\") or \"Unknown error\",\n    )\n    revised: PlanModel = replanner_llm.invoke(msgs)\n    steps = []\n    for s in revised.steps:\n        if s.action == \"tool\" and (not s.name or s.name not in TOOL_REGISTRY):\n            raise ValueError(f\"Replanner proposed unknown tool: {s.name}\")\n        steps.append(s)\n    trace = (state.get(\"trace\") or []) + [{\"event\": \"replan\", \"plan\": [s.model_dump() for s in steps]}]\n    return {\"plan\": steps, \"error\": None, \"trace\": trace}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wire the graph with conditional edges\n\nLangGraph's StateGraph connects all the nodes. After execution, we route to replan on error or end on success. Pretty straightforward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"plan\", plan_node)\ngraph.add_node(\"execute\", execute_node)\ngraph.add_node(\"replan\", replan_node)\n\ngraph.add_edge(\"plan\", \"execute\")\n\ndef route_after_execute(state: AgentState) -> str:\n    \"\"\"Determines the next node after execution.\"\"\"\n    return \"replan\" if state.get(\"error\") else END\n\ngraph.add_conditional_edges(\"execute\", route_after_execute, {\"replan\": \"replan\", END: END})\ngraph.add_edge(\"replan\", \"execute\")\n\ngraph.set_entry_point(\"plan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Persist memory with checkpointers\n\nLangGraph checkpointers persist state across turns per thread. This is what gives you conversation memory or multi\\-call workflows. For quick starts, the in\\-memory saver works fine. For production, you'll want Postgres. Actually, if you're curious about why LLM memory isn't infinite and how to manage all that accumulated context, our guide on [context rot and LLM memory limitations](/article/context-rot-why-llms-forget-as-their-memory-grows-3) has some practical strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\napp_graph = graph.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize your graph\n\nLangGraph has built\\-in utilities for visualizing and inspecting your graph. Super helpful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n\ndisplay(Image(app_graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Serve a production\\-friendly JSON API\n\nFastAPI gives you a lightweight, typed API for the agent. The `/agent` endpoint accepts a `thread_id` and query, invokes the graph, and returns the full state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import Optional\n\napi = FastAPI(title=\"Plan-Execute Agent\")\n\nclass AgentRequest(BaseModel):\n    thread_id: str\n    query: str\n\nclass AgentResponse(BaseModel):\n    thread_id: str\n    plan: list[dict]\n    step_results: list[dict]\n    final_answer: Optional[str]\n    error: Optional[str]\n    trace: list[dict]\n\n@api.post(\"/agent\", response_model=AgentResponse)\ndef agent_endpoint(req: AgentRequest):\n    \"\"\"FastAPI endpoint for agent queries.\"\"\"\n    state = app_graph.invoke({\"user_input\": req.query}, config={\"configurable\": {\"thread_id\": req.thread_id}})\n    plan = [s.model_dump() if hasattr(s, \"model_dump\") else s for s in state.get(\"plan\", [])]\n    return AgentResponse(\n        thread_id=req.thread_id,\n        plan=plan,\n        step_results=state.get(\"step_results\", []),\n        final_answer=state.get(\"final_answer\"),\n        error=state.get(\"error\"),\n        trace=state.get(\"trace\", []),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\n### Test the graph directly\n\nLet's invoke the graph with a sample query to verify planning and execution work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = app_graph.invoke(\n    {\"user_input\": \"What is 10 + 20 + 30?\"},\n    config={\"configurable\": {\"thread_id\": \"test-thread-1\"}}\n)\nprint(\"Plan:\", result.get(\"plan\"))\nprint(\"Final Answer:\", result.get(\"final_answer\"))\nprint(\"Trace:\", result.get(\"trace\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output:** The planner generates a plan with a `sum_numbers` tool step and a respond step. The executor runs the tool and synthesizes the answer.\n\n### Test error handling and replanning\n\nNow let's trigger an error by requesting a non\\-existent KB topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = app_graph.invoke(\n    {\"user_input\": \"What is the warranty policy?\"},\n    config={\"configurable\": {\"thread_id\": \"test-thread-2\"}}\n)\nprint(\"Error:\", result.get(\"error\"))\nprint(\"Trace:\", result.get(\"trace\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output:** The executor fails on the `kb_retrieve` step, routes to replan, and generates a revised plan. Or it returns a partial answer if replanning also fails.\n\n### Run the FastAPI server\n\nStart the server in a notebook or local environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uvicorn\nimport nest_asyncio\n\nnest_asyncio.apply()  # Required for running uvicorn in Jupyter/Colab\nuvicorn.run(api, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a separate terminal or notebook cell, test the endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n\nresponse = requests.post(\"http://localhost:8000/agent\", json={\n    \"thread_id\": \"user-123\",\n    \"query\": \"What is 5 + 10?\"\n})\nprint(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output:** A JSON response with plan, step\\_results, final\\_answer, and trace.\n\n## Conclusion\n\nSo you've built a deterministic, plan\\-execute agent with LangGraph, Pydantic\\-validated tools, and a FastAPI endpoint. The system plans before it acts, validates every tool call, and recovers from errors through replanning. Memory persists across turns using checkpointers, which enables multi\\-turn workflows.\n\nKey decisions we made:\n\n* **LangGraph** for deterministic routing and state management\n* **Pydantic** for strict input/output validation\n* **FastAPI** for a lightweight, typed API\n* **Temperature\\=0** for reproducible planning\n\n\nNext steps to consider:\n\n* Swap MemorySaver for PostgresSaver for production persistence\n* Add retries with exponential backoff for transient tool failures\n* Extend the tool registry with new tools. Think database queries, external APIs. For advanced use cases, consider [standardizing and reusing AI tools across applications](/article/how-to-build-a-model-context-protocol-mcp-server-in-python-2) by building an MCP server. It's a great way to enable scalable and maintainable AI infrastructure.\n* Add observability with structured logging or tracing. LangSmith is a good option here.\n* Harden prompts with explicit constraints and few\\-shot examples\n\n\nIf you're planning to adapt your agent to specialized tasks, our guide to [parameter\\-efficient fine\\-tuning techniques like LoRA](/article/parameter-efficient-fine-tuning-peft-with-lora-2025-hands-on-guide-2) shows how to customize LLMs efficiently for production.\n\nThis architecture scales from prototypes to production. Start with the core build, validate it end\\-to\\-end, then layer in production features as you need them."
      ]
    }
  ],
  "metadata": {
    "title": "LangGraph Agent: How to Build a Deterministic Plan-Execute with Memory",
    "description": "Build a production-ready LangGraph agent that plans, executes, validates tools, persists state, remembers context, and serves a deterministic JSON /agent.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}