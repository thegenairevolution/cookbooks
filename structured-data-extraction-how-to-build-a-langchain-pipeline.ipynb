{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìì The GenAI Revolution Cookbook\n\n**Title:** Structured Data Extraction: How to Build a LangChain Pipeline\n\n**Description:** Build an end-to-end LangChain + OpenAI pipeline for structured data extraction from long documents‚Äîschema validation, batching, retries, and production-ready code.\n\n**üìñ Read the full article:** [Structured Data Extraction: How to Build a LangChain Pipeline](https://blog.thegenairevolution.com/article/structured-data-extraction-how-to-build-a-langchain-pipeline)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Structured data extraction from invoices is one of those tasks that sounds simple until you actually try to build it. JSON formatting breaks, token limits hit you in the face, fields mysteriously disappear, and before you know it, you're burning through API credits like there's no tomorrow. I've been down this road more times than I care to admit, and this tutorial walks you through building a production\\-ready invoice extraction pipeline that actually handles these challenges using LangChain, OpenAI's structured output, and Pydantic validation.\n\nBy the time we're done, you'll have a complete system that extracts invoice fields reliably at scale, tracks where every piece of data came from, and gives you clear cost estimates so there are no surprises.\n\n## Why This Approach Actually Works\n\nHere's the thing about invoice extraction \\- it's never just one LLM call and you're done. Real invoices come in every format imaginable, from pristine digital documents to barely\\-legible scans that look like they went through a fax machine from 1987\\. A robust pipeline needs to:\n\n* **Enforce schema compliance** ‚Äì Pydantic models validate your output structure and types, catching malformed JSON before it breaks something downstream.\n* **Handle long documents** ‚Äì Chunking with overlap makes sure you don't lose that one line item that happens to fall right at a chunk boundary. Aggregation brings it all back together.\n* **Retry transient failures** ‚Äì Rate limits and timeouts are just part of life with APIs. Exponential backoff with tenacity keeps things running when the inevitable hiccup occurs.\n* **Scale with async batching** ‚Äì Concurrent requests with semaphore\\-based rate limiting let you maximize throughput without getting yourself rate\\-limited into oblivion.\n* **Cache results** ‚Äì Content\\-based caching skips redundant extractions. Why pay twice for the same invoice?\n* **Track cost and usage** ‚Äì Token counting and cost estimation help you figure out if you're using the right model or if your prompts are getting out of hand.\n\nThis design balances accuracy, speed, and cost \\- the three things that actually matter when you're processing hundreds or thousands of invoices in production.\n\n## Why LangChain with Structured Output?\n\nLangChain's `with_structured_output` wraps OpenAI's JSON schema enforcement with Pydantic validation, which gives you:\n\n* **Type safety** ‚Äì Pydantic models catch type mismatches and missing fields at runtime, not three weeks later when accounting notices something's wrong.\n* **Schema evolution** ‚Äì Need to add a new field? Just update your Pydantic model. No need to rewrite all your prompt logic.\n* **Ergonomics** ‚Äì Way cleaner than manually parsing JSON and validating every single field yourself.\n\nNow, there's a trade\\-off here. LangChain adds another dependency to your stack. If you're trying to keep things minimal or need maximum control over every API call, OpenAI's SDK with `response_format={\"type\": \"json_schema\"}` is lighter. But honestly, for this tutorial, LangChain's validation and retry integrations more than justify the extra dependency.\n\nQuick note on scope: This pipeline assumes you're starting with text. Most invoices in the real world are PDFs or scans, so you'll need OCR or PDF\\-to\\-text preprocessing (think pdfminer, pypdf, unstructured, or AWS Textract). That's a whole other can of worms that's out of scope here, but you'll definitely need it in production.\n\n## How It Works (The Big Picture)\n\nThe pipeline follows this flow:\n\n1. **Input text** ‚Üí Split into overlapping chunks (if it's too long)\n2. **Per\\-chunk extraction** ‚Üí Extract fields with strict schema enforcement and retries\n3. **Aggregation** ‚Üí Merge partial results, deduplicate line items, track where everything came from\n4. **Post\\-processing** ‚Üí Normalize dates and currency, validate totals\n5. **Batch processing** ‚Üí Async extraction with concurrency control\n6. **Caching** ‚Üí Store results keyed by content hash to skip redundant calls\n7. **Metrics** ‚Üí Count tokens and estimate cost per run\n\nEach component is modular, so you can use just the core extraction logic if that's all you need, or scale up to batches with caching and metrics when you're ready.\n\n## Setup \\& Installation\n\nFirst things first, let's get our dependencies installed (this works in Colab too):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-openai langchain-text-splitters pydantic==2.* tenacity tiktoken python-dotenv openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key. If you're working locally, use a `.env` file. For Colab, you can set it directly or use Colab secrets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\n# Option 1: Set directly (not recommended for shared notebooks)\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n# Option 2: Use Colab userdata (recommended for Colab)\ntry:\n    from google.colab import userdata\n    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\nexcept ImportError:\n    # Option 3: Load from .env for local development\n    from dotenv import load_dotenv\n    load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Schema\n\nWe use Pydantic to define what an invoice looks like. Each field gets a description to guide the LLM, and optional fields allow for partial extraction when the invoice is incomplete or weird."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass LineItem(BaseModel):\n    \"\"\"\n    Represents a single line item in an invoice.\n    \"\"\"\n    description: str = Field(..., description=\"Item description as written\")\n    quantity: Optional[float] = Field(None, description=\"Numeric quantity if available\")\n    unit_price: Optional[float] = Field(None, description=\"Unit price, numeric\")\n    amount: Optional[float] = Field(None, description=\"Line total amount, numeric\")\n\nclass InvoiceEntities(BaseModel):\n    \"\"\"\n    Represents the extracted entities from an invoice.\n    \"\"\"\n    invoice_number: Optional[str] = Field(None, description=\"Invoice identifier\")\n    vendor_name: Optional[str] = Field(None, description=\"Company or person issuing the invoice\")\n    vendor_address: Optional[str] = Field(None, description=\"Address block if present\")\n    bill_to_name: Optional[str] = Field(None, description=\"Recipient name\")\n    bill_to_address: Optional[str] = Field(None, description=\"Recipient address\")\n    invoice_date: Optional[str] = Field(None, description=\"Date string as found\")\n    due_date: Optional[str] = Field(None, description=\"Due date string as found\")\n    currency: Optional[str] = Field(None, description=\"Currency code/symbol if inferable\")\n    subtotal: Optional[float] = Field(None, description=\"Subtotal before tax/fees\")\n    tax: Optional[float] = Field(None, description=\"Total tax amount\")\n    total: Optional[float] = Field(None, description=\"Grand total\")\n    line_items: List[LineItem] = Field(default_factory=list, description=\"List of individual line items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Extraction with Structured Output\n\nHere's where we configure the LLM to return structured output matching our schema. Temperature is set to 0 because we want deterministic results, not creative interpretations of invoice data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",  # Use \"gpt-4o\" for higher accuracy if needed\n    temperature=0,        # Deterministic output for reproducibility\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\nstructured_llm = llm.with_structured_output(InvoiceEntities)\n\nSYSTEM_PROMPT = (\n    \"You are an information extraction engine. \"\n    \"Extract invoice fields precisely as a JSON object matching the schema. \"\n    \"Do not include any text that is not part of the JSON.\"\n)\n\ndef extract_invoice(text: str) -> InvoiceEntities:\n    \"\"\"\n    Extracts invoice fields from text using the structured LLM.\n    \"\"\"\n    return structured_llm.invoke([\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": text}\n    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test it with a sample invoice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_doc = \"\"\"\nINVOICE #INV-1028\nVendor: ACME Supplies Co.\nAddress: 123 Market Street, Springfield\nBill To: Northwind Traders\nDate: 2024-08-12\nDue: 2024-09-11\nItems:\n- Paper Reams (QTY 10) @ $4.50 each = $45.00\n- Pens (QTY 20) @ $1.25 each = $25.00\nSubtotal: $70.00\nTax: $6.30\nTotal Due: $76.30\n\"\"\"\n\nresult = extract_invoice(sample_doc)\nprint(result.model_dump())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling Long Documents with Chunking\n\nToken limits are real, and they'll bite you when you least expect it. We chunk with overlap to preserve context between chunks using LangChain's RecursiveCharacterTextSplitter. Actually, if you're curious about how position bias can cause models to miss details in lengthy prompts, check out our analysis on placing critical info in long prompts \\- it's eye\\-opening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ndef split_document(text: str, chunk_size: int = 2000, chunk_overlap: int = 200):\n    \"\"\"\n    Splits a document into overlapping chunks for LLM processing.\n    Tracks source indices for provenance.\n    \"\"\"\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n    )\n    chunks = splitter.split_text(text)\n    \n    # Track source indices for debugging and provenance\n    offsets = []\n    start = 0\n    for chunk in chunks:\n        idx = text.find(chunk, start)\n        if idx == -1:\n            # Guard against find failure; fall back to chunk index only\n            offsets.append((start, start + len(chunk)))\n        else:\n            offsets.append((idx, idx + len(chunk)))\n            start = idx + len(chunk)\n    \n    return [{\"id\": i, \"text\": c, \"start\": s, \"end\": e} for i, (c, (s, e)) in enumerate(zip(chunks, offsets))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregating Partial Results\n\nNow we extract from each chunk and merge the results. For scalar fields like invoice number or date, we take the first non\\-null value. For line items, we deduplicate by description and amount \\- because sometimes the same item shows up in multiple chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\nfrom pydantic import ValidationError\n\ndef extract_invoice_chunked(text: str) -> dict:\n    \"\"\"\n    Extracts invoice data from each chunk and aggregates results.\n    Deduplicates line items and tracks provenance.\n    \"\"\"\n    chunks = split_document(text)\n    partials = []\n    \n    for ch in chunks:\n        try:\n            r = extract_invoice(ch[\"text\"]).model_dump()\n            r[\"_chunk\"] = {\"id\": ch[\"id\"], \"start\": ch[\"start\"], \"end\": ch[\"end\"]}\n            partials.append(r)\n        except ValidationError as e:\n            # Log error for this chunk; will add retries/logging later\n            partials.append({\"_chunk\": {\"id\": ch[\"id\"], \"start\": ch[\"start\"], \"end\": ch[\"end\"]}, \"_error\": str(e)})\n\n    merged = {\n        \"invoice_number\": None,\n        \"vendor_name\": None,\n        \"vendor_address\": None,\n        \"bill_to_name\": None,\n        \"bill_to_address\": None,\n        \"invoice_date\": None,\n        \"due_date\": None,\n        \"currency\": None,\n        \"subtotal\": None,\n        \"tax\": None,\n        \"total\": None,\n        \"line_items\": [],\n        \"_provenance\": [],  # Track which chunk contributed\n    }\n\n    # For scalar fields, prefer the first non-null value found\n    scalar_fields = [k for k in merged.keys() if k not in (\"line_items\", \"_provenance\")]\n    for p in partials:\n        if \"_error\" in p:\n            continue\n        merged[\"_provenance\"].append(p[\"_chunk\"])\n        for f in scalar_fields:\n            if merged[f] is None and p.get(f) not in (None, \"\", []):\n                merged[f] = p.get(f)\n\n    # Merge and deduplicate line_items by (description, amount)\n    seen = set()\n    for p in partials:\n        if \"_error\" in p:\n            continue\n        for li in p.get(\"line_items\", []):\n            key = (li.get(\"description\", \"\").strip().lower(), li.get(\"amount\"))\n            if key not in seen and li.get(\"description\"):\n                merged[\"line_items\"].append(li)\n                seen.add(key)\n\n    return merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strict Schema Enforcement with Fallback\n\nSometimes the first extraction fails validation. Maybe the model returns malformed JSON or gets the types wrong. We add a fallback prompt that basically says \"hey, try again, but actually follow the schema this time.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "STRICT_SYSTEM_PROMPT = (\n    \"You are an extraction engine. Output MUST be valid JSON only, strictly matching the schema. \"\n    \"Do not include comments or extra keys. Numeric fields must be numbers, not strings.\"\n)\n\ndef extract_invoice_strict(text: str) -> InvoiceEntities:\n    \"\"\"\n    Extracts invoice fields with strict schema enforcement and fallback prompt.\n    \"\"\"\n    try:\n        # First attempt with standard prompt\n        return structured_llm.invoke([\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": text}\n        ])\n    except Exception:\n        # Second attempt with stricter prompt for schema compliance\n        return llm.with_structured_output(InvoiceEntities).invoke([\n            {\"role\": \"system\", \"content\": STRICT_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": text}\n        ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding Retries with Exponential Backoff\n\nTransient errors happen. Rate limits, timeouts, the occasional cosmic ray flipping a bit somewhere. We use tenacity to retry with exponential backoff, because hammering the API immediately after a rate limit error is not a winning strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom langchain_core.exceptions import OutputParserException\n\n# Retry on LangChain-surfaced exceptions or OpenAI SDK exceptions if propagated\n@retry(\n    reraise=True,\n    stop=stop_after_attempt(4),  # Maximum 4 attempts\n    wait=wait_exponential(multiplier=1, min=1, max=8),  # Exponential backoff\n    retry=retry_if_exception_type((OutputParserException, Exception))  # Adjust based on error surface\n)\ndef extract_invoice_with_retry(text: str) -> InvoiceEntities:\n    \"\"\"\n    Extracts invoice fields with retry logic for transient errors.\n    \"\"\"\n    return extract_invoice_strict(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: If you're using OpenAI SDK exceptions directly (like RateLimitError or APIError), make sure openai is installed and exceptions propagate through LangChain properly. You might need to adjust the retry decorator depending on your setup.\n\n## Logging Failures for Debugging\n\nWhen extraction fails, you want to know why. We log failures with document ID, input hash, and error details. Save a sample of the failed input too \\- trust me, you'll thank yourself later when debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nimport hashlib\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO)\nFAIL_DIR = Path(\"failures\")\nFAIL_DIR.mkdir(exist_ok=True)\n\ndef log_failure(doc_id: str, text: str, error: Exception):\n    \"\"\"\n    Logs extraction failure details and saves a sample of the failed input.\n    \"\"\"\n    h = hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:12]\n    logging.error(f\"[extract-fail] doc={doc_id} hash={h} error={type(error).__name__}: {error}\")\n    (FAIL_DIR / f\"{doc_id}_{h}.txt\").write_text(text[:4000], encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Async Batch Processing with Concurrency Control\n\nProcessing one invoice at a time is fine for testing, but in production? You need concurrency. We use asyncio and semaphores to limit concurrent requests, integrating strict extraction, retries, and error logging all in one go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nimport asyncio\nfrom typing import Tuple\n\nclass Extractor:\n    \"\"\"\n    Async batch extractor for invoice documents.\n    Integrates strict schema enforcement, retries, and error logging.\n    \"\"\"\n    def __init__(self, model: str = \"gpt-4o-mini\", concurrency: int = 8):\n        self.llm = ChatOpenAI(model=model, temperature=0, api_key=os.getenv(\"OPENAI_API_KEY\"))\n        self.structured_llm = self.llm.with_structured_output(InvoiceEntities)\n        self.sem = asyncio.Semaphore(concurrency)  # Limit concurrent requests\n\n    async def extract_one(self, doc_id: str, text: str) -> Tuple[str, dict, dict]:\n        \"\"\"\n        Extracts a single document asynchronously with retries and error handling.\n        \"\"\"\n        async with self.sem:\n            t0 = time.perf_counter()\n            try:\n                # Use strict extraction with retries (adapt for async if needed)\n                res = await self.structured_llm.ainvoke([\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": text}\n                ])\n                elapsed = time.perf_counter() - t0\n                meta = getattr(res, \"response_metadata\", {}) or {}\n                return doc_id, res.model_dump(), {\"ok\": True, \"elapsed\": elapsed, \"meta\": meta}\n            except Exception as e:\n                log_failure(doc_id, text, e)\n                elapsed = time.perf_counter() - t0\n                return doc_id, {}, {\"ok\": False, \"elapsed\": elapsed, \"error\": str(e)}\n\nasync def process_batch(docs: list, model=\"gpt-4o-mini\", concurrency=8):\n    \"\"\"\n    Processes a batch of documents asynchronously.\n    \"\"\"\n    ex = Extractor(model=model, concurrency=concurrency)\n    tasks = [ex.extract_one(doc_id, text) for doc_id, text in docs]\n    return await asyncio.gather(*tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caching Results to Save Cost\n\nWhy pay to extract the same invoice twice? We cache results on disk keyed by a content hash. And if you want to get really fancy with caching \\- including semantic caching with embeddings to further reduce LLM spend \\- we've got a walkthrough on implementing semantic cache with Redis Vector that goes deep on this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nimport hashlib\nfrom pathlib import Path\n\nCACHE_DIR = Path(\"cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\ndef cache_key(text: str, model: str) -> str:\n    \"\"\"\n    Generates a cache key based on model and text content.\n    \"\"\"\n    h = hashlib.sha256((model + \"||\" + text).encode(\"utf-8\")).hexdigest()\n    return h\n\ndef get_cached(key: str):\n    \"\"\"\n    Retrieves cached extraction result if available.\n    \"\"\"\n    p = CACHE_DIR / f\"{key}.json\"\n    return json.loads(p.read_text()) if p.exists() else None\n\ndef set_cached(key: str, data: dict):\n    \"\"\"\n    Stores extraction result in cache, including provenance and metadata.\n    \"\"\"\n    p = CACHE_DIR / f\"{key}.json\"\n    p.write_text(json.dumps(data), encoding=\"utf-8\")\n\nasync def process_batch_with_cache(docs: list, model=\"gpt-4o-mini\", concurrency=8):\n    \"\"\"\n    Processes a batch of documents with disk cache.\n    \"\"\"\n    ex = Extractor(model=model, concurrency=concurrency)\n    results = []\n    \n    for doc_id, text in docs:\n        key = cache_key(text, model)\n        cached = get_cached(key)\n        if cached:\n            results.append((doc_id, cached, {\"ok\": True, \"elapsed\": 0.0, \"cached\": True, \"meta\": cached.get(\"_meta\", {})}))\n            continue\n        results.append((doc_id, None, None))\n\n    async_tasks = []\n    for (doc_id, text), (rid, cached, meta) in zip(docs, results):\n        if cached is None:\n            async_tasks.append(ex.extract_one(doc_id, text))\n    \n    fresh = await asyncio.gather(*async_tasks)\n\n    # Stitch back, set cache\n    out = []\n    fresh_iter = iter(fresh)\n    for (doc_id, text), (rid, cached, meta) in zip(docs, results):\n        if cached is None:\n            did, data, info = next(fresh_iter)\n            if info[\"ok\"]:\n                data[\"_meta\"] = info.get(\"meta\", {})\n                set_cached(cache_key(text, model), data)\n            out.append((did, data, info))\n        else:\n            out.append((doc_id, cached, meta))\n    \n    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitoring Token Usage and Cost\n\nCount tokens using tiktoken and estimate cost based on model pricing. Always double\\-check current pricing at OpenAI's pricing page because these things change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n\ndef count_tokens(text: str, model_encoding: str = \"cl100k_base\"):\n    \"\"\"\n    Counts the number of tokens in a text string.\n    \"\"\"\n    enc = tiktoken.get_encoding(model_encoding)\n    return len(enc.encode(text))\n\n# Example price map; always verify with OpenAI pricing page\nPRICE_PER_1K = {\n    \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n    \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},\n}\n\ndef estimate_cost(input_tokens: int, output_tokens: int, model: str) -> float:\n    \"\"\"\n    Estimates the cost of a request based on token usage and model.\n    \"\"\"\n    p = PRICE_PER_1K.get(model)\n    if not p:\n        return 0.0\n    return (input_tokens / 1000.0) * p[\"input\"] + (output_tokens / 1000.0) * p[\"output\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post\\-Processing: Normalize and Validate\n\nThis is where we clean up the extracted data. Normalize currency symbols, parse dates consistently, and validate that invoice totals actually add up. This catches a surprising number of extraction errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\nfrom datetime import datetime\n\ndef normalize_currency(s: str | None) -> str | None:\n    \"\"\"\n    Normalizes currency symbols to standard codes.\n    \"\"\"\n    if not s:\n        return None\n    s2 = s.strip().upper()\n    if s2 in {\"$\", \"USD\"}:\n        return \"USD\"\n    if s2 in {\"‚Ç¨\", \"EUR\"}:\n        return \"EUR\"\n    if s2 in {\"¬£\", \"GBP\"}:\n        return \"GBP\"\n    return s2\n\ndef normalize_date(s: str | None) -> str | None:\n    \"\"\"\n    Normalizes date strings to YYYY-MM-DD format.\n    \"\"\"\n    if not s:\n        return None\n    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%d-%m-%Y\", \"%m/%d/%Y\", \"%Y-%m-%dT%H:%M:%S\"):\n        try:\n            return datetime.strptime(s.strip()[:19], fmt).strftime(\"%Y-%m-%d\")\n        except Exception:\n            continue\n    return s  # Return as-is if unknown\n\ndef postprocess_invoice(inv: dict) -> dict:\n    \"\"\"\n    Postprocesses invoice dict: normalizes currency, dates, and checks totals.\n    \"\"\"\n    inv[\"currency\"] = normalize_currency(inv.get(\"currency\"))\n    inv[\"invoice_date\"] = normalize_date(inv.get(\"invoice_date\"))\n    inv[\"due_date\"] = normalize_date(inv.get(\"due_date\"))\n    \n    # Sanity check totals\n    try:\n        subtotal = float(inv[\"subtotal\"]) if inv.get(\"subtotal\") is not None else None\n        tax = float(inv[\"tax\"]) if inv.get(\"tax\") is not None else None\n        total = float(inv[\"total\"]) if inv.get(\"total\") is not None else None\n        if subtotal is not None and tax is not None and total is not None:\n            if abs((subtotal + tax) - total) > 0.05:\n                inv[\"_warning\"] = \"Totals may not add up\"\n    except Exception:\n        inv[\"_warning\"] = \"Totals not numeric\"\n    \n    return inv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\nLet's generate some synthetic invoices for testing and run the full pipeline with caching and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n\ndef synthetic_invoice(i: int) -> str:\n    \"\"\"\n    Generates a synthetic invoice for testing.\n    \"\"\"\n    base = f\"\"\"\nINVOICE #INV-{1000+i}\nVendor: Vendor {i} LLC\nAddress: {i} Main Street, Metropolis\nBill To: Customer {i%7}\nDate: 2024-09-{(i%28)+1:02d}\nDue: 2024-10-{(i%28)+1:02d}\nItems:\n- Widget A (QTY {1+i%5}) @ ${3.5 + (i%3)*0.75:.2f} each = ${((1+i%5)*(3.5 + (i%3)*0.75)):.2f}\n- Widget B (QTY {2+i%3}) @ ${1.25 + (i%4)*0.5:.2f} each = ${((2+i%3)*(1.25 + (i%4)*0.5)):.2f}\nSubtotal: $100.00\nTax: $8.25\nTotal Due: $108.25\n\"\"\"\n    # Inject noise for some invoices\n    if i % 9 == 0:\n        base += \"\\nNote: Prices include a 5% eco-fee.\"\n    if i % 11 == 0:\n        base = base.replace(\"Total Due\", \"Grand Total\")\n    return base\n\nasync def run_demo(n_docs=60, model=\"gpt-4o-mini\", concurrency=8):\n    \"\"\"\n    Runs a demo batch extraction and prints metrics.\n    \"\"\"\n    docs = [(f\"doc-{i}\", synthetic_invoice(i)) for i in range(n_docs)]\n    t0 = time.perf_counter()\n    results = await process_batch_with_cache(docs, model=model, concurrency=concurrency)\n    elapsed = time.perf_counter() - t0\n\n    ok = sum(1 for _, _, info in results if info.get(\"ok\"))\n    fail = n_docs - ok\n    total_input_tokens = 0\n    total_output_tokens = 0\n    est_cost = 0.0\n\n    # Prefer usage from metadata if available; otherwise estimate input tokens only\n    for (doc_id, data, info), (_, text) in zip(results, docs):\n        meta = info.get(\"meta\", {})\n        usage = meta.get(\"token_usage\", {}) if isinstance(meta, dict) else {}\n        in_tok = usage.get(\"input_tokens\", 0) or count_tokens(text)\n        out_tok = usage.get(\"output_tokens\", 0)\n        total_input_tokens += in_tok\n        total_output_tokens += out_tok\n        est_cost += estimate_cost(in_tok, out_tok, model)\n\n    print(f\"Processed: {n_docs} docs\")\n    print(f\"Success: {ok}, Failures: {fail}\")\n    print(f\"Elapsed: {elapsed:.2f}s, Avg per doc: {elapsed/n_docs:.2f}s\")\n    print(f\"Tokens ~ input: {total_input_tokens}, output: {total_output_tokens}\")\n    print(f\"Estimated cost: ${est_cost:.4f}\")\n    print(\"‚ö†Ô∏è Verify current pricing at https://openai.com/pricing and adjust PRICE_PER_1K accordingly.\")\n    \n    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the demo in a notebook cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For notebooks/Colab, use await directly or nest_asyncio if needed\nresults = await run_demo(n_docs=60, model=\"gpt-4o-mini\", concurrency=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And inspect a single result with provenance tracking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_id, data, info = results[0]\nprint(f\"Document: {doc_id}\")\nprint(f\"Extracted: {data}\")\nprint(f\"Provenance: {data.get('_provenance', [])}\")\nprint(f\"Warnings: {data.get('_warning', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection: Accuracy vs. Cost\n\nPicking the right model is all about balancing accuracy and cost:\n\n* **gpt\\-4 class** (like gpt\\-4o, gpt\\-4\\-turbo): Best accuracy for complex schemas, noisy text, and multi\\-entity extraction. Check the OpenAI Models docs for the latest.\n* **gpt\\-3\\.5\\-turbo class**: Cheaper and faster, but honestly, less reliable for nuanced fields and maintaining JSON structure.\n\nIf you're not sure how to evaluate which model fits your needs, our guide on how to choose an AI model for your app breaks down performance, context length, and pricing considerations in detail.\n\nHere are my recommended defaults after way too much experimentation:\n\n* **Model**: gpt\\-4o\\-mini\n* **Temperature**: 0\n* **Max concurrency**: 8\n* **Chunk size**: 2000 tokens\n* **Chunk overlap**: 200 tokens\n* **Retry attempts**: 4 with exponential backoff\n\nIf you're seeing frequent validation errors or missing fields, bite the bullet and upgrade to gpt\\-4o. The extra cost is usually worth it.\n\n## Conclusion\n\nAnd there you have it \\- a production\\-ready invoice extraction pipeline that handles schema validation, long documents, retries, async batching, caching, and cost tracking. The system is modular, so use just the core extraction logic if that's all you need, or scale to thousands of documents with the batch processor when you're ready for the big leagues.\n\nWhere to go from here:\n\n* **Add OCR/PDF preprocessing** ‚Äì Integrate pdfminer, pypdf, or AWS Textract to handle those scanned invoices that are 90% of what you'll see in the real world.\n* **Improve aggregation** ‚Äì Use semantic similarity (embeddings) to merge duplicate line items more intelligently.\n* **Deploy as an API** ‚Äì Wrap the batch processor in FastAPI or Flask and you've got yourself a production service.\n* **Monitor in production** ‚Äì Add structured logging, alerting, and dashboards to track extraction accuracy and cost over time. Because what gets measured gets managed."
      ]
    }
  ],
  "metadata": {
    "title": "Structured Data Extraction: How to Build a LangChain Pipeline",
    "description": "Build an end-to-end LangChain + OpenAI pipeline for structured data extraction from long documents‚Äîschema validation, batching, retries, and production-ready code.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}