{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build Reliable LangChain LLM Workflows in 15 Minutes Flat\n\n**Description:** Get hands-on with LangChain: install, configure models, build prompt-driven chains, and parse structured outputsâ€”launch reliable Python LLM workflows fast today.\n\n**ðŸ“– Read the full article:** [How to Build Reliable LangChain LLM Workflows in 15 Minutes Flat](https://blog.thegenairevolution.com/article/langchain-101-build-your-first-real-llm-application-step-by-step)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're reading this, you probably want to start building with LLMs but don't know where to begin with LangChain. I get it. The documentation can feel overwhelming, and most tutorials jump straight into complex RAG systems or agent architectures. Let's take a step back.\n\nThis is your LangChain 101\\. Think of it as your first day learning to code, except instead of \"Hello World,\" we're going to build something actually useful. By the end of this guide, you'll understand the core concepts that make LangChain tick, and more importantly, you'll have working code that does something real.\n\n![Uploaded image](/public-objects/user_insert_44830763_1764093555105.png \"Uploaded image\")\n\n## What We're Building (And Why It Matters)\n\nWe're going to build a system that takes messy customer emails and turns them into clean, structured JSON data. Why this example? Because it shows you every fundamental LangChain concept in action without getting lost in the weeds. You'll see prompts, chains, parsers, and error handling all working together.\n\nBut honestly, the specific use case doesn't matter that much. What matters is that you'll understand how to connect these pieces. Once you get this, you can build anything.\n\n## The Core Concepts You Actually Need\n\nBefore we write any code, let me explain the four concepts that power everything in LangChain. And I mean everything.\n\n**Runnables and Chains**\n\nA runnable is just something you can call with input and get output. That's it. A chain is when you connect multiple runnables together with the pipe operator. Think of it like Unix pipes but for LLMs. You take a prompt, pipe it to a model, pipe that to a parser. Simple.\n\n**Prompt Templates**\n\nThese let you create reusable prompts with variables. Instead of hardcoding \"Extract data from this email: \\[email text]\" every time, you create a template once and inject different emails at runtime. It keeps your prompts organized and testable. For more advanced prompting strategies, check out our [techniques for prompting reasoning models for clear, accurate answers](/article/how-to-prompt-reasoning-models-for-clear-accurate-answers-techniques-examples-2).\n\n**Structured Output Parsers**\n\nHere's where things get interesting. Parsers force the LLM to return data in a specific format. You define what fields you want, what types they should be, and the parser validates everything. No more regex nightmares trying to extract data from free\\-form text.\n\n**Messages**\n\nLangChain uses message objects to represent conversation turns. SystemMessage for instructions, HumanMessage for user input, AIMessage for model responses. This makes multi\\-turn conversations explicit and portable across different models. If you want to dive deeper into making your LLMs learn from examples on the fly, explore our guide on [in\\-context learning techniques to boost LLM accuracy](/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3).\n\n## Quick Start: From Zero to Working Code\n\nAlright, let's build something. First, install what you need. Nothing fancy here, just the basics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-community langchain-openai openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, about API keys. Please don't hardcode them. I learned this the hard way in a previous role. Use environment variables or, if you're in Colab, use their secrets feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom google.colab import userdata\nfrom google.colab.userdata import SecretNotFoundError\n\nkeys = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\nmissing = []\nfor k in keys:\n    value = None\n    try:\n        value = userdata.get(k)\n    except SecretNotFoundError:\n        pass\n\n    os.environ[k] = value if value is not None else \"\"\n\n    if not os.environ[k]:\n        missing.append(k)\n\nif missing:\n    raise EnvironmentError(f\"Missing keys: {', '.join(missing)}. Add them in Colab â†’ Settings â†’ Secrets.\")\n\nprint(\"All keys loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify everything's working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\nassert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in your Colab secrets\"\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.output_parsers import ResponseSchema, StructuredOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create your model instance. I'm using temperature 0 because we want consistent outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0,\n    max_tokens=300,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test it with a simple prompt to make sure your API key works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "msg = llm.invoke(\"Summarize why consistent JSON outputs help downstream systems.\")\nprint(type(msg))\nprint(msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the response metadata. This shows you token usage, which matters for cost:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Response metadata:\", getattr(msg, \"response_metadata\", {}))\nprint(\"Usage metadata:\", getattr(msg, \"usage_metadata\", {}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Your First Chain\n\nNow for the fun part. Let's build a conversation with explicit message roles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n    SystemMessage(content=\"You are a concise assistant that extracts key facts.\"),\n    HumanMessage(content=\"I purchased earbuds last week. The left bud is dead.\"),\n    AIMessage(content=\"Noted. A device failure on the left earbud.\"),\n    HumanMessage(content=\"What information would you need to process a warranty claim?\")\n]\n\nreply = llm.invoke(messages)\nprint(reply.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a prompt template. This is where you inject variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"{persona}\"),\n        (\"human\", \"{user_input}\")\n    ]\n)\n\nrendered = prompt.invoke({\n    \"persona\": \"You are a helpful customer support assistant.\",\n    \"user_input\": \"Customer reports a faulty left earbud after 7 days. Next step?\"\n})\nprint(rendered.to_messages())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actually, let me emphasize something. Parameterized prompts are crucial for production systems. You want to version control these, test them, swap them out easily. For more on building reliable LLM features, see our guide on [prompt engineering with LLM APIs for reliable outputs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4).\n\nNow compose a chain using LCEL (LangChain Expression Language):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = prompt | llm\n\nresp = chain.invoke({\n    \"persona\": \"You are a helpful customer support assistant.\",\n    \"user_input\": \"The customer wants a refund for defective earbuds. What should we do?\"\n})\nprint(resp.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding Structure with Parsers\n\nThis is where LangChain really shines. Define what fields you want to extract:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schemas = [\n    ResponseSchema(\n        name=\"type\",\n        description=\"One of complaint, inquiry, feedback.\"\n    ),\n    ResponseSchema(\n        name=\"product\",\n        description=\"Product or service mentioned, string.\"\n    ),\n    ResponseSchema(\n        name=\"action\",\n        description=\"Recommended action like refund, replace, clarify, route_to_support.\"\n    ),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a parser and generate format instructions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser = StructuredOutputParser.from_response_schemas(schemas)\nformat_instructions = parser.get_format_instructions()\nprint(format_instructions[:200], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the complete extraction chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extraction_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You extract structured fields from customer emails. \"\n            \"Return JSON that strictly follows these rules. {format_instructions}\"\n        ),\n        (\"human\", \"Email:\\n{email}\")\n    ]\n)\n\nextraction_chain = extraction_prompt | llm | parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run some customer emails through it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emails = [\n    \"Hi, my left earbud stopped working after a week. I want a refund please.\",\n    \"Hello, can you tell me if the Model X earbuds support wireless charging?\",\n    \"Just wanted to say the new firmware fixed my microphone issue. Thanks.\"\n]\n\nfor e in emails:\n    result = extraction_chain.invoke({\n        \"email\": e,\n        \"format_instructions\": format_instructions\n    })\n    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validate that you're getting the right fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_result(d):\n    assert isinstance(d, dict)\n    assert d[\"type\"] in {\"complaint\", \"inquiry\", \"feedback\"}\n    assert isinstance(d[\"product\"], str)\n    assert isinstance(d[\"action\"], str)\n\nfor e in emails:\n    d = extraction_chain.invoke({\"email\": e, \"format_instructions\": format_instructions})\n    validate_result(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making It Production\\-Ready\n\nLet's be honest, the basic chain works but it's not ready for production. You need error handling, monitoring, and flexibility.\n\nFirst, check performance without parsing overhead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import perf_counter\n\nraw_chain = extraction_prompt | llm\n\nstart = perf_counter()\nraw_msg = raw_chain.invoke({\"email\": emails[0], \"format_instructions\": format_instructions})\nelapsed = perf_counter() - start\n\nprint(\"Latency seconds:\", round(elapsed, 3))\nprint(\"Usage:\", getattr(raw_msg, \"usage_metadata\", {}))\n\nparsed = parser.invoke(raw_msg)\nprint(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a reusable function with proper validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_email_fields(email: str) -> dict:\n    raw = raw_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n    usage = getattr(raw, \"usage_metadata\", {})\n    parsed = parser.invoke(raw)\n    validate_result(parsed)\n    return {\"data\": parsed, \"usage\": usage}\n\nprint(extract_email_fields(\"The Model X case will not charge. Need a replacement.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add domain\\-specific personas. I've found this incredibly useful when dealing with technical support versus sales inquiries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "persona = \"You are a support triage assistant for consumer audio devices.\"\npersona_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", persona + \" Return strict JSON. {format_instructions}\"),\n        (\"human\", \"Email:\\n{email}\")\n    ]\n)\npersona_chain = persona_prompt | llm | parser\n\nprint(persona_chain.invoke({\"email\": emails[1], \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test with your own examples, including edge cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_emails = [\n    \"Order 1234. Model X earbuds arrived scratched. I want a refund.\",\n    \"Do the Model Y earbuds pair with two phones at once?\",\n    \"Love the sound on Model Z. Battery could be better, just feedback.\"\n]\n\nfor e in my_emails:\n    print(extraction_chain.invoke({\"email\": e, \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Track telemetry for each call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def timed_invoke(email):\n    import time\n    t0 = time.perf_counter()\n    raw = raw_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n    dt = time.perf_counter() - t0\n    usage = getattr(raw, \"usage_metadata\", {})\n    return dt, usage, parser.invoke(raw)\n\nfor e in emails:\n    dt, usage, data = timed_invoke(e)\n    print({\"latency_s\": round(dt, 3), \"usage\": usage, \"data\": data})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's something important. Models sometimes return malformed JSON. You need retry logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.output_parsers import OutputParserException\n\ndef safe_extract(email, max_retries=1):\n    for attempt in range(max_retries + 1):\n        try:\n            return extraction_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n        except OutputParserException:\n            corrective = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", \"Return valid JSON only. Do not include commentary. {format_instructions}\"),\n                    (\"human\", \"Email:\\n{email}\")\n                ]\n            )\n            retry_chain = corrective | llm | parser\n            if attempt < max_retries:\n                result = retry_chain.invoke({\"email\": email, \"format_instructions\": format_instructions})\n                return result\n            raise\n\nprint(safe_extract(\"Refund me please. Model X left earbud broke in a week.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extending and Customizing\n\nAs your needs grow, you'll want to expand the schema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schemas_extended = schemas + [\n    ResponseSchema(name=\"urgency\", description=\"low, medium, high based on sentiment and urgency cues.\")\n]\nparser_ext = StructuredOutputParser.from_response_schemas(schemas_extended)\nfmt_ext = parser_ext.get_format_instructions()\n\nprompt_ext = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"Extract fields and urgency. Return strict JSON. {format_instructions}\"),\n        (\"human\", \"Email:\\n{email}\")\n    ]\n)\nchain_ext = prompt_ext | llm | parser_ext\n\nprint(chain_ext.invoke({\"email\": emails[0], \"format_instructions\": fmt_ext}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add basic tests. Trust me, you'll thank yourself later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_extraction():\n    sample = \"Left earbud on Model X stopped working. Please replace.\"\n    d = extraction_chain.invoke({\"email\": sample, \"format_instructions\": format_instructions})\n    assert d[\"type\"] in {\"complaint\", \"inquiry\", \"feedback\"}\n    assert isinstance(d[\"product\"], str)\n    assert d[\"action\"] in {\"refund\", \"replace\", \"clarify\", \"route_to_support\"}\n\ntest_extraction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use fixtures for reproducible testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fixtures = [\n    {\n        \"email\": \"Model X case not charging. Need a replacement.\",\n        \"expect_type\": {\"complaint\"},\n    },\n    {\n        \"email\": \"Do Model Y earbuds support USB C charging?\",\n        \"expect_type\": {\"inquiry\"},\n    },\n]\n\nfor fx in fixtures:\n    d = extraction_chain.invoke({\"email\": fx[\"email\"], \"format_instructions\": format_instructions})\n    assert d[\"type\"] in fx[\"expect_type\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Process multiple emails efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = [extraction_chain.invoke({\"email\": e, \"format_instructions\": format_instructions}) for e in emails]\nprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The beauty of LangChain? Swapping models is trivial. In a personal project, I compared GPT\\-4 against Claude for extraction tasks with just this change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_alt = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=300)\nextraction_chain_alt = extraction_prompt | llm_alt | parser\n\nprint(extraction_chain_alt.invoke({\"email\": emails[2], \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Version your prompts and parsers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPT_VERSION = \"v1.2\"\nSCHEMA_VERSION = \"v1.1\"\n\nprint({\"prompt_version\": PROMPT_VERSION, \"schema_version\": SCHEMA_VERSION})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use environment variables for configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\nTEMP = float(os.getenv(\"TEMP\", \"0\"))\nllm_cfg = ChatOpenAI(model=MODEL_NAME, temperature=TEMP, max_tokens=300)\ncfg_chain = extraction_prompt | llm_cfg | parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try these test cases to see it all work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cases = [\n    \"I love the sound on Model X, but the right bud randomly disconnects. Can you replace it?\",\n    \"Do Model Y earbuds work with iOS 17? If yes, how to pair?\",\n    \"Great update, pairing is faster now. Just a note for your team.\"\n]\n\nfor c in cases:\n    print(cfg_chain.invoke({\"email\": c, \"format_instructions\": format_instructions}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where to Go From Here\n\nYou've just built a complete LangChain workflow. You understand runnables, chains, prompts, and parsers. You can handle errors, track usage, and swap models. That's the foundation.\n\nWhat's next? Well, if you need to ground your responses in external data, look into RAG (Retrieval\\-Augmented Generation). Our [ultimate guide to vector store retrieval for RAG systems](/article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it) shows you how to add semantic search to your chains.\n\nWant more control over model behavior? Consider fine\\-tuning. Our [step\\-by\\-step guide to fine\\-tuning large language models](/article/fine-tuning-large-language-models-a-step-by-step-guide-2025-5) walks through the entire process.\n\nAnd if you're curious about what's happening under the hood, check out our [ultimate guide to transformer models for LLM practitioners](/article/transformers-demystifying-the-magic-behind-large-language-models-2).\n\nBut honestly? Start by building something with what you learned today. Pick a problem, any problem, and solve it with LangChain. The best way to learn is by doing. And now you know enough to actually do something useful."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build Reliable LangChain LLM Workflows in 15 Minutes Flat",
    "description": "Get hands-on with LangChain: install, configure models, build prompt-driven chains, and parse structured outputsâ€”launch reliable Python LLM workflows fast today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}