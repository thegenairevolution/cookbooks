{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Model Context Protocol (MCP) Server in Python\n\n**Description:** Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.\n\n**ðŸ“– Read the full article:** [How to Build a Model Context Protocol (MCP) Server in Python](https://blog.thegenairevolution.com/article/how-to-build-a-model-context-protocol-mcp-server-in-python)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Model Context Protocol (MCP) lets you define tools, resources, and prompts once and expose them to any MCP\\-capable client. This includes CLIs, agents, and chatbots. Instead of rewriting tool logic for each application, you build a single server that clients discover and invoke automatically.\n\nThis guide walks you through building a minimal Python MCP server over stdio. You'll test it with a client and verify that tools, resources, and prompts work as expected. By the end, you'll have a working server that exposes math tools, static documentation, and a prompt template. You can run it in a notebook or a local terminal.\n\nFor a comprehensive overview of how MCP standardizes tool and data access, see [Model Context Protocol (MCP) Explained.](/article/model-context-protocol-mcp-explained-2025-guide-for-builders)\n\n## Why Use MCP for This Problem\n\nWhen you need to share tools across multiple applications, you have several options.\n\n* **Shared Python package**: Every app must import and maintain the same library version. You get no runtime discovery or schema negotiation.\n\n* **Bespoke HTTP microservice**: You add network overhead and custom API design. You also lack standardized tool metadata.\n\n* **OpenAI\\-native tools defined per app**: You duplicate tool definitions in every codebase. You lose a single source of truth.\n\n* **Agent\\-framework\\-specific tools**: You get locked into one framework API. Porting to another requires rewriting.\n\nMCP solves these problems by providing:\n\n* **Automatic discovery**: Clients list available tools, resources, and prompts at runtime.\n\n* **Standardized schemas**: JSON Schema definitions ensure consistent validation and documentation.\n\n* **Transport abstraction**: Stdio, SSE, or WebSocket. Clients and servers negotiate capabilities without custom protocols.\n\nCentralizing these capabilities in one server removes repetition. Clients can discover and invoke standardized functionality automatically. If you want to avoid subtle bugs caused by tokenization quirks, read [Tokenization Pitfalls: Invisible Characters That Break Prompts and RAG](/article/tokenization-pitfalls-invisible-characters-that-break-prompts-and-rag-2), and consider [implementing reliable vector store retrieval for RAG](/article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it).\n\nA single MCP server gives you one place to update logic and metadata. You get one schema for validation and consistent behavior across apps. Keep in mind that LLM context is not infinite memory. If you plan to scale prompt sizes or chain calls, see [Context Rot \\- Why LLMs \"Forget\" as Their Memory Grows](/article/context-rot-why-llms-forget-as-their-memory-grows-3).\n\nIf you want to compare MCP with a workflow\\-first approach, explore [building robust LLM workflows with LangChain](/article/langchain-101-build-your-first-real-llm-application-step-by-step). It complements this pattern by showing how to orchestrate tools and prompts in a structured pipeline.\n\n## Core Concepts for This Use Case\n\nBefore building, let's understand these MCP primitives.\n\n* **Tools**: Functions the client can invoke with typed arguments. The server executes and returns results.\n\n* **Resources**: Static or dynamic data identified by URI. Clients read them on demand as text, JSON, or binary.\n\n* **Prompts**: Reusable templates that generate messages for LLM conversations. Clients fetch and render them with arguments.\n\n* **Stdio transport**: Server and client communicate over standard input and output. This is the simplest option for local development and subprocess integration.\n\n* **Schema negotiation**: The server advertises tool input schemas with JSON Schema and resource MIME types. Clients validate input and adapt automatically.\n\nIf you're new to crafting effective templates and roles, see [best practices for prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4). It will help you get consistent results from your MCP prompts.\n\n## Setup\n\nConfirm you have Python 3\\.9 or later. Create a clean virtual environment to avoid dependency conflicts. Decide whether you'll run in a notebook or in a terminal. If you use a notebook, make sure you can write files to the working directory.\n\nInstall the required packages in a notebook cell or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"mcp[cli]>=0.9.0\" anyio>=4.0.0 openai>=1.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This installs the MCP SDK, an async runtime, and an OpenAI client. Pin versions for reproducibility. If installation fails, check your Python version and virtual environment first. Then retry the install. If you plan to operate your stack outside managed services, consider [deploying a self\\-hosted LLM server](/article/how-to-run-a-self-hosted-llm-on-your-server-practical-guide-2025-2) to pair with your MCP server.\n\n## Using the Tool in Practice\n\n### Build the MCP Server\n\nCreate a server that exposes two math tools, a static markdown resource, and a prompt template. This example uses stdio transport for simplicity.\n\nDecide on a filename for the server. For example, you can name it mcp\\_server.py. If you work in a notebook, use your environment's file writing utility to create the file. If you work in a terminal, create the file with your preferred editor.\n\nWrite the server to a file using a notebook cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile mcp_server.py\n# Purpose: Minimal MCP server exposing math tools, a static resource, and a prompt template over stdio.\n\nimport anyio\nfrom typing import Annotated\n\n# MCP server APIs\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import (\n    PromptMessage,\n    TextResourceContents,\n    Resource,\n    Prompt,\n)\n\n# Instantiate the MCP server with a unique name\nserver = Server(\"calc-server\")\n\n@server.tool()\nasync def add(a: Annotated[int, \"First integer\"], b: Annotated[int, \"Second integer\"]) -> int:\n    \"\"\"\n    Add two integers and return the sum.\n\n    Args:\n        a (int): First integer.\n        b (int): Second integer.\n\n    Returns:\n        int: The sum of a and b.\n    \"\"\"\n    # Simple addition; no edge cases for int\n    return a + b\n\n@server.tool()\nasync def subtract(a: Annotated[int, \"Minuend\"], b: Annotated[int, \"Subtrahend\"]) -> int:\n    \"\"\"\n    Subtract b from a and return the difference.\n\n    Args:\n        a (int): Minuend.\n        b (int): Subtrahend.\n\n    Returns:\n        int: The result of a - b.\n    \"\"\"\n    # Simple subtraction; no edge cases for int\n    return a - b\n\n# Resource: static markdown documentation\nDOCS_ID = \"docs/usage\"\nDOCS_CONTENT = \"\"\"# Calc Server Usage\n\nTools:\n- add(a: int, b: int): returns a + b\n- subtract(a: int, b: int): returns a - b\n\nPrompt:\n- math_helper(expression: string): step-by-step computation\n\"\"\"\n\n@server.resource(DOCS_ID, mime_type=\"text/markdown\")\nasync def read_docs() -> TextResourceContents:\n    \"\"\"\n    Return static markdown documentation for the server.\n\n    Returns:\n        TextResourceContents: Markdown-formatted usage documentation.\n    \"\"\"\n    # TextResourceContents includes text and optional annotations\n    return TextResourceContents(text=DOCS_CONTENT)\n\n@server.prompt(\"math_helper\")\nasync def math_helper(expression: Annotated[str, \"Math expression to compute\"]):\n    \"\"\"\n    Generate a prompt for step-by-step math computation.\n\n    Args:\n        expression (str): Math expression to compute.\n\n    Returns:\n        list[PromptMessage]: System and user prompt messages.\n    \"\"\"\n    # System prompt instructs the assistant to show work\n    system = PromptMessage(role=\"system\", content=\"You are a careful math assistant. Show your work.\")\n    # User prompt includes the expression to compute\n    user = PromptMessage(role=\"user\", content=f\"Compute the following expression step by step: {expression}\")\n    return [system, user]\n\nasync def main():\n    \"\"\"\n    Main entry point: runs the MCP server over stdio until EOF.\n    \"\"\"\n    # stdio transport: run until EOF\n    async with stdio_server() as (read_stream, write_stream):\n        await server.run(read_stream, write_stream)\n\nif __name__ == \"__main__\":\n    anyio.run(main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The @server.tool() decorator registers async functions as tools. The client receives JSON Schema for each tool, so it can validate arguments before calling. The @server.resource() decorator exposes static or dynamic data by URI. Clients read the content using the resource URI, and the server returns the body with a MIME type. The @server.prompt() decorator defines reusable prompt templates. A prompt can define named input variables, message roles, and a message sequence. The server runs over stdio, reads requests from stdin, and writes responses to stdout.\n\nAs you build, keep these checks in mind:\n\n* Each tool should define a clear JSON Schema for inputs. This helps clients validate before execution.\n\n* Each resource should return a stable URI and a correct content type. This helps clients render content correctly.\n\n* Each prompt should declare required arguments and message roles. This helps clients supply all needed variables.\n\n### Test the Server with a Client\n\nCreate a separate file for the client. You can name it mcp\\_client.py or reuse a notebook cell. The client will start the server as a subprocess and connect over stdio.\n\nWrite a client that connects to the server, lists capabilities, calls a tool, reads a resource, and fetches a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile mcp_client.py\n# Purpose: Async MCP client to test server tools, resources, and prompts over stdio.\n\nimport anyio\nfrom mcp.client.stdio import stdio_client\nfrom mcp.client.session import ClientSession\n\nasync def main():\n    \"\"\"\n    Connects to the MCP server, lists tools/resources/prompts, calls a tool, and fetches a prompt.\n\n    Raises:\n        Exception: If server connection or calls fail.\n    \"\"\"\n    # Launch the server as a subprocess and connect over stdio\n    async with stdio_client([\"python\", \"mcp_server.py\"]) as (read_stream, write_stream):\n        async with ClientSession(read_stream, write_stream) as session:\n            # List tools and print their names\n            tools = await session.list_tools()\n            print(\"Tools:\", [t.name for t in tools.tools])\n\n            # List resources and print their URIs\n            resources = await session.list_resources()\n            print(\"Resources:\", [r.uri for r in resources.resources])\n\n            # Read and print documentation resource if present\n            for r in resources.resources:\n                if r.uri.endswith(\"docs/usage\"):\n                    content = await session.read_resource(r.uri)\n                    # content.contents is a list of typed chunks (e.g., text, blob)\n                    for c in content.contents:\n                        if hasattr(c, \"text\"):\n                            print(\"Docs:\\n\", c.text)\n\n            # Call 'add' tool with arguments and print the result\n            result = await session.call_tool(\"add\", {\"a\": 5, \"b\": 7})\n            # result.content is list of output messages; pick first text\n            out = result.content[0].text if result.content else None\n            print(\"add(5,7) =\", out)\n\n            # Fetch prompt template and print its messages\n            prompts = await session.list_prompts()\n            print(\"Prompts:\", [p.name for p in prompts.prompts])\n            prompt = await session.get_prompt(\"math_helper\", {\"expression\": \"3*(4+2)\"})\n            print(\"Prompt messages:\", [(m.role, m.content) for m in prompt.messages])\n\nif __name__ == \"__main__\":\n    anyio.run(main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The client launches the server as a subprocess, establishes a session over stdio, and exercises all three primitives. It lists tools, resources, and prompts. It calls a math tool to confirm schema validation and execution. It reads a markdown resource to confirm URI resolution and content types. It fetches a prompt and renders it with arguments. The stdio\\_client context manager handles process lifecycle and stream wiring. If the server crashes, you'll see an error during capability discovery or the first call. Check the server logs or print statements to diagnose.\n\n### Run and Evaluate\n\nRun the client to verify the server works end to end. If you use a terminal, run the client from the same virtual environment. If you use a notebook, run the cell that executes the client code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python mcp_client.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tools: ['add', 'subtract']\nResources: ['docs/usage']\nDocs:\n # Calc Server Usage\n\nTools:\n- add(a: int, b: int): returns a + b\n- subtract(a: int, b: int): returns a - b\n\nPrompt:\n- math_helper(expression: string): step-by-step computation\n\nadd(5,7) = 12\nPrompts: ['math_helper']\nPrompt messages: [('system', 'You are a careful math assistant. Show your work.'), ('user', 'Compute the following expression step by step: 3*(4+2)')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output confirms that:\n\n* The client discovered tools, resources, and prompts during session setup.\n\n* The tool call returned a valid result and matched the expected schema.\n\n* The resource read returned the expected content with the correct MIME type.\n\n* The prompt rendered correctly with your input arguments.\n\nIf any step fails, verify your Python path, virtual environment, and file locations. Make sure your server file is executable in your environment. On Windows, confirm that your Python launcher runs the correct interpreter. On macOS or Linux, confirm permissions and the working directory.\n\n### Optional: Add Resilience to Tool Calls\n\nFor production use, add a timeout and error handler around tool calls. This prevents a slow or failing tool from blocking the client. You'll also capture and surface meaningful error messages.\n\nCreate a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile helpers/safe_call.py\n# Purpose: Utility for safe, timeout-guarded MCP tool calls.\n\nfrom anyio import fail_after, WouldBlock\nimport logging\n\nasync def safe_call_tool(mcp, name, args, seconds=10):\n    \"\"\"\n    Call an MCP tool with a timeout and error handling.\n\n    Args:\n        mcp: MCP client session.\n        name (str): Tool name.\n        args (dict): Tool arguments.\n        seconds (int): Timeout in seconds.\n\n    Returns:\n        Tool call result or None if failed/timed out.\n\n    Raises:\n        None: All exceptions are caught and logged.\n    \"\"\"\n    try:\n        with fail_after(seconds):\n            return await mcp.call_tool(name, args)\n    except WouldBlock:\n        # Timeout occurred\n        logging.warning(f\"Tool call to {name} timed out after {seconds}s.\")\n        return None\n    except Exception as e:\n        # Log error and return None\n        logging.error(f\"Tool call {name} failed: {e}\")\n        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use safe\\_call\\_tool in place of direct session.call\\_tool. Start with a conservative timeout. Expand it only if you confirm that a tool needs more time. Always log errors and return structured error information to the caller.\n\n## Troubleshooting\n\n* **The client cannot discover tools**: Ensure the server process starts correctly. Check that the stdio transport is not buffered or redirected. Confirm the working directory contains the server file.\n\n* **JSON Schema validation errors**: Verify that your tool input matches the schema. Check required fields, types, and enum values. Update the tool schema if your arguments are valid but the schema is too strict.\n\n* **Resource not found**: Confirm the resource URI matches exactly. If the server builds resource URIs dynamically, print the final URI and compare it to the client request.\n\n* **Prompt rendering errors**: Make sure you pass all required variables. If your template variables change, update both the server prompt declaration and the client call.\n\n* **Hanging calls**: Add timeouts using the resilience pattern. If a tool depends on external services, handle network retries and backoff inside the tool implementation.\n\n* **Version conflicts**: Pin your dependencies and reinstall in a fresh virtual environment. Confirm that the client and server rely on compatible MCP SDK versions.\n\n## Conclusion\n\nYou built a minimal MCP server that exposes tools, resources, and prompts over stdio, and you verified it with a test client. This pattern lets you define capabilities once and reuse them across any MCP\\-compatible application. You now have a repeatable way to standardize tool access without rewriting code for each app.\n\n## Next Steps\n\n* **Integrate with OpenAI function calling**. Convert MCP tools to OpenAI tool schemas and route calls from GPT\\-4 to your server. This is covered in a separate guide. For deeper customization, see the [step\\-by\\-step guide to fine\\-tuning large language models](/article/fine-tuning-large-language-models-a-step-by-step-guide-2025-6).\n\n* **Add dynamic resources**: Serve real\\-time data like database queries or API responses instead of static markdown.\n\n* **Explore other transports**: Use SSE or WebSocket for remote or browser\\-based clients.\n\n* **Package and deploy**: Containerize your server and expose it through a network transport for multi\\-user access."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Model Context Protocol (MCP) Server in Python",
    "description": "Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}