{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Model Context Protocol (MCP) Server in Python\n\n**Description:** Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.\n\n**ðŸ“– Read the full article:** [How to Build a Model Context Protocol (MCP) Server in Python](https://blog.thegenairevolution.com/article/how-to-build-a-model-context-protocol-mcp-server-in-python-2)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So here's the thing about the Model Context Protocol (MCP): it lets you define tools, resources, and prompts just once, then expose them to any MCP\\-capable client out there. We're talking CLIs, agents, chatbots, you name it. Instead of constantly rewriting the same tool logic for every single application (which, let's be honest, gets old fast), you build one server that clients can discover and invoke automatically.\n\nI'm going to walk you through building a minimal Python MCP server over stdio. We'll test it with a client and make sure everything actually works. By the time we're done, you'll have a working server that exposes math tools, static documentation, and a prompt template. You can run this in a notebook or just fire it up in your local terminal, whatever works for you.\n\nIf you want the full picture of how MCP standardizes tool and data access, check out [Model Context Protocol (MCP) Explained.](/article/model-context-protocol-mcp-explained-2025-guide-for-builders)\n\n## Why Use MCP for This Problem\n\nWhen I first started thinking about sharing tools across multiple applications, I quickly realized there were several approaches, each with their own headaches.\n\nYou could go with a **shared Python package**. But then every app needs to import and maintain the exact same library version. And forget about runtime discovery or schema negotiation, that's just not happening.\n\nMaybe a **bespoke HTTP microservice**? Sure, but now you're dealing with network overhead and custom API design. Plus you still don't have standardized tool metadata.\n\nWhat about **OpenAI\\-native tools defined per app**? Well, now you're duplicating tool definitions everywhere. You completely lose that single source of truth.\n\nOr **agent\\-framework\\-specific tools**? Great until you realize you're locked into one framework API. Want to port to another framework? Time to rewrite everything.\n\nHere's where MCP actually solves these problems:\n\n* **Automatic discovery**: Clients can list available tools, resources, and prompts right at runtime. No guesswork.\n* **Standardized schemas**: JSON Schema definitions mean you get consistent validation and documentation everywhere.\n* **Transport abstraction**: Whether it's stdio, SSE, or WebSocket, clients and servers negotiate capabilities without you having to build custom protocols.\n\n\nCentralizing these capabilities in one server just makes sense. It removes all that repetition. Clients discover and invoke standardized functionality automatically. And honestly, if you're worried about those subtle bugs that tokenization quirks can cause, you should really read [Tokenization Pitfalls: Invisible Characters That Break Prompts and RAG](/article/tokenization-pitfalls-invisible-characters-that-break-prompts-and-rag-2). You might also want to consider [implementing reliable vector store retrieval for RAG](/article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it).\n\nThe beauty of a single MCP server is that you have one place to update logic and metadata. One schema for validation. Consistent behavior across all your apps. But remember, LLM context isn't infinite memory. If you're planning to scale prompt sizes or chain calls, definitely check out [Context Rot \\- Why LLMs \"Forget\" as Their Memory Grows](/article/context-rot-why-llms-forget-as-their-memory-grows-3).\n\nActually, if you're curious about comparing MCP with a workflow\\-first approach, take a look at [building robust LLM workflows with LangChain](/article/langchain-101-build-your-first-real-llm-application-step-by-step). It's a nice complement to this pattern, showing you how to orchestrate tools and prompts in a structured pipeline.\n\n## Core Concepts for This Use Case\n\nBefore we start building anything, let me break down these MCP primitives.\n\n**Tools** are basically functions that the client can invoke with typed arguments. The server executes them and sends back results. Pretty straightforward.\n\n**Resources** are static or dynamic data identified by URI. Clients read them on demand as text, JSON, or binary, whatever you need.\n\n**Prompts** are these reusable templates that generate messages for LLM conversations. Clients fetch them and render them with arguments.\n\nThe **stdio transport** is what we'll use here. Server and client communicate over standard input and output. Honestly, it's the simplest option for local development and subprocess integration.\n\nAnd then there's **schema negotiation**. The server advertises tool input schemas with JSON Schema and resource MIME types. Clients validate input and adapt automatically, which is really nice.\n\nIf you're still getting the hang of crafting effective templates and roles, you might want to see [best practices for prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4). It'll help you get consistent results from your MCP prompts.\n\n## Setup\n\nFirst things first, make sure you have Python 3\\.9 or later. I always create a clean virtual environment to avoid those annoying dependency conflicts. You'll need to decide whether you're running this in a notebook or in a terminal. If you're using a notebook, just make sure you can write files to the working directory.\n\nInstall the required packages in a notebook cell or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"mcp[cli]>=0.9.0\" anyio>=4.0.0 openai>=1.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gets you the MCP SDK, an async runtime, and an OpenAI client. I'd recommend pinning versions for reproducibility. If the installation fails, check your Python version and virtual environment first, then give it another shot. Oh, and if you're thinking about operating your stack outside managed services, you might want to consider [deploying a self\\-hosted LLM server](/article/how-to-run-a-self-hosted-llm-on-your-server-practical-guide-2025-2) to pair with your MCP server.\n\n## Using the Tool in Practice\n\n### Build the MCP Server\n\nLet's create a server that exposes two math tools, a static markdown resource, and a prompt template. I'm using stdio transport here because, well, it's simple.\n\nPick a filename for the server. I usually go with something like mcp\\_server.py. If you're working in a notebook, use whatever file writing utility your environment provides. Terminal users, just fire up your preferred editor.\n\nWrite the server to a file using a notebook cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile mcp_server.py\n# Purpose: Minimal MCP server exposing math tools, a static resource, and a prompt template over stdio.\n\nimport anyio\nfrom typing import Annotated\n\n# MCP server APIs\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import (\n    PromptMessage,\n    TextResourceContents,\n    Resource,\n    Prompt,\n)\n\n# Instantiate the MCP server with a unique name\nserver = Server(\"calc-server\")\n\n@server.tool()\nasync def add(a: Annotated[int, \"First integer\"], b: Annotated[int, \"Second integer\"]) -> int:\n    \"\"\"\n    Add two integers and return the sum.\n\n    Args:\n        a (int): First integer.\n        b (int): Second integer.\n\n    Returns:\n        int: The sum of a and b.\n    \"\"\"\n    # Simple addition; no edge cases for int\n    return a + b\n\n@server.tool()\nasync def subtract(a: Annotated[int, \"Minuend\"], b: Annotated[int, \"Subtrahend\"]) -> int:\n    \"\"\"\n    Subtract b from a and return the difference.\n\n    Args:\n        a (int): Minuend.\n        b (int): Subtrahend.\n\n    Returns:\n        int: The result of a - b.\n    \"\"\"\n    # Simple subtraction; no edge cases for int\n    return a - b\n\n# Resource: static markdown documentation\nDOCS_ID = \"docs/usage\"\nDOCS_CONTENT = \"\"\"# Calc Server Usage\n\nTools:\n- add(a: int, b: int): returns a + b\n- subtract(a: int, b: int): returns a - b\n\nPrompt:\n- math_helper(expression: string): step-by-step computation\n\"\"\"\n\n@server.resource(DOCS_ID, mime_type=\"text/markdown\")\nasync def read_docs() -> TextResourceContents:\n    \"\"\"\n    Return static markdown documentation for the server.\n\n    Returns:\n        TextResourceContents: Markdown-formatted usage documentation.\n    \"\"\"\n    # TextResourceContents includes text and optional annotations\n    return TextResourceContents(text=DOCS_CONTENT)\n\n@server.prompt(\"math_helper\")\nasync def math_helper(expression: Annotated[str, \"Math expression to compute\"]):\n    \"\"\"\n    Generate a prompt for step-by-step math computation.\n\n    Args:\n        expression (str): Math expression to compute.\n\n    Returns:\n        list[PromptMessage]: System and user prompt messages.\n    \"\"\"\n    # System prompt instructs the assistant to show work\n    system = PromptMessage(role=\"system\", content=\"You are a careful math assistant. Show your work.\")\n    # User prompt includes the expression to compute\n    user = PromptMessage(role=\"user\", content=f\"Compute the following expression step by step: {expression}\")\n    return [system, user]\n\nasync def main():\n    \"\"\"\n    Main entry point: runs the MCP server over stdio until EOF.\n    \"\"\"\n    # stdio transport: run until EOF\n    async with stdio_server() as (read_stream, write_stream):\n        await server.run(read_stream, write_stream)\n\nif __name__ == \"__main__\":\n    anyio.run(main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So what's happening here? The @server.tool() decorator registers async functions as tools. The client gets JSON Schema for each tool, which means it can validate arguments before calling. The @server.resource() decorator exposes static or dynamic data by URI. Clients read the content using the resource URI, and the server returns the body with a MIME type. The @server.prompt() decorator defines those reusable prompt templates I mentioned. A prompt can define named input variables, message roles, and a message sequence. The whole thing runs over stdio, reading requests from stdin and writing responses to stdout.\n\nAs you're building, keep these things in mind:\n\n* Each tool needs a clear JSON Schema for inputs. This is what helps clients validate before execution.\n* Each resource should return a stable URI and the correct content type. Otherwise clients won't render content correctly.\n* Each prompt should declare required arguments and message roles. Clients need to know what variables to supply.\n\n### Test the Server with a Client\n\nNow let's create a separate file for the client. Call it mcp\\_client.py or just reuse a notebook cell. The client will start the server as a subprocess and connect over stdio.\n\nWrite a client that connects to the server, lists capabilities, calls a tool, reads a resource, and fetches a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile mcp_client.py\n# Purpose: Async MCP client to test server tools, resources, and prompts over stdio.\n\nimport anyio\nfrom mcp.client.stdio import stdio_client\nfrom mcp.client.session import ClientSession\n\nasync def main():\n    \"\"\"\n    Connects to the MCP server, lists tools/resources/prompts, calls a tool, and fetches a prompt.\n\n    Raises:\n        Exception: If server connection or calls fail.\n    \"\"\"\n    # Launch the server as a subprocess and connect over stdio\n    async with stdio_client([\"python\", \"mcp_server.py\"]) as (read_stream, write_stream):\n        async with ClientSession(read_stream, write_stream) as session:\n            # List tools and print their names\n            tools = await session.list_tools()\n            print(\"Tools:\", [t.name for t in tools.tools])\n\n            # List resources and print their URIs\n            resources = await session.list_resources()\n            print(\"Resources:\", [r.uri for r in resources.resources])\n\n            # Read and print documentation resource if present\n            for r in resources.resources:\n                if r.uri.endswith(\"docs/usage\"):\n                    content = await session.read_resource(r.uri)\n                    # content.contents is a list of typed chunks (e.g., text, blob)\n                    for c in content.contents:\n                        if hasattr(c, \"text\"):\n                            print(\"Docs:\\n\", c.text)\n\n            # Call 'add' tool with arguments and print the result\n            result = await session.call_tool(\"add\", {\"a\": 5, \"b\": 7})\n            # result.content is list of output messages; pick first text\n            out = result.content[0].text if result.content else None\n            print(\"add(5,7) =\", out)\n\n            # Fetch prompt template and print its messages\n            prompts = await session.list_prompts()\n            print(\"Prompts:\", [p.name for p in prompts.prompts])\n            prompt = await session.get_prompt(\"math_helper\", {\"expression\": \"3*(4+2)\"})\n            print(\"Prompt messages:\", [(m.role, m.content) for m in prompt.messages])\n\nif __name__ == \"__main__\":\n    anyio.run(main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What this client does is launch the server as a subprocess, establish a session over stdio, and exercise all three primitives. It lists tools, resources, and prompts. Calls a math tool to confirm schema validation and execution work. Reads a markdown resource to confirm URI resolution and content types are right. And fetches a prompt and renders it with arguments. The stdio\\_client context manager handles all the process lifecycle and stream wiring stuff. If the server crashes, you'll see an error during capability discovery or the first call. Just check the server logs or print statements to figure out what went wrong.\n\n### Run and Evaluate\n\nTime to run the client and verify the server works end to end. If you're using a terminal, run the client from the same virtual environment. Notebook users, just run the cell that executes the client code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python mcp_client.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tools: ['add', 'subtract']\nResources: ['docs/usage']\nDocs:\n # Calc Server Usage\n\nTools:\n- add(a: int, b: int): returns a + b\n- subtract(a: int, b: int): returns a - b\n\nPrompt:\n- math_helper(expression: string): step-by-step computation\n\nadd(5,7) = 12\nPrompts: ['math_helper']\nPrompt messages: [('system', 'You are a careful math assistant. Show your work.'), ('user', 'Compute the following expression step by step: 3*(4+2)')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output tells us that:\n\n* The client successfully discovered tools, resources, and prompts during session setup.\n* The tool call returned a valid result that matched the expected schema.\n* The resource read returned the expected content with the correct MIME type.\n* The prompt rendered correctly with your input arguments.\n\n\nIf something fails, check your Python path, virtual environment, and file locations. Make sure your server file is actually executable in your environment. Windows users, confirm that your Python launcher is running the correct interpreter. macOS or Linux folks, check permissions and the working directory.\n\n### Optional: Add Resilience to Tool Calls\n\nFor production use, you really want to add a timeout and error handler around tool calls. This prevents a slow or failing tool from blocking the client. Plus you'll capture and surface meaningful error messages.\n\nCreate a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile helpers/safe_call.py\n# Purpose: Utility for safe, timeout-guarded MCP tool calls.\n\nfrom anyio import fail_after, WouldBlock\nimport logging\n\nasync def safe_call_tool(mcp, name, args, seconds=10):\n    \"\"\"\n    Call an MCP tool with a timeout and error handling.\n\n    Args:\n        mcp: MCP client session.\n        name (str): Tool name.\n        args (dict): Tool arguments.\n        seconds (int): Timeout in seconds.\n\n    Returns:\n        Tool call result or None if failed/timed out.\n\n    Raises:\n        None: All exceptions are caught and logged.\n    \"\"\"\n    try:\n        with fail_after(seconds):\n            return await mcp.call_tool(name, args)\n    except WouldBlock:\n        # Timeout occurred\n        logging.warning(f\"Tool call to {name} timed out after {seconds}s.\")\n        return None\n    except Exception as e:\n        # Log error and return None\n        logging.error(f\"Tool call {name} failed: {e}\")\n        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use safe\\_call\\_tool instead of calling session.call\\_tool directly. I'd start with a conservative timeout. Only expand it if you confirm that a tool actually needs more time. Always log errors and return structured error information to whoever's calling.\n\n## Troubleshooting\n\nLet me save you some time with common issues I've run into:\n\n**The client cannot discover tools**: Make sure the server process actually starts correctly. Check that the stdio transport isn't buffered or redirected somewhere weird. Confirm the working directory contains the server file.\n\n**JSON Schema validation errors**: Double\\-check that your tool input matches the schema. Look at required fields, types, and enum values. If your arguments are valid but the schema is too strict, update the tool schema.\n\n**Resource not found**: The resource URI needs to match exactly. If the server builds resource URIs dynamically, print the final URI and compare it to what the client is requesting.\n\n**Prompt rendering errors**: You need to pass all required variables. If your template variables change, update both the server prompt declaration and the client call.\n\n**Hanging calls**: Add timeouts using that resilience pattern I showed you. If a tool depends on external services, handle network retries and backoff inside the tool implementation itself.\n\n**Version conflicts**: Pin your dependencies and reinstall in a fresh virtual environment. Make sure the client and server are using compatible MCP SDK versions.\n\n## Conclusion\n\nSo there you have it. You've built a minimal MCP server that exposes tools, resources, and prompts over stdio, and you've verified it works with a test client. This pattern lets you define capabilities once and reuse them across any MCP\\-compatible application. You now have a repeatable way to standardize tool access without constantly rewriting code for each app.\n\n## Next Steps\n\nHere's what I'd tackle next:\n\n**Integrate with OpenAI function calling**. Convert MCP tools to OpenAI tool schemas and route calls from GPT\\-4 to your server. There's a separate guide for this. For deeper customization, check out the [step\\-by\\-step guide to fine\\-tuning large language models](/article/fine-tuning-large-language-models-a-step-by-step-guide-2025-6).\n\n**Add dynamic resources**: Instead of just static markdown, serve real\\-time data like database queries or API responses.\n\n**Explore other transports**: Try SSE or WebSocket for remote or browser\\-based clients.\n\n**Package and deploy**: Containerize your server and expose it through a network transport for multi\\-user access."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Model Context Protocol (MCP) Server in Python",
    "description": "Learn how to build an MCP server in Python to standardize and reuse AI tools, resources, and prompts across applications. This hands-on guide walks you through server setup, client testing, and GPT-4 chatbot integration for production-ready systems.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}