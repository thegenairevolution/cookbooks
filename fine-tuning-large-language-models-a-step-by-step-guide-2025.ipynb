{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Fine-tuning large language models: a step-by-step guide [2025]\n\n**Description:** Master full fine-tuning with Hugging Face: generate reliable datasets, configure seq2seq training, tune hyperparameters, and produce consistently formatted, higher-quality outputs.\n\n**ðŸ“– Read the full article:** [Fine-tuning large language models: a step-by-step guide [2025]](https://blog.thegenairevolution.com/article/fine-tuning-large-language-models-a-step-by-step-guide-2025-3)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So you've been working with an LLM and the first thing you tried was probably prompt engineering. You write clear prompts, follow the [Guidelines to Effective Prompting](https://thegenairevolution.com/harnessing-the-power-of-llm-apis-a-guide-to-effective-prompting/), try to be direct. For complex stuff, you ask the model to reason step by step. If you want a complete guide on [building production\\-ready LLM features with prompt engineering](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4), I put together something that covers pretty much everything.\n\nBut sometimes that's not enough. I've been there. So you try in\\-context learning. Like I explained in [The Magic of In\\-Context Learning](https://thegenairevolution.com/the-magic-of-in-context-learning-teach-your-llm-on-the-fly/), you basically add examples to help the model understand what you want. For more on [in\\-context learning techniques](/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3), check out my practical guide. Saved me countless hours.\n\nIf that still doesn't work, it's probably time for fine\\-tuning. This is where things get interesting. I covered this in [Fine\\-Tuning 101](https://thegenairevolution.com/fine-tuning-101-how-to-customize-llms-to-your-specific-needs/). Fine\\-tuning lets you customize the LLM for your specific use case. Depending on your resources and needs, you can do full fine\\-tuning or parameter\\-efficient fine\\-tuning. You might want to explore [approaches like LoRA](/article/parameter-efficient-fine-tuning-peft-with-lora-2025-hands-on-guide-2) to reduce costs while keeping performance strong. Actually, LoRA was a game\\-changer for some personal projects where GPU memory was tight.\n\nIn this cookbook, I'll focus on full fine\\-tuning. I'll walk you through how to apply it step by step. Let me show you what worked for me.\n\n## Setup\n\nLet's set up our environment. I'm following the same setup from [Running an LLM Locally on Your Own Server: A Practical Guide](https://thegenairevolution.com/running-an-llm-locally-on-your-own-server-a-practical-guide/). Check that post if you need detailed instructions. Getting the setup right saves so much headache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import the necessary packages\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig\n\n# Load the FLAN-T5 model\nmodel_name = \"google/flan-t5-base\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Few-shot learning\ninput_text = \"\"\"\nAnswer the following geography questions using the format shown in the context. \nAnswer with a single sentence containing the cityâ€™s name, country, population, and three famous landmarks. \n\nFollow the pattern below:\n\nQ: Tell me about Paris.  \nA: Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n\nQ: Describe New York.  \nA: New York is a city in the United States with a population of 8.5 million, known for landmarks such as the Statue of Liberty, Central Park, and Times Square.\n\nQ: What can you say about Tokyo?  \nA: Tokyo is a city in Japan with a population of 14 million, known for landmarks such as the Tokyo Tower, Shibuya Crossing, and the Meiji Shrine.\n\nQ: Tell me some information about Sydney.  \nA: Sydney is a city in Australia with a population of 5.3 million, known for landmarks such as the Sydney Opera House, the Harbour Bridge, and Bondi Beach.\n\nQ: Could you give me some details about Cairo?  \nA: Cairo is a city in Egypt with a population of 9.5 million, known for landmarks such as the Pyramids of Giza, the Sphinx, and the Egyptian Museum.\n\nNow, describe Vancouver in the same format.\n\"\"\"\n\n# Tokenize input\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n#  Generate response\noutputs = model.generate(inputs.input_ids, max_length=50)\n\n# Decode and print the ouput\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Vancouver is a city in Canada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When our task is requesting city information, including name, country, population, and landmarks, in\\-context learning hits its limit. We're not getting what we want. I spent hours trying different prompts before accepting we needed more. Time for fine\\-tuning.\n\n## Preparing the Training Data\n\nHere's what I'm going to do: use another LLM, a larger one, to create training data. I'll generate 100 labeled input\\-output pairs for fine\\-tuning, then see if that's enough.\n\nThis method is great for generating datasets quickly. Saves tons of time. But you need to make sure the data is accurate and diverse. I learned this the hard way when I didn't validate generated data properly. The model learned some weird patterns.\n\n### Advantages of Using an LLM\n\n* **Speed**: An LLM generates thousands of examples in minutes. I once needed 500 examples for a personal project. Got them done during lunch.\n* **Consistency**: The model ensures consistent formatting across your dataset. No more worrying about semicolons versus commas in example 47\\.\n* **Adaptability**: You can adjust prompts for more diverse outputs. Want different question phrasing? Add facts about different countries? Just tweak and regenerate.\n\n### Process Using GPT\\-4\n\n**1\\. Define a Template for the Desired Output**\n\nCreate a template for the question\\-answer pairs you want. For cities, include fields like name, country, population, landmarks. I start simple then refine based on what the model produces.\n\n**2\\. Provide Few\\-Shot Examples to the LLM**\n\nUse few\\-shot learning to guide the model. Start with manually written examples, prompt the model to generate more in the same style. Be specific about what you want.\n\n**3\\. Use the LLM to Generate Multiple Examples**\n\nWith your template and examples ready, the LLM can generate more pairs. Use API calls, loop over geographic entities, generate what you need. Actually, wait. Don't generate everything at once. I do batches of 20\\-30 to check quality.\n\n**4\\. Review and Refine the Data**\n\nLLMs produce structured outputs, but quality varies. You need to review for:\n\n* **Accuracy**: Check facts like capitals, populations, landmarks. GPT\\-4 once told me the Eiffel Tower was in Berlin. These things happen.\n* **Format Consistency**: All answers follow the template\n* **Diversity**: Include various questions and phrasing. You don't want 100 examples starting with \"Tell me about...\"\n\nTo improve quality, you might explore [techniques for building RAG systems and managing datasets](/article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it). These help curate and validate training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the necessary Python libraries\nfrom dotenv import load_dotenv, find_dotenv\nfrom openai import OpenAI\nimport json\nimport time\n\n# Load the OPENAI_API_KEY from local .env file\n_ = load_dotenv(find_dotenv())\n\n# Instantiate the OpenAI client\nclient = OpenAI()\n\n# List of cities for generating question-answer pairs\ncities = [\n    \"Paris\", \"Tokyo\", \"New York\", \"Sydney\", \"Cairo\", \"Rio de Janeiro\",\n    \"London\", \"Berlin\", \"Dubai\", \"Rome\", \"Beijing\", \"Bangkok\", \"Moscow\",\n    \"Toronto\", \"Los Angeles\", \"Cape Town\", \"Mumbai\", \"Seoul\", \"Buenos Aires\",\n    \"Istanbul\", \"Mexico City\", \"Jakarta\", \"Shanghai\", \"Lagos\", \"Madrid\",\n    \"Lisbon\", \"Stockholm\", \"Vienna\", \"Prague\", \"Warsaw\", \"Helsinki\", \"Oslo\",\n    \"Brussels\", \"Zurich\", \"Kuala Lumpur\", \"Singapore\", \"Manila\", \"Lima\",\n    \"Santiago\", \"BogotÃ¡\", \"Nairobi\", \"Havana\", \"San Francisco\", \"Chicago\",\n    \"Venice\", \"Florence\", \"Edinburgh\", \"Glasgow\", \"Dublin\", \"Athens\",\n    \"Melbourne\", \"Perth\", \"Hong Kong\", \"Doha\", \"Casablanca\", \"Tehran\",\n    \"Bucharest\", \"Munich\", \"Barcelona\", \"Kyoto\", \"Kolkata\", \"Amman\",\n    \"Lyon\", \"Nice\", \"Marseille\", \"Tel Aviv\", \"Jerusalem\", \"Geneva\", \n    \"Ho Chi Minh City\", \"Phnom Penh\", \"Yangon\", \"Colombo\", \"Riyadh\",\n    \"Abu Dhabi\", \"Addis Ababa\", \"Seville\", \"Bilbao\", \"Porto\", \"Bratislava\",\n    \"Ljubljana\", \"Tallinn\", \"Riga\", \"Vilnius\", \"Belgrade\", \"Sarajevo\",\n    \"Skopje\", \"Tirana\", \"Baku\", \"Yerevan\", \"Tashkent\", \"Almaty\", \"Ulaanbaatar\",\n    \"Karachi\", \"Islamabad\", \"Helsinki\", \"Chennai\", \"Kigali\", \"Antananarivo\",\n    \"Bangui\", \"San Juan\"\n]\n\n# Function to create a prompt for a specific city\ndef create_prompt(city):\n    return f\"\"\"\n    Your task is to provide question-and-answer pairs about cities following this format:\n    {{\n      \"input\": \"[A unique way to ask for a description of the city]\",\n      \"output\": \"[City Name] is a city in [Country] with a population of [Population], known for landmarks such as [Landmark 1], [Landmark 2], and [Landmark 3].\"\n    }}\n\n    Here are a few examples:\n    {{\n      \"input\": \"Tell me about Paris.\",\n      \"output\": \"Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\"\n    }}\n    {{\n      \"input\": \"Can you provide information on Tokyo?\",\n      \"output\": \"Tokyo is a city in Japan with a population of 37 million, known for landmarks such as the Tokyo Tower, Shibuya Crossing, and Meiji Shrine.\"\n    }}\n    {{\n      \"input\": \"What can you tell me about New York?\",\n      \"output\": \"New York is a city in the United States with a population of 8.4 million, known for landmarks such as the Statue of Liberty, Central Park, and Times Square.\"\n    }}\n\n    Now, generate a similar question-answer pair for the city {city}.\n    \"\"\"\n\n# Function to generate a Q&A pair using GPT-4 for a given city\ndef generate_city_qa(city):\n    prompt = create_prompt(city)\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.7,\n            max_tokens=200\n        )\n        return json.loads(response.choices[0].message.content.strip())\n    except Exception as e:\n        print(f\"Error generating data for {city}: {e}\")\n        return None\n        \n# Generate and save Q&A pairs incrementally to a JSONL file\nwith open(\"city_qna.jsonl\", \"w\") as f:\n    for i, city in enumerate(cities):\n        qa_pair = generate_city_qa(city)\n        if qa_pair:\n            f.write(json.dumps(qa_pair) + \"\\n\")\n            # Print the first 3\n            if i < 3:\n                print(qa_pair)\n            else:\n                print(\".\", end=\"\")\n        time.sleep(1)  # Add delay to manage rate limits\n\nprint(\"\\n\\nCity Q&A pairs saved to city_qna.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{'input': 'Can you describe the city of Paris to me?', 'output': 'Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.'}\n{'input': 'What should I know about Tokyo?', 'output': 'Tokyo is a city in Japan with a population of 37 million, known for landmarks such as the Tokyo Skytree, Senso-ji Temple, and the Imperial Palace.'}\n{'input': 'What do you know about New York?', 'output': 'New York is a city in the United States with a population of 8.4 million, known for landmarks such as the Empire State Building, Brooklyn Bridge, and Wall Street.'}\n.................................................................................................\n\nCity Q&A pairs saved to city_qna.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Fine\\-Tuning\n\nWe've prepared our training data, saved it as JSONL. Ready for fine\\-tuning. This dataset is the foundation for customizing our model. Next, we configure the environment, load parameters, and adapt the model for our tasks.\n\n### Load the Dataset from the JSONL File\n\nWe'll use Hugging Face datasets library to load our JSONL. Pretty straightforward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n\n# Load your dataset from the JSONL file\ndataset = load_dataset(\"json\", data_files=\"city_qna.jsonl\")\n\n# Check the dataset structure\nprint(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Generating train split: 0 examples [00:00, ? examples/s]\n{'input': 'Can you describe the city of Paris to me?', 'output': 'Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess the Data\n\nFlan\\-T5 is seq2seq, so we tokenize input and output appropriately. This part always takes me a minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n\n# Load the tokenizer for Flan-T5\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\n# Tokenize the dataset\ndef preprocess_data(examples):\n    # Extract inputs and outputs as lists from the dictionary\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n\n    # Tokenize inputs and outputs with padding and truncation\n    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n    labels = tokenizer(outputs, max_length=128, padding=\"max_length\", truncation=True).input_ids\n\n    # Replace padding token IDs with -100 to ignore them in the loss function\n    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n    model_inputs[\"labels\"] = labels\n\n    return model_inputs\n\n# Use the map function to apply the preprocessing to the whole dataset\ntokenized_dataset = dataset[\"train\"].map(preprocess_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Map: 0%| | 0/100 [00:00<?, ? examples/s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up the Model for Fine\\-Tuning\n\nFor a refresher on core concepts, check my guide on [understanding transformer architecture](/article/transformers-demystifying-the-magic-behind-large-language-models-2). Helps to know what's happening under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n\n# Load the Flan-T5 model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Training Arguments\n\nSet training parameters. Epochs, batch size, learning rate. Getting these right is half art, half science. I start conservative and adjust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/home/ubuntu/flan-t5-city-tuning\",  # Output directory\n    eval_strategy=\"no\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=1,  # Only keep the most recent checkpoint\n    logging_dir=\"./logs\",  # Directory for logs\n    logging_steps=10,\n    push_to_hub=False  # Set this to True if you want to push to Hugging Face Hub\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a Trainer and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer\n)\n\n# Start fine-tuning\ntrainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[39/39 02:59, Epoch 3/3]\nStep Training Loss\n10 1.516000\n20 1.537800\n30 1.412000\n\nTrainOutput(global_step=39, training_loss=1.4650684992472331, metrics={'train_runtime': 184.0522, 'train_samples_per_second': 1.63, 'train_steps_per_second': 0.212, 'total_flos': 51356801433600.0, 'train_loss': 1.4650684992472331, 'epoch': 3.0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the Model Qualitatively (Human Evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\nmodel_name = \"/home/ubuntu/flan-t5-city-tuning/checkpoint-39\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Few-shot learning\ninput_text = \"Describe the city of Vancouver\"\n\n# Tokenize input\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n#  Generate response\noutputs = model.generate(inputs.input_ids, max_length=50)\n\n# Decode and print the ouput\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Vancouver is a city in Canada with a population of 2.8 million, known for landmarks such as the Vancouver Bridge, Vancouver City Hall, and Vancouver International Airport."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! The model now provides exactly the format we wanted. Some information isn't entirely accurate, I'll admit, but the structure is much improved. This shows significant progress. Best part? We achieved this without any in\\-context learning. Pretty cool.\n\n## Conclusion\n\nI demonstrated how to fine\\-tune an LLM using a dataset generated with another LLM. We prepared training data, emphasized accuracy and diversity, walked through fine\\-tuning. I highlighted key advantages, particularly speed and consistency.\n\nOur fine\\-tuning showed meaningful improvement in response format. Yes, some factual errors persisted. But the structured output shows fine\\-tuning can significantly enhance a model's ability to meet requirements, even without in\\-context learning. That's a win.\n\nFine\\-tuning offers a powerful way to tailor models to specialized tasks. Makes your model more effective. But you need to review data and outputs for accuracy and diversity. I can't stress this enough. With the right approach, fine\\-tuning unlocks new precision and customization. Once you get the hang of it, it's another valuable tool in your toolkit."
      ]
    }
  ],
  "metadata": {
    "title": "Fine-tuning large language models: a step-by-step guide [2025]",
    "description": "Master full fine-tuning with Hugging Face: generate reliable datasets, configure seq2seq training, tune hyperparameters, and produce consistently formatted, higher-quality outputs.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}