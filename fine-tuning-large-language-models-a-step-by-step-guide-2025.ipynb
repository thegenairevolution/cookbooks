{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Fine-tuning large language models: a step-by-step guide [2025]\n\n**Description:** Master full fine-tuning with Hugging Face: generate reliable datasets, configure seq2seq training, tune hyperparameters, and produce consistently formatted, higher-quality outputs.\n\n**ðŸ“– Read the full article:** [Fine-tuning large language models: a step-by-step guide [2025]](https://blog.thegenairevolution.com/article/fine-tuning-large-language-models-a-step-by-step-guide-2025)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you start working with a large language model (LLM), the first thing you'll probably try is getting your use case to work with prompt engineering. You write clear prompts, follow the [Guidelines to Effective Prompting](https://thegenairevolution.com/harnessing-the-power-of-llm-apis-a-guide-to-effective-prompting/), and try to be as direct and unambiguous as you can. For complex problems, you ask the model to reason step by step. If you want an end\\-to\\-end guide on [building reliable, production\\-ready LLM features with effective prompt engineering](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4), I've put together a detailed walkthrough that covers everything.\n\nBut here's the thing. Sometimes that's just not enough. So you move on to in\\-context learning. As I explained in [The Magic of In\\-Context Learning](https://thegenairevolution.com/the-magic-of-in-context-learning-teach-your-llm-on-the-fly/), this basically means adding examples to help the model understand what you're trying to achieve. For a deeper look at [in\\-context learning techniques and when to use them](/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3), check out the practical guide I wrote on this.\n\nNow, if those steps still don't get you the results you need, it's probably time to consider fine\\-tuning. I covered this in detail in [Fine\\-Tuning 101](https://thegenairevolution.com/fine-tuning-101-how-to-customize-llms-to-your-specific-needs/). Fine\\-tuning lets you customize the LLM to handle your specific use case more effectively. Depending on what resources you have and how precise you need the model to be, you can either do full fine\\-tuning or parameter\\-efficient fine\\-tuning. You might also want to explore [parameter\\-efficient fine\\-tuning approaches like LoRA](/article/parameter-efficient-fine-tuning-peft-with-lora-2025-hands-on-guide-2) to reduce costs and memory requirements while keeping strong performance.\n\nIn this cookbook, I'm going to focus specifically on full fine\\-tuning. I'll walk you through exactly how to apply it to your task, step by step. You'll see how to put everything into practice and really get the most out of your model.\n\n## Setup\n\nLet's start by setting up our environment for this demonstration. I'm going to follow the same setup I outlined in [Running an LLM Locally on Your Own Server: A Practical Guide](https://thegenairevolution.com/running-an-llm-locally-on-your-own-server-a-practical-guide/). If you're not sure about any part of the process, feel free to go back to that post for the detailed instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import the necessary packages\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig\n\n# Load the FLAN-T5 model\nmodel_name = \"google/flan-t5-base\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Few-shot learning\ninput_text = \"\"\"\nAnswer the following geography questions using the format shown in the context. \nAnswer with a single sentence containing the cityâ€™s name, country, population, and three famous landmarks. \n\nFollow the pattern below:\n\nQ: Tell me about Paris.  \nA: Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n\nQ: Describe New York.  \nA: New York is a city in the United States with a population of 8.5 million, known for landmarks such as the Statue of Liberty, Central Park, and Times Square.\n\nQ: What can you say about Tokyo?  \nA: Tokyo is a city in Japan with a population of 14 million, known for landmarks such as the Tokyo Tower, Shibuya Crossing, and the Meiji Shrine.\n\nQ: Tell me some information about Sydney.  \nA: Sydney is a city in Australia with a population of 5.3 million, known for landmarks such as the Sydney Opera House, the Harbour Bridge, and Bondi Beach.\n\nQ: Could you give me some details about Cairo?  \nA: Cairo is a city in Egypt with a population of 9.5 million, known for landmarks such as the Pyramids of Giza, the Sphinx, and the Egyptian Museum.\n\nNow, describe Vancouver in the same format.\n\"\"\"\n\n# Tokenize input\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n#  Generate response\noutputs = model.generate(inputs.input_ids, max_length=50)\n\n# Decode and print the ouput\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Vancouver is a city in Canada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, when our task is to request information about cities, including the city's name, country, population, and three famous landmarks, in\\-context learning has reached its limit. We're not getting the results we want. So now it's time to move on to fine\\-tuning and see if we can do better. Let's begin with preparing the training data.\n\n## Preparing the Training Data\n\nHere's what I'm going to do: use another LLM, a larger one, to create the training data we need. For this example, I'll generate 100 labeled input\\-output pairs for fine\\-tuning our model, and then we'll see if that's enough. This method is particularly useful for generating datasets quickly. It saves a lot of time compared to manual data collection. But you really need to make sure the generated data is accurate, well\\-structured, and diverse enough to support robust training.\n\n### Advantages of Using an LLM\n\n* **Speed**: An LLM can generate thousands of examples in just minutes. This significantly speeds up dataset creation.\n* **Consistency**: The model ensures consistent formatting across your entire dataset, which reduces variability and the need for manual corrections.\n* **Adaptability**: You can easily adjust your prompts to generate more diverse or creative outputs. Want to vary question phrasing? Add facts about different countries? It's all possible.\n\n### Process Using GPT\\-4\n\n**1\\. Define a Template for the Desired Output**\n\nFirst, you need to create a template for the type of question\\-answer pairs you want the model to generate. For instance, if your task involves cities, you might include fields like the city's name, country, population, and key landmarks.\n\n**2\\. Provide Few\\-Shot Examples to the LLM**\n\nUse few\\-shot learning to guide the model in generating examples that match your desired format. Start with a small set of manually written examples, like we did earlier, and prompt the model to generate additional pairs in the same style.\n\n**3\\. Use the LLM to Generate Multiple Examples**\n\nOnce you have your template and few\\-shot examples ready, the LLM can generate more question\\-answer pairs. You could prompt it to produce data for various countries or cities. By using API calls and looping over a list of geographic entities, you can generate as many examples as you need.\n\n**4\\. Review and Refine the Data**\n\nWhile LLMs can produce structured and informative outputs, the quality and accuracy can vary. Actually, it's essential to review or validate the generated data to ensure:\n\n* **Accuracy**: Facts like capitals, populations, and landmarks are correct\n* **Format Consistency**: All answers follow the defined template\n* **Diversity**: The dataset includes a variety of questions and phrasing to enhance task coverage and dataset diversity\n\n\nTo further improve quality and coverage, you might want to explore [techniques for building robust retrieval\\-augmented generation (RAG) systems and managing high\\-quality datasets](/article/rag-101-build-an-index-run-semantic-search-and-use-langchain-to-automate-it). These approaches help you curate, validate, and retrieve the most relevant context for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the necessary Python libraries\nfrom dotenv import load_dotenv, find_dotenv\nfrom openai import OpenAI\nimport json\nimport time\n\n# Load the OPENAI_API_KEY from local .env file\n_ = load_dotenv(find_dotenv())\n\n# Instantiate the OpenAI client\nclient = OpenAI()\n\n# List of cities for generating question-answer pairs\ncities = [\n    \"Paris\", \"Tokyo\", \"New York\", \"Sydney\", \"Cairo\", \"Rio de Janeiro\",\n    \"London\", \"Berlin\", \"Dubai\", \"Rome\", \"Beijing\", \"Bangkok\", \"Moscow\",\n    \"Toronto\", \"Los Angeles\", \"Cape Town\", \"Mumbai\", \"Seoul\", \"Buenos Aires\",\n    \"Istanbul\", \"Mexico City\", \"Jakarta\", \"Shanghai\", \"Lagos\", \"Madrid\",\n    \"Lisbon\", \"Stockholm\", \"Vienna\", \"Prague\", \"Warsaw\", \"Helsinki\", \"Oslo\",\n    \"Brussels\", \"Zurich\", \"Kuala Lumpur\", \"Singapore\", \"Manila\", \"Lima\",\n    \"Santiago\", \"BogotÃ¡\", \"Nairobi\", \"Havana\", \"San Francisco\", \"Chicago\",\n    \"Venice\", \"Florence\", \"Edinburgh\", \"Glasgow\", \"Dublin\", \"Athens\",\n    \"Melbourne\", \"Perth\", \"Hong Kong\", \"Doha\", \"Casablanca\", \"Tehran\",\n    \"Bucharest\", \"Munich\", \"Barcelona\", \"Kyoto\", \"Kolkata\", \"Amman\",\n    \"Lyon\", \"Nice\", \"Marseille\", \"Tel Aviv\", \"Jerusalem\", \"Geneva\", \n    \"Ho Chi Minh City\", \"Phnom Penh\", \"Yangon\", \"Colombo\", \"Riyadh\",\n    \"Abu Dhabi\", \"Addis Ababa\", \"Seville\", \"Bilbao\", \"Porto\", \"Bratislava\",\n    \"Ljubljana\", \"Tallinn\", \"Riga\", \"Vilnius\", \"Belgrade\", \"Sarajevo\",\n    \"Skopje\", \"Tirana\", \"Baku\", \"Yerevan\", \"Tashkent\", \"Almaty\", \"Ulaanbaatar\",\n    \"Karachi\", \"Islamabad\", \"Helsinki\", \"Chennai\", \"Kigali\", \"Antananarivo\",\n    \"Bangui\", \"San Juan\"\n]\n\n# Function to create a prompt for a specific city\ndef create_prompt(city):\n    return f\"\"\"\n    Your task is to provide question-and-answer pairs about cities following this format:\n    {{\n      \"input\": \"[A unique way to ask for a description of the city]\",\n      \"output\": \"[City Name] is a city in [Country] with a population of [Population], known for landmarks such as [Landmark 1], [Landmark 2], and [Landmark 3].\"\n    }}\n\n    Here are a few examples:\n    {{\n      \"input\": \"Tell me about Paris.\",\n      \"output\": \"Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\"\n    }}\n    {{\n      \"input\": \"Can you provide information on Tokyo?\",\n      \"output\": \"Tokyo is a city in Japan with a population of 37 million, known for landmarks such as the Tokyo Tower, Shibuya Crossing, and Meiji Shrine.\"\n    }}\n    {{\n      \"input\": \"What can you tell me about New York?\",\n      \"output\": \"New York is a city in the United States with a population of 8.4 million, known for landmarks such as the Statue of Liberty, Central Park, and Times Square.\"\n    }}\n\n    Now, generate a similar question-answer pair for the city {city}.\n    \"\"\"\n\n# Function to generate a Q&A pair using GPT-4 for a given city\ndef generate_city_qa(city):\n    prompt = create_prompt(city)\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.7,\n            max_tokens=200\n        )\n        return json.loads(response.choices[0].message.content.strip())\n    except Exception as e:\n        print(f\"Error generating data for {city}: {e}\")\n        return None\n        \n# Generate and save Q&A pairs incrementally to a JSONL file\nwith open(\"city_qna.jsonl\", \"w\") as f:\n    for i, city in enumerate(cities):\n        qa_pair = generate_city_qa(city)\n        if qa_pair:\n            f.write(json.dumps(qa_pair) + \"\\n\")\n            # Print the first 3\n            if i < 3:\n                print(qa_pair)\n            else:\n                print(\".\", end=\"\")\n        time.sleep(1)  # Add delay to manage rate limits\n\nprint(\"\\n\\nCity Q&A pairs saved to city_qna.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{'input': 'Can you describe the city of Paris to me?', 'output': 'Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.'}\n{'input': 'What should I know about Tokyo?', 'output': 'Tokyo is a city in Japan with a population of 37 million, known for landmarks such as the Tokyo Skytree, Senso-ji Temple, and the Imperial Palace.'}\n{'input': 'What do you know about New York?', 'output': 'New York is a city in the United States with a population of 8.4 million, known for landmarks such as the Empire State Building, Brooklyn Bridge, and Wall Street.'}\n.................................................................................................\n\nCity Q&A pairs saved to city_qna.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Fine\\-Tuning\n\nNow that we've prepared our training data and saved it in a JSONL file, we're ready to move on to the actual fine\\-tuning process. This dataset is going to serve as the foundation for customizing the model to meet our specific needs. With the data ready, the next step is configuring the fine\\-tuning environment, loading the training parameters, and beginning the process of adapting the model to perform better on our targeted tasks.\n\n### Load the Dataset from the JSONL File\n\nWe'll use the Hugging Face datasets library to load our JSONL file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n\n# Load your dataset from the JSONL file\ndataset = load_dataset(\"json\", data_files=\"city_qna.jsonl\")\n\n# Check the dataset structure\nprint(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Generating train split: 0 examples [00:00, ? examples/s]\n{'input': 'Can you describe the city of Paris to me?', 'output': 'Paris is a city in France with a population of 2.1 million, known for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess the Data\n\nSince Flan\\-T5 is a seq2seq model, we need to tokenize both the input and output appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n\n# Load the tokenizer for Flan-T5\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\n# Tokenize the dataset\ndef preprocess_data(examples):\n    # Extract inputs and outputs as lists from the dictionary\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n\n    # Tokenize inputs and outputs with padding and truncation\n    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n    labels = tokenizer(outputs, max_length=128, padding=\"max_length\", truncation=True).input_ids\n\n    # Replace padding token IDs with -100 to ignore them in the loss function\n    labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n    model_inputs[\"labels\"] = labels\n\n    return model_inputs\n\n# Use the map function to apply the preprocessing to the whole dataset\ntokenized_dataset = dataset[\"train\"].map(preprocess_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Map: 0%| | 0/100 [00:00<?, ? examples/s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up the Model for Fine\\-Tuning\n\nIf you want a quick refresher on the core building blocks, take a look at my guide on [understanding the transformer architecture that powers models like Flan\\-T5](/article/transformers-demystifying-the-magic-behind-large-language-models-2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n\n# Load the Flan-T5 model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Training Arguments\n\nWe need to set the training parameters. Things like the number of epochs, batch size, and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/home/ubuntu/flan-t5-city-tuning\",  # Output directory\n    eval_strategy=\"no\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=1,  # Only keep the most recent checkpoint\n    logging_dir=\"./logs\",  # Directory for logs\n    logging_steps=10,\n    push_to_hub=False  # Set this to True if you want to push to Hugging Face Hub\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a Trainer and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer\n)\n\n# Start fine-tuning\ntrainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[39/39 02:59, Epoch 3/3]\nStep Training Loss\n10 1.516000\n20 1.537800\n30 1.412000\n\nTrainOutput(global_step=39, training_loss=1.4650684992472331, metrics={'train_runtime': 184.0522, 'train_samples_per_second': 1.63, 'train_steps_per_second': 0.212, 'total_flos': 51356801433600.0, 'train_loss': 1.4650684992472331, 'epoch': 3.0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the Model Qualitatively (Human Evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\nmodel_name = \"/home/ubuntu/flan-t5-city-tuning/checkpoint-39\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Few-shot learning\ninput_text = \"Describe the city of Vancouver\"\n\n# Tokenize input\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n#  Generate response\noutputs = model.generate(inputs.input_ids, max_length=50)\n\n# Decode and print the ouput\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Vancouver is a city in Canada with a population of 2.8 million, known for landmarks such as the Vancouver Bridge, Vancouver City Hall, and Vancouver International Airport."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! Now the model provides exactly the format we were aiming for. Some of the information isn't entirely accurate, I'll admit, but the structure is much improved and aligns closely with what we expected. This result demonstrates significant progress. And the best part? We achieved this without any in\\-context learning at all.\n\n## Conclusion\n\nIn this post, I demonstrated how to fine\\-tune a large language model using a structured dataset generated with the help of another LLM. We started by preparing our training data, emphasized why accuracy and diversity matter, and walked through the entire fine\\-tuning process. Along the way, I highlighted key advantages, particularly the speed and consistency that LLMs offer when generating datasets.\n\nThe results of our fine\\-tuning showed meaningful improvement in the model's ability to provide responses in the desired format. Yes, some factual errors persisted. But the structured output shows that fine\\-tuning can significantly enhance a model's ability to meet specific use case requirements, even without in\\-context learning.\n\nFine\\-tuning offers a powerful way to tailor a model to specialized tasks. It makes your model more effective and responsive. But you really need to carefully review both the data and outputs to ensure factual correctness and diversity. With the right approach, fine\\-tuning can unlock new levels of precision and customization, taking your LLM to the next level."
      ]
    }
  ],
  "metadata": {
    "title": "Fine-tuning large language models: a step-by-step guide [2025]",
    "description": "Master full fine-tuning with Hugging Face: generate reliable datasets, configure seq2seq training, tune hyperparameters, and produce consistently formatted, higher-quality outputs.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}