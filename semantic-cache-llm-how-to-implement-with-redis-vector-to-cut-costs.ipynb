{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n\n**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n\n**ðŸ“– Read the full article:** [Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs](https://blog.thegenairevolution.com/article/semantic-cache-llm-how-to-implement-with-redis-vector-to-cut-costs)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This MattersMost LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantlyâ€”no LLM call required.\nThis guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a FastAPI microservice with a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n**What you'll build:**\n<ul><li>A Redis HNSW vector index for semantic similarity search\n</li><li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n</li><li>A FastAPI endpoint to serve cached or fresh LLM answers\n</li><li>A demo script to validate cache hit rates and latency improvements\n</li></ul>**Prerequisites:**\n<ul><li>Python 3.9+\n</li><li>Redis Stack (local via Docker or managed Redis Cloud)\n</li><li>OpenAI API key\n</li><li>Basic familiarity with embeddings and vector search\n</li></ul>If you're using Google Colab or a cloud notebook, connect to a managed Redis Stack instance (e.g., Redis Cloud) instead of running Docker locally.\nFor a deeper understanding of how LLMs manage memory and the concept of context rot, see our article on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/context-rot-why-llms-forget-as-their-memory-grows\">why LLMs \"forget\" as their memory grows</a>.\n<hr>## How It Works (High-Level Overview)**The paraphrase problem:**<br>Users ask the same question in many ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as different keys.\n**The embedding advantage:**<br>Embeddings map text into a high-dimensional vector space where semantically similar phrases cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses.\n**Why Redis Vector:**<br>Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. It combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilitiesâ€”ideal for production caching.\n**Architecture:**\n<ol><li>Normalize the user query (lowercase, strip volatile patterns like timestamps)\n</li><li>Generate an embedding for the normalized query\n</li><li>Search the Redis HNSW index for the nearest cached embedding\n</li><li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n</li><li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n</li></ol><hr>## Setup & Installation### Option 1: Managed Redis (Recommended for Notebooks)Sign up for a free Redis Cloud account at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://redis.com/try-free\">redis.com/try-free</a> and create a Redis Stack database. Copy the connection URL.\nIn your notebook or terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\" # Replace with your actual Redis URL\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" # OpenAI API key\nos.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\" # OpenAI embedding model to use\nos.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\" # OpenAI chat model to use\nos.environ[\"SIMILARITY_THRESHOLD\"] = \"0.10\" # Cosine similarity threshold for cache hit\nos.environ[\"TOP_K\"] = \"5\" # Number of nearest neighbors to retrieve in vector search\nos.environ[\"CACHE_TTL_SECONDS\"] = \"86400\" # Time-to-live for cache entries in seconds (1 day)\nos.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\" # Namespace for cache keys in Redis\nos.environ[\"CORPUS_VERSION\"] = \"v1\" # Version of the underlying data corpus (for cache invalidation)\nos.environ[\"TEMPERATURE\"] = \"0.2\" # Temperature parameter for the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Local Redis with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a `.env` file:\n<pre><code>REDIS_URL=redis://localhost:6379\nOPENAI_API_KEY=sk-...\nEMBEDDING_MODEL=text-embedding-3-small\nCHAT_MODEL=gpt-4o-mini\nSIMILARITY_THRESHOLD=0.10\nTOP_K=5\nCACHE_TTL_SECONDS=86400\nCACHE_NAMESPACE=sc:v1:\nCORPUS_VERSION=v1\nTEMPERATURE=0.2\n</code></pre>Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy fastapi uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>## Step-by-Step Implementation### Step 1: Create the Redis HNSW IndexThe index stores embeddings and metadata for cached responses. We use HNSW for fast approximate nearest neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport redis\nfrom dotenv import load_dotenv\n\n<p>load_dotenv()</p>\n<p>r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))</p>\n<p>INDEX = \"sc_idx\"\nPREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\nDIM = 1536  # Dimension for text-embedding-3-small\nM = 16  # HNSW graph connectivity\nEF_CONSTRUCTION = 200  # HNSW construction quality</p>\n<p>def create_index():\n    try:\n        r.execute_command(\"FT.INFO\", INDEX)\n        print(\"Index already exists.\")\n        return\n    except redis.ResponseError:\n        pass</p>\n<pre><code># Create index with vector field and metadata tags\ncmd = [\n    \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n    \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n    \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n    \"SCHEMA\",  # Define the schema of the index\n    \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n    \"model\", \"TAG\",  # Tag field for the LLM model used\n    \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n    \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n    \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n    \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n    \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n    \"response\", \"TEXT\",  # Text field for the LLM's response\n    \"user_question\", \"TEXT\", # Text field for the original user question\n    \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n    \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n    \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n    \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n    \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n    \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n]\nr.execute_command(*cmd)\nprint(\"Index created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "create_index()</code></pre><p style=\"text-align: left;\">**Validation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\nprint(\"Index info:\", info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see `num_docs: 0` initially.\n<hr>### Step 2: Normalize Queries for Stable Cache KeysCanonicalization removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\nimport hashlib</p>\n<h1>Note: Normalization adequacy depends on expected query variations and embedding model robustness.</h1>\n<p>VOLATILE_PATTERNS = [\n    # ISO timestamps and variations\n    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n    # UUID v4\n    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n    # Long IDs (6+ digits)\n    r\"\\b\\d{6,}\\b\",\n    # Email addresses (often contain volatile parts or personally identifiable info)\n    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\\b\",\n]</p>\n<p>def canonicalize(text: str) -> str:\n    t = text.strip().lower()\n    for pat in VOLATILE_PATTERNS:\n        t = re.sub(pat, \" \", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t</p>\n<p>def sha256(s: str) -> str:\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()</p>\n<p>def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n    # Unique hash for cache scope including all parameters\n    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Test:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\nq2 = \"what is our refund policy on 2025-01-20?\"\nprint(canonicalize(q1))\nprint(canonicalize(q2))</p>\n<h1>Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>### Step 3: Initialize Clients and Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np</h1>\n<p>from openai import OpenAI</p>\n<p>client = OpenAI()</p>\n<p>EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\nCHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\nTHRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\nTOP_K = int(os.getenv(\"TOP_K\", 5))\nTTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\nNS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\nCORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\nTEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))</p>\n<p>def embed(text: str) -> np.ndarray:\n    # Generate embedding and normalize for cosine distance\n    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n    vec = np.array(e.data[0].embedding, dtype=np.float32)\n    norm = np.linalg.norm(vec)\n    return vec / max(norm, 1e-12)</p>\n<p>def to_bytes(vec: np.ndarray) -> bytes:\n    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>### Step 4: Implement Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_vec = embed(\"hello world\")\nprint(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")</p>\n<h1>Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>### Step 5: Build the Cache Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time</h1>\n<p>from typing import Optional, Dict, Any, Tuple</p>\n<p>def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n    # Perform KNN search with EF_RUNTIME parameter\n    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n    q = f\"*=>[KNN {TOP_K} @vector $vec EF_RUNTIME $ef_runtime AS score]\"\n    try:\n        res = r.execute_command(\n            \"FT.SEARCH\", INDEX,\n            q, \"PARAMS\", str(len(params)), *params,\n            \"SORTBY\", \"score\", \"ASC\",\n            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"user_question\", \"score\",\n            \"DIALECT\", \"2\"\n        )\n    except redis.RedisError:\n        return None</p>\n<pre><code>total = res[0] if res else 0\nif total < 1:\n    return None\n\ndoc_id = res[1]\nfields = res[2]\nf = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n     fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n     for i in range(0, len(fields), 2)}\n\ntry:\n    distance = float(f[\"score\"])\nexcept Exception:\n    distance = 1.0\n\nreturn doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance</code></pre><hr><h3 style=\"text-align: left;\">Step 6: Add Metrics Tracking</h3><pre><code class=\"language-python\">import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from typing import Optional, Dict, Any, Tuple\n\n<p>def sys_hash(system_prompt: str) -> str:\n    return sha256(system_prompt.strip())</p>\n<p>def key(doc_id_hash: str) -> str:\n    return f\"{NS}{doc_id_hash}\"</p>\n<p>def metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n    try:\n        if f.get(\"model\") != model: return False\n        if f.get(\"sys_hash\") != sys_h: return False\n        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n        if f.get(\"corpus_version\") != corpus: return False\n        return True\n    except Exception:\n        return False</p>\n<p>def chat_call(system_prompt: str, user_prompt: str):\n    t0 = time.perf_counter()\n    resp = client.chat.completions.create(\n        model=CHAT_MODEL,\n        temperature=TEMPERATURE,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n    )\n    latency_ms = (time.perf_counter() - t0) * 1000\n    content = resp.choices[0].message.content\n    usage = getattr(resp, \"usage\", None)\n    return content, latency_ms, usage</p>\n<p>def cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n    # Start timing the cache lookup process\n    t0 = time.perf_counter()</p>\n<pre><code># Generate hashes and canonicalize prompt for consistent caching\nsp_hash = sys_hash(system_prompt)\nprompt_norm = canonicalize(user_prompt)\np_hash = sha256(prompt_norm)\n\n# Generate embedding for the normalized prompt\nqvec = embed(prompt_norm)\n\n# Perform vector search in Redis to find similar cached entries\nres = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n\n# Check if a cached result was found and meets criteria\nif res:\n    doc_id, fields, distance = res\n    # Check if the semantic distance is within the threshold and metadata matches\n    if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n        try:\n            # If cache hit, update the last hit timestamp in Redis\n            r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n        except redis.RedisError:\n            # Handle potential Redis errors during update\n            pass\n        # Return the cached response with details\n        return {\n            \"source\": \"cache\",\n            \"response\": fields[\"response\"],\n            \"distance\": distance,\n            \"latency_ms\": (time.perf_counter() - t0) * 1000,\n            \"user_question\": fields.get(\"user_question\"), # Include user_question in cache hit response\n        }\n\n# If no cache hit, call the LLM to generate a new response\ncontent, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n\n# Generate a unique key for the new cache entry based on prompt scope\ndoc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\ndoc_key = key(doc_scope)\n\n# Prepare data to be cached\ntry:\n    mapping = {\n        \"prompt_hash\": p_hash,\n        \"model\": CHAT_MODEL,\n        \"sys_hash\": sp_hash,\n        \"corpus_version\": CORPUS_VERSION,\n        \"temperature\": TEMPERATURE,\n        \"created_at\": time.time(),\n        \"last_hit_at\": time.time(),\n        \"response\": content,\n        \"user_question\": user_prompt,\n        \"vector\": to_bytes(qvec),\n    }\n    # Use a Redis pipeline for atomic HSET and EXPIRE commands\n    pipe = r.pipeline(transaction=True)\n    pipe.hset(doc_key, mapping=mapping)\n    pipe.expire(doc_key, int(TTL))\n    pipe.execute()\nexcept redis.RedisError:\n    # Handle potential Redis errors during caching\n    pass\n\n# Return the LLM-generated response with details\nreturn {\n    \"source\": \"llm\",\n    \"response\": content,\n    \"distance\": None,\n    \"latency_ms\": llm_latency_ms,\n    \"usage\": {\n        \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n        \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n        \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n    },\n    \"user_question\": user_prompt, # Include user_question in LLM response\n}</code></pre><hr>### Step 7: Build the FastAPI Service<pre><code class=\"language-python\">import statistics\n</code></pre>\n<p>class Metrics:\n    def **init**(self):\n        self.hits = 0\n        self.misses = 0\n        self.cache_latencies = []\n        self.llm_latencies = []</p>\n<pre><code>def record(self, result):\n    if result[\"source\"] == \"cache\":\n        self.hits += 1\n        self.cache_latencies.append(result[\"latency_ms\"])\n    else:\n        self.misses += 1\n        self.llm_latencies.append(result[\"latency_ms\"])\n\ndef snapshot(self):\n    def safe_percentile(vals, p):\n        if not vals:\n            return None\n        sorted_vals = sorted(vals)\n        idx = int(len(sorted_vals) * p / 100) - 1\n        return sorted_vals[max(0, idx)]\n\n    return {\n        \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n        \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n        \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n        \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n        \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n    }\n</code></pre>\nmetrics = Metrics()\n\n<p>def answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH):\n    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold)\n    metrics.record(res)\n    return res</code></pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\nfrom pydantic import BaseModel</p>\n<p>app = FastAPI()</p>\n<p>class Query(BaseModel):\n    system_prompt: str\n    user_prompt: str\n    ef_runtime: int | None = 100</p>\n<p>@app.post(\"/semantic-cache/answer\")\ndef semantic_answer(q: Query):\n    res = answer(q.system_prompt, q.user_prompt, ef_runtime=q.ef_runtime or 100)\n    return res</p>\n<p>@app.get(\"/semantic-cache/metrics\")\ndef get_metrics():\n    return metrics.snapshot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Run the service:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport os\nimport redis\nimport time\nimport re\nimport hashlib\nimport numpy as np\nfrom openai import OpenAI\nimport statistics\nfrom dotenv import load_dotenv\nfrom typing import Optional, Dict, Any, Tuple # Import Optional, Dict, Any, Tuple</p>\n<h1>Load environment variables (assuming .env or environment variables are set)</h1>\n<p>load_dotenv()</p>\n<h1>Define constants from environment variables - Ensure these are defined before app is created</h1>\n<p>EMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\nCHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\nTHRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\nTOP_K = int(os.getenv(\"TOP_K\", 5))\nTTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\nNS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\nCORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\nTEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\nINDEX = \"sc_idx\" # Define INDEX here as it's used in create_index</p>\n<h1>Initialize Redis client - Ensure REDIS_URL is set in environment variables</h1>\n<p>try:\n    r = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n    # Check connection\n    r.ping()\n    print(\"Connected to Redis successfully!\")\nexcept redis.exceptions.ConnectionError as e:\n    print(f\"Could not connect to Redis: {e}\")\n    r = None # Set r to None if connection fails</p>\n<h1>Define HNSW parameters (should match index creation)</h1>\n<p>DIM = 1536\nM = 16\nEF_CONSTRUCTION = 200</p>\n<h1>--- Index Creation Function (Moved from earlier cell) ---</h1>\n<h1>This function should ideally be run once during setup, not every time the app starts</h1>\n<h1>For this notebook demo, we include it, but in a production app, index creation</h1>\n<h1>is usually handled separately.</h1>\n<p>def create_index():\n    if not r: return # Skip if Redis connection failed\n    try:\n        r.execute_command(\"FT.INFO\", INDEX)\n        print(\"Index already exists.\")\n        return\n    except redis.ResponseError:\n        pass</p>\n<pre><code>cmd = [\n    \"FT.CREATE\", INDEX,\n    \"ON\", \"HASH\",\n    \"PREFIX\", \"1\", NS, # Use NS here as defined from env var\n    \"SCHEMA\",\n    \"prompt_hash\", \"TAG\",\n    \"model\", \"TAG\",\n    \"sys_hash\", \"TAG\",\n    \"corpus_version\", \"TAG\",\n    \"temperature\", \"NUMERIC\",\n    \"created_at\", \"NUMERIC\",\n    \"last_hit_at\", \"NUMERIC\",\n    \"response\", \"TEXT\",\n    \"user_question\", \"TEXT\",\n    \"vector\", \"VECTOR\", \"HNSW\", \"10\",\n    \"TYPE\", \"FLOAT32\",\n    \"DIM\", str(DIM),\n    \"DISTANCE_METRIC\", \"COSINE\",\n    \"M\", str(M),\n    \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),\n]\ntry:\n    r.execute_command(*cmd)\n    print(\"Index created.\")\nexcept redis.RedisError as e:\n    print(f\"Error creating index: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Call create index when the app file is written/imported\ncreate_index()\n\n# --- Normalization Functions (Moved from earlier cell) ---\n<p>VOLATILE_PATTERNS = [\n    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\",\n    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n    r\"\\b\\d{6,}\\b\",\n    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\\b\",\n]</p>\n<p>def canonicalize(text: str) -> str:\n    t = text.strip().lower()\n    for pat in VOLATILE_PATTERNS:\n        t = re.sub(pat, \" \", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t</p>\n<p>def sha256(s: str) -> str:\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()</p>\n<p>def scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n    return sha256(payload)</p>\n<p>def sys_hash(system_prompt: str) -> str:\n    return sha256(system_prompt.strip())</p>\n<p>def key(doc_id_hash: str) -> str:\n    return f\"{NS}{doc_id_hash}\"</p>\n# --- OpenAI Client & Embedding Function (Moved from earlier cell) ---\n# Initialize OpenAI client - Ensure OPENAI_API_KEY is set in environment variables\n<p>try:\n    client = OpenAI()\n    # Optional: check if client can connect/authenticate\n    # client.models.list()\n    print(\"OpenAI client initialized.\")\nexcept Exception as e:\n    print(f\"Error initializing OpenAI client: {e}\")\n    client = None # Set client to None if initialization fails</p>\n<p>def embed(text: str) -> np.ndarray:\n    if not client: raise ConnectionError(\"OpenAI client not initialized.\")\n    # Generate embedding and normalize for cosine distance\n    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n    vec = np.array(e.data[0].embedding, dtype=np.float32)\n    norm = np.linalg.norm(vec)\n    return vec / max(norm, 1e-12)</p>\n<p>def to_bytes(vec: np.ndarray) -> bytes:\n    return vec.astype(np.float32).tobytes()</p>\n# --- Vector Search Function (Moved from earlier cell) ---\n<p>def vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n    if not r: return None # Skip if Redis connection failed\n    # Perform KNN search with EF_RUNTIME parameter\n    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n    # Correct query syntax: Use $vec for vector parameter in KNN search\n    q = f\"*=>[KNN {TOP_K} @vector $vec EF_RUNTIME $ef_runtime AS score]\"\n    try:\n        res = r.execute_command(\n            \"FT.SEARCH\", INDEX,\n            q, \"PARAMS\", str(len(params)), *params,\n            \"SORTBY\", \"score\", \"ASC\",\n            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"user_question\", \"score\",\n            \"DIALECT\", \"2\"\n        )\n    except redis.RedisError as e:\n        print(f\"Redis search error: {e}\")\n        return None</p>\n<pre><code>total = res[0] if res else 0\nif total < 1:\n    return None\n\n# Process search results\n# The result format is [total_results, doc1_id, [field1, value1, field2, value2, ...], doc2_id, [...], ...]\n# We expect the first result to be the best match\nif len(res) > 1:\n    doc_id = res[1]\n    fields = res[2]\n    # Decode bytes to string for keys and values where appropriate\n    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n         for\n</code></pre>"
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}