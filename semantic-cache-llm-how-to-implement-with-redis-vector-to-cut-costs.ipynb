{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs\n\n**Description:** Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.\n\n**ðŸ“– Read the full article:** [Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs](https://blog.thegenairevolution.com/article/semantic-cache-llm-how-to-implement-with-redis-vector-to-cut-costs-3)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most LLM applications waste money and time answering the same question phrased slightly differently. A semantic cache solves this by recognizing when a new query is semantically similar to a previous one and returning the cached response instantly, eliminating the need for an LLM call.\n\nThis guide walks you through building a production-grade semantic cache using embeddings and Redis Vector. You'll create a Redis-backed semantic cache, complete with thresholds, TTLs, and metrics. By the end, you'll have working code, a tunable architecture, and a clear path to immediate latency and cost reductions.\n\nHere's what we're building:\n\n<ul>\n<li>A Redis HNSW vector index for semantic similarity search\n\n</li>\n<li>A cache layer that normalizes queries, generates embeddings, and retrieves cached responses\n\n</li>\n<li>A demo script to validate cache hit rates and latency improvements\n\n</li>\n</ul>\nWhat you'll need:\n\n<ul>\n<li>Python 3.9+\n\n</li>\n<li>Redis Stack (local via Docker or managed Redis Cloud)\n\n</li>\n<li>OpenAI API key\n\n</li>\n<li>Basic familiarity with embeddings and vector search\n\n</li>\n</ul>\nIf you're using Google Colab or a cloud notebook, I'd recommend connecting to a managed Redis Stack instance (like Redis Cloud) instead of wrestling with Docker locally.\n\nFor a deeper understanding of how LLMs manage memory and the concept of context rot, check out our article on why LLMs \"forget\" as their memory grows.\n\n<hr>\nHow It Works (High-Level Overview)\n\nThe paraphrase problem is something I've seen waste countless API calls: Users ask the same question in many different ways. \"What's your refund policy?\" and \"Can I get my money back?\" are semantically identical, but traditional caching treats them as completely different keys.\n\nHere's where embeddings come in: They map text into a high-dimensional vector space where semantically similar phrases naturally cluster together. By comparing query embeddings using cosine similarity, you can detect paraphrases and return cached responses. It's actually pretty elegant when you see it in action.\n\nWhy Redis Vector? Redis Stack provides HNSW (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search. And here's the thing - it combines low-latency vector search with Redis's native TTL, tagging, and filtering capabilities. That makes it ideal for production caching, not just a proof of concept.\n\nThe architecture is straightforward:\n\n<ol>\n<li>Normalize the user query (lowercase, strip out volatile patterns like timestamps)\n\n</li>\n<li>Generate an embedding for the normalized query\n\n</li>\n<li>Search the Redis HNSW index for the nearest cached embedding\n\n</li>\n<li>If distance < threshold and metadata matches (model, temperature, system prompt hash), return the cached response\n\n</li>\n<li>Otherwise, call the LLM, cache the new response with its embedding, and return it\n\n</li>\n</ol>\n<hr>\nSetup & Installation\n\nOption 1: Managed Redis (Recommended for Notebooks)\n\nSign up for a free Redis Cloud account at redis.com/try-free and create a Redis Stack database. Copy the connection URL - you'll need it in a second.\n\nIn your notebook or terminal:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nSet your environment variables:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n# os.environ[\"REDIS_URL\"] = \"redis://default:password@your-redis-host:port\"\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\nos.environ[\"EMBEDDING_MODEL\"] = \"text-embedding-3-small\"\nos.environ[\"CHAT_MODEL\"] = \"gpt-4o-mini\"\nos.environ[\"SIMILARITY_THRESHOLD\"] = \"0.30\"\nos.environ[\"TOP_K\"] = \"5\"\nos.environ[\"CACHE_TTL_SECONDS\"] = \"86400\"\nos.environ[\"CACHE_NAMESPACE\"] = \"sc:v1:\"\nos.environ[\"CORPUS_VERSION\"] = \"v1\"\nos.environ[\"TEMPERATURE\"] = \"0.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nOption 2: Local Redis with Docker\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nCreate a .env file:\n\n<p><pre><code>REDIS_URL=redis://localhost:6379\nOPENAI_API_KEY=sk-...\nEMBEDDING_MODEL=text-embedding-3-small\nCHAT_MODEL=gpt-4o-mini\nSIMILARITY_THRESHOLD=0.10\nTOP_K=5\nCACHE_TTL_SECONDS=86400\nCACHE_NAMESPACE=sc:v1:\nCORPUS_VERSION=v1\nTEMPERATURE=0.2\n</code></pre></p>\nInstall dependencies:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install redis openai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\n<hr>\nStep-by-Step Implementation\n\nStep 1: Create the Redis HNSW Index\n\nThe index stores embeddings and metadata for cached responses. We're using HNSW for fast approximate nearest neighbor search - it's the sweet spot between speed and accuracy.\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport redis\nimport time\n\nr = redis.Redis.from_url(os.getenv(\"REDIS_URL\"))\n\nINDEX = \"sc_index\" # Make sure to update this variable if you want a different index name\nPREFIX = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\nDIM = 1536  # Dimension for text-embedding-3-small\nM = 16  # HNSW graph connectivity\nEF_CONSTRUCTION = 200  # HNSW construction quality\n\ndef create_index():\n    print(f\"Using index name: {INDEX}\") # Print the index name being used\n\n    # Drop index if it exists, and delete associated documents (DD)\n    try:\n        r.execute_command(\"FT.DROPINDEX\", INDEX, \"DD\")\n        print(f\"Dropped existing index '{INDEX}' including documents.\")\n    except redis.ResponseError:\n        print(f\"Index '{INDEX}' did not exist, proceeding with creation.\")\n        pass # Index does not exist, safe to ignore\n\n    # Create index with vector field and metadata tags\n    cmd = [\n        \"FT.CREATE\", INDEX,  # Command to create a full-text search index with the given name\n        \"ON\", \"HASH\",  # Index applies to Redis Hash data structures\n        \"PREFIX\", \"1\", PREFIX,  # Only index keys starting with the defined prefix\n        \"SCHEMA\",  # Define the schema of the index\n        \"prompt_hash\", \"TAG\",  # Tag field for hashing the canonicalized prompt\n        \"model\", \"TAG\",  # Tag field for the LLM model used\n        \"sys_hash\", \"TAG\",  # Tag field for hashing the system prompt\n        \"corpus_version\", \"TAG\",  # Tag field for tracking the version of the underlying corpus\n        \"temperature\", \"NUMERIC\",  # Numeric field for the temperature parameter used by the LLM\n        \"created_at\", \"NUMERIC\",  # Numeric field for the creation timestamp\n        \"last_hit_at\", \"NUMERIC\",  # Numeric field for the timestamp of the last cache hit\n        \"response\", \"TEXT\",  # Text field for the LLM's response\n        \"user_question\", \"TEXT\", # Text field for the original user question\n        \"vector\", \"VECTOR\", \"HNSW\", \"10\",  # Define a vector field named \"vector\" using the HNSW algorithm. \"10\" specifies the number of pairs for the HNSW vector definition.\n        \"TYPE\", \"FLOAT32\",  # Specify the data type of the vector embeddings\n        \"DIM\", str(DIM),  # Specify the dimension of the vector embeddings\n        \"DISTANCE_METRIC\", \"COSINE\",  # Specify the distance metric to use for vector similarity search\n        \"M\", str(M),  # HNSW parameter: number of established connections for each element during graph construction\n        \"EF_CONSTRUCTION\", str(EF_CONSTRUCTION),  # HNSW parameter: size of the dynamic list for heuristic search during graph construction\n    ]\n    r.execute_command(*cmd)\n    print(f\"Index '{INDEX}' created.\")\n\ncreate_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nQuick validation:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\n\n# Helper function to decode bytes to string\ndef decode_bytes(item):\n    if isinstance(item, bytes):\n        return item.decode()\n    return item\n\n# Parse the info output for better readability\nparsed_info = {}\nfor i in range(0, len(info), 2):\n    key = decode_bytes(info[i])\n    value = info[i+1]\n    if isinstance(value, list):\n        # Decode lists of bytes\n        parsed_info[key] = [decode_bytes(item) for item in value]\n    else:\n        parsed_info[key] = decode_bytes(value)\n\nprint(\"Index Info:\")\nprint(f\"  index_name: {parsed_info.get('index_name')}\")\nprint(f\"  num_docs: {parsed_info.get('num_docs')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nYou should see num_docs: 0 initially. That's what we want.\n\n<hr>\nStep 2: Normalize Queries for Stable Cache Keys\n\nCanonicalization is crucial here. It removes volatile elements (timestamps, UUIDs, IDs) and normalizes whitespace to ensure paraphrases map to the same cache key. I learned this the hard way when cache hit rates were mysteriously low in an earlier project.\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\nimport hashlib\n\n# Note: Normalization adequacy depends on expected query variations and embedding model robustness.\nVOLATILE_PATTERNS = [\n    # ISO timestamps and variations\n    r\"\\b\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(Z|[+-]\\d{2}:\\d{2})?\\b\",\n    # Common date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY/MM/DD, YYYY-MM-DD)\n    r\"\\b\\d{1,4}[-/.]?\\d{1,2}[-/.]?\\d{2,4}\\b\", # Updated to be more flexible with separators and year length\n    # UUID v4\n    r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\",\n    # Long IDs (6+ digits)\n    r\"\\b\\d{6,}\\b\",\n    # Email addresses (often contain volatile parts or personally identifiable info)\n    r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\",\n]\n\ndef canonicalize(text: str) -> str:\n    # Removes volatile patterns (like dates, IDs) and standardizes whitespace\n    # to create a consistent representation of the query for caching.\n    t = text.strip().lower()\n    for pat in VOLATILE_PATTERNS:\n        t = re.sub(pat, \" \", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef sha256(s: str) -> str:\n    # Generates a SHA256 hash of a string. Used for creating stable identifiers\n    # for prompts and system prompts.\n    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n\ndef scope_hash(prompt_norm: str, model: str, sys_hash: str, temperature: float, corpus_version: str) -> str:\n    # Creates a unique hash that defines the scope of a cache entry.\n    # This ensures that a cache hit is only valid if all relevant parameters\n    # (normalized prompt, model, system prompt hash, temperature, corpus version) match.\n    payload = f\"{prompt_norm}|{model}|{sys_hash}|{temperature}|{corpus_version}\"\n    return sha256(payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nLet's test it:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q1 = \"What is our refund policy on 2025-01-15?\"\nq2 = \"what is our refund policy on 2025-01-20?\"\nprint(canonicalize(q1))\nprint(canonicalize(q2))\n# Both should output: \"what is our refund policy on\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\n<hr>\nStep 3: Initialize Clients and Embedding Function\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nEMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\nCHAT_MODEL = os.getenv(\"CHAT_MODEL\", \"gpt-4o-mini\")\nTHRESH = float(os.getenv(\"SIMILARITY_THRESHOLD\", 0.10))\nTOP_K = int(os.getenv(\"TOP_K\", 5))\nTTL = int(os.getenv(\"CACHE_TTL_SECONDS\", 86400))\nNS = os.getenv(\"CACHE_NAMESPACE\", \"sc:v1:\")\nCORPUS_VERSION = os.getenv(\"CORPUS_VERSION\", \"v1\")\nTEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n\ndef embed(text: str) -> np.ndarray:\n    # Generates a vector embedding for the input text using the specified embedding model.\n    # The vector is then L2 normalized, which is standard practice for cosine\n    # similarity search (But optional as it's already handled by Redis)\n    e = client.embeddings.create(model=EMBED_MODEL, input=text)\n    vec = np.array(e.data[0].embedding, dtype=np.float32)\n    norm = np.linalg.norm(vec)\n    return vec / max(norm, 1e-12) # L2 normalization\n\ndef to_bytes(vec: np.ndarray) -> bytes:\n    # Converts a NumPy array (the vector embedding) into bytes.\n    # This is necessary for storing the vector data in Redis, as Redis\n    # stores data as bytes.\n    return vec.astype(np.float32).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nQuick test to make sure everything's connected:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_vec = embed(\"hello world\")\nprint(f\"Embedding shape: {test_vec.shape}, norm: {np.linalg.norm(test_vec):.4f}\")\n# Should output shape (1536,) and norm ~1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\n<hr>\nStep 4: Implement Vector Search\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nfrom typing import Optional, Dict, Any, Tuple\n\ndef vector_search(query_vec, ef_runtime: int = 100, threshold: float = THRESH) -> Optional[Tuple[str, Dict[str, Any], float]]:\n    # Performs a vector similarity search in the Redis HNSW index.\n    # It searches for the nearest neighbor(s) to the query vector and\n    # returns the document(s) that are within the specified distance threshold.\n    # Perform KNN search with EF_RUNTIME parameter\n    # Define the parameters for the search query\n    params = [\"vec\", to_bytes(query_vec), \"ef_runtime\", ef_runtime]\n    # Define the search query using RediSearch's query syntax\n    # * => search all documents\n    # [KNN {TOP_K} @vector $vec => search for KNN of the vector parameter named \"vec\"\n    # AS score => return the score (distance) as \"score\"\n    # EF_RUNTIME $ef_runtime => specify the ef_runtime parameter for HNSW search\n    q = f\"*=>[KNN {TOP_K} @vector $vec AS score]\"\n    try:\n        # Execute the RediSearch query\n        res = r.execute_command(\n            \"FT.SEARCH\", INDEX, # Index name\n            q, \"PARAMS\", str(len(params)), *params, # Query and parameters\n            \"SORTBY\", \"score\", \"ASC\", # Sort results by score in ascending order (smaller distance is better)\n            \"RETURN\", \"8\", \"response\", \"model\", \"sys_hash\", \"corpus_version\", \"temperature\", \"prompt_hash\", \"score\", \"user_question\", # Return these fields, added \"user_question\"\n            \"DIALECT\", \"2\" # Use dialect 2 for parameters\n        )\n    except redis.RedisError as e:\n        # Handle Redis errors during search\n        print(f\"Redis search error: {e}\") # Modified to print the exception\n        return None\n\n    # Process the search results\n    total = res[0] if res else 0 # Total number of results (should be 1 if a match is found)\n    if total < 1:\n        # No results found\n        return None\n\n    # Extract document id and fields from the result\n    doc_id = res[1]\n    fields = res[2]\n    # Convert field names and values from bytes to strings\n    f = {fields[i].decode() if isinstance(fields[i], bytes) else fields[i]:\n         fields[i+1].decode() if isinstance(fields[i+1], bytes) else fields[i+1]\n         for i in range(0, len(fields), 2)}\n\n    try:\n        # Extract the score (distance)\n        distance = float(f[\"score\"])\n    except Exception:\n        # Handle error in extracting score\n        print(\"Error extracting score\") # Added error print for debugging\n        distance = 1.0\n\n    # Return the document id, fields, and distance\n    return doc_id.decode() if isinstance(doc_id, bytes) else doc_id, f, distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\n<hr>\nStep 5: Build the Cache Layer\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nfrom typing import Optional, Dict, Any, Tuple\n\ndef sys_hash(system_prompt: str) -> str:\n    # Generates a SHA256 hash of the system prompt\n    return sha256(system_prompt.strip())\n\ndef key(doc_id_hash: str) -> str:\n    # Creates a Redis key with a namespace prefix\n    return f\"{NS}{doc_id_hash}\"\n\ndef metadata_matches(f: Dict[str, Any], model: str, sys_h: str, temp: float, corpus: str) -> bool:\n    # Checks if the metadata from a cached document matches the current query parameters\n    try:\n        if f.get(\"model\") != model: return False\n        if f.get(\"sys_hash\") != sys_h: return False\n        # Compare temperatures with a tolerance for floating point precision\n        if abs(float(f.get(\"temperature\", temp)) - temp) > 1e-6: return False\n        if f.get(\"corpus_version\") != corpus: return False\n        return True\n    except Exception:\n        # Return False if there's an error during metadata comparison\n        return False\n\ndef chat_call(system_prompt: str, user_prompt: str):\n    # Calls the OpenAI chat completion API\n    t0 = time.perf_counter()\n    resp = client.chat.completions.create(\n        model=CHAT_MODEL,\n        temperature=TEMPERATURE,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n    )\n    latency_ms = (time.perf_counter() - t0) * 1000\n    content = resp.choices[0].message.content\n    usage = getattr(resp, \"usage\", None)\n    return content, latency_ms, usage\n\ndef cache_get_or_generate(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n    # Attempts to retrieve a response from the cache; if not found, calls the LLM and caches the response (optionally)\n    t0 = time.perf_counter()\n    sp_hash = sys_hash(system_prompt)\n    prompt_norm = canonicalize(user_prompt)\n    p_hash = sha256(prompt_norm)\n\n    qvec = embed(prompt_norm)\n\n    # --- Cache Lookup ---\n    res = vector_search(qvec, ef_runtime=ef_runtime, threshold=threshold)\n\n    # Check if a cached response was found and if its metadata matches\n    if res:\n        doc_id, fields, distance = res\n        if distance < threshold and metadata_matches(fields, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION):\n            try:\n                # Update the last hit timestamp for cache freshness\n                r.hset(doc_id, mapping={\"last_hit_at\": time.time()})\n            except redis.RedisError:\n                # Handle potential Redis errors during hset\n                pass\n            # Return the cached response details\n            return {\n                \"source\": \"cache\",\n                \"response\": fields[\"response\"],\n                \"user_question\": fields[\"user_question\"], # Include user_question for cache hits\n                \"distance\": distance,\n                \"latency_ms\": (time.perf_counter() - t0) * 1000,\n                \"closest_match_before_llm\": None # No pre-LLM closest match info on a cache hit\n            }\n\n    # --- Cache Miss - Call LLM and Cache (Optionally) ---\n\n    # If no cache hit, perform a debugging search for the closest match *before* adding the new item\n    closest_res_before_llm = vector_search(qvec, ef_runtime=ef_runtime, threshold=1.0) # Use high threshold to find closest regardless of match\n\n    content, llm_latency_ms, usage = chat_call(system_prompt, user_prompt)\n\n    # Only add to cache if add_to_cache is True\n    if add_to_cache:\n        # Generate a unique key for the new cache entry\n        doc_scope = scope_hash(prompt_norm, CHAT_MODEL, sp_hash, TEMPERATURE, CORPUS_VERSION)\n        redis_key = key(doc_scope)\n\n        try:\n            # Prepare data to be stored in Redis Hash\n            mapping = {\n                \"prompt_hash\": p_hash,\n                \"model\": CHAT_MODEL,\n                \"sys_hash\": sp_hash,\n                \"corpus_version\": CORPUS_VERSION,\n                \"temperature\": TEMPERATURE,\n                \"created_at\": time.time(),\n                \"last_hit_at\": time.time(),\n                \"response\": content,\n                \"user_question\": user_prompt,\n                \"vector\": to_bytes(qvec), # Store the embedding as bytes\n            }\n            # Use a pipeline for atomic HSET and EXPIRE operations\n            pipe = r.pipeline(transaction=True)\n            pipe.hset(redis_key, mapping=mapping)\n            pipe.expire(redis_key, int(TTL)) # Set the time-to-live for the cache entry\n            pipe.execute()\n        except redis.RedisError:\n            # Handle potential Redis errors during caching\n            pass\n\n    # Prepare closest match info for the return dictionary\n    closest_match_info = None\n    if closest_res_before_llm:\n         doc_id, fields, distance = closest_res_before_llm\n         closest_match_info = {\n             \"user_question\": fields.get('user_question'),\n             \"distance\": distance\n         }\n\n\n    # Return the LLM response details\n    return {\n        \"source\": \"llm\",\n        \"response\": content,\n        \"user_question\": user_prompt, # Include user_question for LLM responses\n        \"distance\": None, # No distance for an LLM response\n        \"latency_ms\": llm_latency_ms,\n        \"usage\": {\n            \"prompt_tokens\": getattr(usage, \"prompt_tokens\", None) if usage else None,\n            \"completion_tokens\": getattr(usage, \"completion_tokens\", None) if usage else None,\n            \"total_tokens\": getattr(usage, \"total_tokens\", None) if usage else None,\n        },\n        \"closest_match_before_llm\": closest_match_info # Include closest match info before LLM call\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\n<hr>\nStep 6: Add Metrics Tracking\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statistics\n\nclass Metrics:\n    def __init__(self):\n        # Initialize counters for cache hits and misses\n        self.hits = 0\n        self.misses = 0\n        # Lists to store latencies for cache hits and LLM calls\n        self.cache_latencies = []\n        self.llm_latencies = []\n\n    def record(self, result):\n        # Record metrics based on the source of the response (cache or LLM)\n        if result[\"source\"] == \"cache\":\n            self.hits += 1\n            self.cache_latencies.append(result[\"latency_ms\"])\n        else:\n            self.misses += 1\n            self.llm_latencies.append(result[\"latency_ms\"])\n\n    def snapshot(self):\n        # Calculate and return a snapshot of the current metrics\n        def safe_percentile(vals, p):\n            # Helper function to calculate percentiles safely\n            if not vals:\n                return None\n            sorted_vals = sorted(vals)\n            idx = int(len(sorted_vals) * p / 100) - 1\n            return sorted_vals[max(0, idx)]\n\n        return {\n            # Calculate the cache hit rate\n            \"hit_rate\": self.hits / max(self.hits + self.misses, 1),\n            # Calculate the median and 95th percentile latency for cache hits\n            \"p50_cache_ms\": statistics.median(self.cache_latencies) if self.cache_latencies else None,\n            \"p95_cache_ms\": safe_percentile(self.cache_latencies, 95),\n            # Calculate the median and 95th percentile latency for LLM calls\n            \"p50_llm_ms\": statistics.median(self.llm_latencies) if self.llm_latencies else None,\n            \"p95_llm_ms\": safe_percentile(self.llm_latencies, 95),\n        }\n\nmetrics = Metrics()\n\n# Modify the answer function to accept add_to_cache and pass it down\ndef answer(system_prompt: str, user_prompt: str, ef_runtime: int = 100, threshold: float = THRESH, add_to_cache: bool = True):\n    # Main function to get an answer, using the cache or calling the LLM\n    # Pass the add_to_cache parameter to cache_get_or_generate\n    res = cache_get_or_generate(system_prompt, user_prompt, ef_runtime=ef_runtime, threshold=threshold, add_to_cache=add_to_cache)\n    # Record the result in the metrics tracker\n    metrics.record(res)\n    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\n<hr>\nRun and Validate\n\nWarm the Cache\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"You are a concise support assistant for ACME Corp. Use internal policy v1 for refunds and returns.\"\nseed_prompts = [\n    \"What is your refund policy?\",\n    \"How long is the return window?\",\n    \"Do you offer exchanges?\",\n]\n\nprint(\"Warming cache...\")\nfor p in seed_prompts:\n    res = answer(SYSTEM_PROMPT, p, add_to_cache=True)\n    print(f\"{res['source']} {res['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nInspect the Cache\n\nCount indexed documents:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sweep_thresholds(thresholds):\n    for t in thresholds:\n        print(f\"\\nThreshold={t}\")\n        for p in paraphrases:\n            res = cache_get_or_generate(SYSTEM_PROMPT, p, ef_runtime=150, threshold=t, add_to_cache=False)\n            distance_str = f\"{res.get('distance'):.2f}\" if res.get('distance') is not None else 'N/A'\n            cached_question_str = f\" (Cached: {res.get('user_question')})\" if res['source'] == 'cache' else ''\n            print(f\"{p} => {res['source']} dist={distance_str}{cached_question_str}\")\n\n# Assuming 'paraphrases' list is defined earlier in the notebook\nsweep_thresholds([0.06, 0.08, 0.10, 0.12, 0.14])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nTake a look at a document:\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "info = r.execute_command(\"FT.INFO\", INDEX)\nnum_docs = info[info.index(b'num_docs') + 1]\nprint(f\"Cached documents: {num_docs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nTest Paraphrases\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paraphrases = [\n    # Refunds\n    \"Could you explain your refund policy?\",\n    \"Can you tell me how refunds work?\",\n    \"How do I request a refund?\",\n    \"Do you offer refunds if I'm not satisfied?\",\n    \"How can I get my money back after a purchase?\",\n\n    # Returns\n    \"What is the timeframe for returns?\",\n    \"How long is the return window?\",\n    \"Whatâ€™s the time limit to send something back?\",\n    \"When does the return period expire?\",\n    \"How many days do I have to return an item?\",\n\n    # Exchanges\n    \"Do you permit exchanges instead of refunds?\",\n    \"Can I exchange a product I bought?\",\n    \"Is it possible to swap an item for another?\",\n    \"Do you allow exchanges for different sizes or colors?\",\n    \"How do exchanges work in your store?\",\n]\n\nprint(\"\\nTesting paraphrases...\")\nfor p in paraphrases:\n    print(f\"\\n--- Testing Paraphrase ---\")\n    print(f\"Original: {p}\")\n    canonical_p = canonicalize(p)\n    print(f\"Canonicalized: {canonical_p}\")\n\n    # We don't want to polute the cache while testing\n    res = answer(SYSTEM_PROMPT, p, add_to_cache=False)\n\n    if res['source'] == 'cache':\n        print(f\"Result: CACHE HIT\")\n        print(f\"  Cached Question: {res.get('user_question')}\")\n        print(f\"  Distance: {res.get('distance'):.2f}\") # Formatted to 2 decimal places\n        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n    else: # res['source'] == 'llm'\n        print(f\"Result: CACHE MISS (LLM Call)\")\n        print(f\"  Latency: {res['latency_ms']:.1f}ms\")\n        if res.get('usage'):\n             print(f\"  Token Usage: Prompt={res['usage'].get('prompt_tokens')}, Completion={res['usage'].get('completion_tokens')}, Total={res['usage'].get('total_tokens')}\")\n\n        # Display closest match information found *before* the LLM call\n        closest_info = res.get('closest_match_before_llm')\n        if closest_info:\n            print(f\"  Closest match in cache (before LLM call):\")\n            print(f\"    Original Cached Q: '{closest_info.get('user_question')}'\")\n            print(f\"    Distance: {closest_info.get('distance'):.2f}\") # Formatted to 2 decimal places\n            print(f\"    Current THRESHOLD: {THRESH:.4f}\")\n        else:\n            print(f\"  No close match found in cache (even with high threshold) before LLM call.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nPrint Metrics\n\n<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Get the snapshot of the metrics\nmetrics_snapshot = metrics.snapshot()\n\n# Extract data for plotting\nlabels = ['Cache (P50)', 'Cache (P95)', 'LLM (P50)', 'LLM (P95)']\nlatency_values = [\n    metrics_snapshot.get('p50_cache_ms'),\n    metrics_snapshot.get('p95_cache_ms'),\n    metrics_snapshot.get('p50_llm_ms'),\n    metrics_snapshot.get('p95_llm_ms')\n]\n\n# Filter out None values if no cache hits or LLM calls occurred\nfiltered_labels = [labels[i] for i in range(len(latency_values)) if latency_values[i] is not None]\nfiltered_values = [value for value in latency_values if value is not None]\n\nif not filtered_values:\n    print(\"No latency data available to plot.\")\nelse:\n    # Create the bar chart\n    x = np.arange(len(filtered_labels))\n    fig, ax = plt.subplots(figsize=(8, 6))\n    bars = ax.bar(x, filtered_values, color=['skyblue', 'deepskyblue', 'lightcoral', 'indianred'])\n\n    # Add labels and title\n    ax.set_ylabel('Latency (ms)')\n    ax.set_title('Cache vs. LLM Latency (P50 and P95)')\n    ax.set_xticks(x)\n    ax.set_xticklabels(filtered_labels)\n    ax.set_ylim(0, max(filtered_values) * 1.2) # Set y-axis limit\n\n    # Add value labels on top of the bars\n    for bar in bars:\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, yval + 5, f'{yval:.1f}', ha='center', va='bottom')\n\n    # Display the plot\n    plt.tight_layout()\n    plt.show()\n\n# Optionally, print the hit rate separately\nprint(f\"\\nCache Hit Rate: {metrics_snapshot.get('hit_rate', 0.0):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</p>\nConclusion\n\nYou've just shipped a production-grade semantic cache on Redis Vector: normalize â†’ embed â†’ vector search â†’ serve cached response when \"close enough.\" In your run, the cache cut latency dramatically:\n\n<ul>\n<li>Median (P50): 1506.1 ms â†’ 244.3 ms (â‰ˆ 6.2Ã— faster, ~84% reduction)\n\n</li>\n<li>Tail (P95): 2743.0 ms â†’ 374.3 ms (â‰ˆ 7.3Ã— faster, ~86% reduction)\n\n</li>\n</ul>\nWith a healthy hit rate, this translates to significant cost savings. You're avoiding full LLM calls on repeats, which adds up quickly.\n\nKey design choices that matter:\n\n<ul>\n<li>Canonicalization stabilizes keys across paraphrases - this is non-negotiable.\n\n</li>\n<li>HNSW delivers sub-100 ms vector search at scale. Actually, in my testing, it's often much faster.\n\n</li>\n<li>Metadata gating (model/temp/sys prompt) prevents stale or mismatched hits. Trust me, you want this.\n\n</li>\n<li>TTL + namespace versioning give you safe, bulk invalidation when you need it.\n\n</li>\n</ul>\nNext Steps\n\n<ul>\n<li>Refine similarity thresholds based on ongoing analysis of paraphrase hit and miss patterns. You'll want to balance precision and recall for your specific use case.\n\n</li>\n<li>Evaluate embedding quality - consider higher-fidelity or hybrid models to tighten semantic clustering across paraphrases.\n\n</li>\n<li>Enrich cache coverage by indexing additional rephrasings and related expressions for your most common queries.\n\n</li>\n<li>Enhance observability. Monitor cache hit rate, latency percentiles, and similarity distributions over time. The patterns will surprise you.\n\n</li>\n<li>Maintain retrieval accuracy through consistent metadata filtering and versioned namespaces to isolate context shifts.\n\n</li>\n</ul>"
      ]
    }
  ],
  "metadata": {
    "title": "Semantic Cache LLM: How to Implement with Redis Vector to Cut Costs",
    "description": "Build a semantic cache LLM using embeddings and Redis Vector with TTLs, thresholds, metrics to reduce LLM spend and latency.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}