{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]\n\n**Description:** Build a production-ready multi-document agent with LlamaIndex, turning PDFs into retrieval and summarization tools using semantic selection for accurate answers.\n\n**ðŸ“– Read the full article:** [Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]](https://blog.thegenairevolution.com/article/multi-document-agent-with-llamaindex-the-ultimate-guide-2025-2)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So here's what we're going to build: a multi\\-document research assistant that can actually answer questions across multiple PDFs and tell you exactly where it found the information. I've been working on this kind of system for a while now, and the key breakthrough was combining semantic vector search for those specific \"what did they say about X\" questions with hierarchical summarization for the bigger picture stuff. The whole thing uses function calling to figure out which tool to use when.\n\nBy the time you're done with this tutorial, you'll have a working notebook that handles cross\\-document Q\\&A, gives you proper citations in \\[file\\_name p.page\\_label] format (which honestly took me way too long to get right the first time), and includes some basic tests to make sure everything's working.\n\n**Prerequisites:**\n\n* Python 3\\.10\\+\n* OpenAI API key\n* 2 to 3 sample PDFs (research papers, reports, technical documents, whatever you have)\n* Expected cost: about $0\\.10 to $0\\.50 per summary\\-heavy query, depending on how big your documents are\n\n## Why This Approach Works\n\n**Per\\-Document Tool Isolation**\n\nEach PDF gets its own vector and summary tool. Sounds simple, but this is actually crucial. It prevents the system from mixing up information between documents, makes citations precise, and lets the agent figure out which document to look at for any given question. Plus, when something goes wrong (and it will), debugging is so much easier when everything's cleanly separated.\n\n**Semantic Tool Retrieval**\n\nHere's something I learned the hard way: when you have more than a handful of documents, you can't just throw all the tools at the agent and hope for the best. Instead, we use an object index that embeds tool descriptions and retrieves the top\\-k relevant ones for each query. Works beautifully even with dozens of documents.\n\n**Dual Retrieval Strategy**\n\nVector tools handle the narrow stuff. \"What dataset did the authors use?\" That kind of thing. Summary tools handle the broad synthesis questions. \"Compare the main contributions across papers.\" The agent picks which one based on what you're asking. I've found this combination covers about 95% of research queries.\n\n**Citation Enforcement**\n\nEvery tool attaches file name and page metadata to its results. Then the system prompt basically hammers into the agent that it needs to cite sources after each claim. You can also post\\-process responses to clean up the citation format if needed. No more vague \"according to the document\" nonsense that drives everyone crazy.\n\n## How It Works (High\\-Level Overview)\n\n1. **Load and chunk PDFs** â€“ Extract text, split into chunks that respect sentence boundaries, normalize all the metadata for citations.\n2. **Build per\\-document tools** â€“ Create both vector and summary tools for each PDF, give them clear descriptions so the agent knows what they do.\n3. **Index tools semantically** â€“ Embed those tool descriptions in an object index so we can retrieve them dynamically.\n4. **Assemble the agent** â€“ Use function calling with a pretty strict system prompt to route queries and make sure citations happen.\n5. **Validate and iterate** â€“ Run test queries, see which tools get selected, adjust retrieval thresholds and temperature until it works right.\n\n## Setup \\& Installation\n\nFirst things first, let's install everything with pinned versions so we don't run into compatibility issues later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install llama-index llama-index-llms-openai  llama-index-embeddings-openai pypdf nest_asyncio python-dotenv numpy pandas jedi>=0.16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the OpenAI API key. If you're in Colab, go to Settings, then Secrets, and add OPENAI\\_API\\_KEY there. Otherwise, just make a .env file with OPENAI\\_API\\_KEY\\=your\\_key. Pretty standard stuff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Fail early if key is missing\nassert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in .env or Colab Secrets\"\nprint(\"API key loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's set up logging and suppress those annoying warnings. Also enabling async support because why not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nimport warnings\nimport nest_asyncio\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO)\nnest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the LLM and embedding model globally. This way we don't have to specify them everywhere:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# Use GPT-4o for reliable function calling; fallback to gpt-4o-mini if needed\nSettings.llm = OpenAI(model=\"gpt-4o\", temperature=0.1)\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a data directory and grab some sample PDFs so you can actually run this thing end\\-to\\-end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n\nDATA_DIR = \"data\"\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Example: download public arXiv papers (replace with your own PDFs)\nsample_urls = [\n    (\"https://arxiv.org/pdf/2005.11401.pdf\", \"paper1.pdf\"),  # GPT-3 paper\n    (\"https://arxiv.org/pdf/2303.08774.pdf\", \"paper2.pdf\"),  # GPT-4 paper\n]\n\nfor url, fname in sample_urls:\n    fpath = os.path.join(DATA_DIR, fname)\n    if not os.path.exists(fpath):\n        print(f\"Downloading {fname}...\")\n        urllib.request.urlretrieve(url, fpath)\n\npdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".pdf\")]\nprint(f\"Found {len(pdf_files)} PDFs:\", pdf_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Step 1: Load and Chunk PDFs\n\nLoad documents from the data directory. The PDF reader is nice enough to attach page metadata automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, Document\n\ndocs = SimpleDirectoryReader(DATA_DIR, recursive=False).load_data()\nprint(f\"Loaded {len(docs)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we split documents into chunks. But here's the thing about chunking: you want to respect sentence boundaries. I learned this after watching my early systems split sentences in half and completely lose the meaning. Sentence\\-aware splitting gives the vector index much better semantic units to work with. Makes a huge difference with dense technical writing. Actually, if you want to dive deeper into retrieval optimization, I wrote up some [retrieval tricks to boost answer accuracy](/article/rag-application-7-retrieval-tricks-to-boost-answer-accuracy-2) that might help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n\nsplitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\nnodes = splitter.get_nodes_from_documents(docs, show_progress=True)\nprint(f\"Total chunks: {len(nodes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalize metadata for citations. Every node needs file\\_name and page\\_label, otherwise your citations will be a mess:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for n in nodes:\n    meta = n.metadata or {}\n    if \"file_name\" not in meta:\n        file_path = meta.get(\"file_path\", meta.get(\"source\", \"unknown\"))\n        meta[\"file_name\"] = os.path.basename(file_path) if isinstance(file_path, str) else \"unknown\"\n    if \"page_label\" not in meta:\n        meta[\"page_label\"] = str(meta.get(\"page_number\", \"N/A\"))\n    n.metadata = meta\n\nprint(\"Sample chunk metadata:\", nodes[0].metadata)\nprint(\"Sample chunk text:\", nodes[0].text[:300], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Group nodes by document so we can create per\\-document tools:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nnodes_by_file = defaultdict(list)\nfor n in nodes:\n    nodes_by_file[n.metadata[\"file_name\"]].append(n)\n\nprint({k: len(v) for k, v in nodes_by_file.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Build Per\\-Document Vector Tools\n\nCreate a vector index for each document. This handles precise passage retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\nfrom llama_index.core.tools import QueryEngineTool\n\nvector_tools = {}\n\nfor fname, doc_nodes in nodes_by_file.items():\n    v_index = VectorStoreIndex(doc_nodes, show_progress=True)\n    v_engine = v_index.as_query_engine(similarity_top_k=5)\n    v_tool = QueryEngineTool.from_defaults(\n        name=f\"vector_{fname.replace('.', '_')}\",\n        query_engine=v_engine,\n        description=(\n            f\"Semantic vector search for {fname}. \"\n            \"Use for targeted, specific questions that require exact passages and citations.\"\n        )\n    )\n    vector_tools[fname] = v_tool\n\nprint(f\"Vector tools created: {len(vector_tools)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Always test your tools. Seriously, always:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = next(iter(vector_tools.keys()))\nresp = vector_tools[sample_file].query_engine.query(\"What problem does this paper address?\")\nprint(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Build Per\\-Document Summary Tools\n\nCreate a summary index for each document. This is what handles the high\\-level synthesis questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex\n\nsummary_tools = {}\n\nfor fname, doc_nodes in nodes_by_file.items():\n    s_index = SummaryIndex(doc_nodes)\n    s_engine = s_index.as_query_engine(\n        response_mode=\"tree_summarize\",\n        use_async=True\n    )\n    s_tool = QueryEngineTool.from_defaults(\n        name=f\"summary_{fname.replace('.', '_')}\",\n        query_engine=s_engine,\n        description=(\n            f\"Hierarchical summarization for {fname}. \"\n            \"Use for overviews, key contributions, limitations, and document-wide synthesis.\"\n        )\n    )\n    summary_tools[fname] = s_tool\n\nprint(f\"Summary tools created: {len(summary_tools)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the summary tool too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = next(iter(summary_tools.keys()))\nresp = summary_tools[sample_file].query_engine.query(\"Provide a 5-bullet executive summary.\")\nprint(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Index Tools Semantically\n\nBuild an object index over all the tools. This is the clever bit. It embeds tool descriptions and retrieves the most relevant ones for each query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.objects import ObjectIndex\n\nall_tools = list(vector_tools.values()) + list(summary_tools.values())\n\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    index_cls=VectorStoreIndex,\n    show_progress=True\n)\n\ntool_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check which tools get retrieved for different queries. This debugging step has saved me hours of head\\-scratching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n\ndef inspect_tools(query: str):\n    retrieved_results = tool_retriever.retrieve(query)\n    print(f\"Query: {query}\")\n    for i, res in enumerate(retrieved_results, 1):\n        tool_obj = None\n        # Check if res is a NodeWithScore object (standard behavior for ObjectRetriever)\n        if hasattr(res, 'node') and hasattr(res.node, 'obj'):\n            tool_obj = res.node.obj\n        # Otherwise, assume res is the QueryEngineTool object directly\n        else:\n            tool_obj = res\n\n        tool_name = getattr(getattr(tool_obj, 'metadata', None), 'name', None)\n\n        if tool_name:\n            name_parts = tool_name.split('_', 1)\n            tool_type = name_parts[0] if len(name_parts) > 0 else \"unknown\"\n            file_name = name_parts[1] if len(name_parts) > 1 else \"unknown\"\n            print(f\"#{i} -> {tool_type} | {file_name} | {tool_name}\")\n        else:\n            print(f\"#{i} -> Could not determine tool properties: tool object {tool_obj} has no valid 'name' attribute in its metadata or it's empty/None.\")\n        print(\"-\" * 30)\n\n# Test calls for inspect_tools\ninspect_tools(\"Provide an executive summary across all documents.\")\ninspect_tools(\"Which sections discuss model architecture details?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Assemble the Agent\n\nTime to create the actual agent with function calling and a system prompt that's pretty insistent about citations. I've tried a bunch of frameworks for this. LangChain and CrewAI are solid, but LlamaIndex just clicks for document workflows. It has all the indexing, retrieval, and summarization stuff built in. If you're curious about the fundamentals of how agents work, I put together a tutorial on [building an LLM agent from scratch with GPT\\-4 ReAct](/article/how-to-build-an-llm-agent-from-scratch-with-gpt-4-react-5) that breaks it all down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.core import Settings\n\nSYSTEM_PROMPT = \"\"\"You are a multi-document research assistant.\n- Use only the provided tools.\n- Prefer vector tools for specific, narrow questions.\n- Prefer summary tools for high-level synthesis.\n- Always cite sources as [file_name p.page_label] after each relevant sentence.\n- If you cannot find relevant evidence, say so explicitly.\"\"\"\n\n# If you previously used a tool retriever, see Option B below.\nagent = FunctionAgent(\n    tools=all_tools,          # same list you used before\n    llm=Settings.llm,         # your configured LLM\n    system_prompt=SYSTEM_PROMPT,\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\nLet's run a cross\\-document query and see if the agent actually synthesizes answers with proper citations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\nresponse = asyncio.run(\n    agent.run(\"Compare the main challenges and proposed collaboration mechanisms across the papers.\")\n)\nprint(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a bunch of test queries to really put the agent through its paces. This is where you find out if everything's actually working together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n\ntests = [\n    \"List the datasets used by each paper and compare evaluation metrics.\",\n    \"Provide a high-level summary of the main contributions across documents.\",\n    \"According to the authors, what are the primary limitations?\"\n]\n\nasync def main():\n    for q in tests:\n        print(\"\\nQ:\", q)\n        resp = await agent.run(q)\n        print(\"A:\", str(resp))\n\nasyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nAnd there you have it. You've built a multi\\-document research assistant that routes queries to the right tool, pulls out precise passages, and actually tells you where it got the information from.\n\nThe key decisions that make this work: keeping tools separate per document for clean attribution, using semantic retrieval to scale beyond a handful of documents, and having two retrieval modes. Vector for the specific stuff, summary for the big picture.\n\nIf you want to take this to production, here's what I'd do next:\n\n1. **Persist indices** â€“ Save those vector and summary indices to disk or something like pgvector or Pinecone. Re\\-embedding everything on every run gets expensive fast. Learned that one the hard way.\n2. **Add retries and rate limits** â€“ Wrap your LLM calls with exponential backoff and timeouts. Things fail. Better to handle it gracefully than crash.\n3. **Implement structured logging** â€“ Use LlamaIndex callbacks or a proper logging framework to track tool calls, latency, token usage. Future you will thank present you when something weird happens.\n4. **Cache answers** â€“ Use an LRU cache or Redis for repeated queries. Actually, I wrote up [how to implement semantic cache with Redis Vector](/article/semantic-cache-llm-how-to-implement-with-redis-vector-to-cut-costs-6) if you want to get fancy with it and save some serious API costs.\n5. **Post\\-process citations** â€“ Extract source\\_nodes from responses and format citations programmatically. Relying purely on prompts for formatting only gets you so far. Sometimes you need to just fix it in post.\n\nThat's pretty much it. You now have a working multi\\-document research assistant that knows where its information comes from. Pretty handy for any serious document work where you need to back up your claims."
      ]
    }
  ],
  "metadata": {
    "title": "Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]",
    "description": "Build a production-ready multi-document agent with LlamaIndex, turning PDFs into retrieval and summarization tools using semantic selection for accurate answers.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}