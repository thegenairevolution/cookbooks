{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]\n\n**Description:** Build a production-ready multi-document agent with LlamaIndex, turning PDFs into retrieval and summarization tools using semantic selection for accurate answers.\n\n**ðŸ“– Read the full article:** [Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]](https://blog.thegenairevolution.com/article/multi-document-agent-with-llamaindex-the-ultimate-guide-2025)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You're Building\n\nYou'll create a multi\\-document research assistant that can answer questions across multiple PDFs with precise citations. The agent leverages semantic vector search for targeted queries, hierarchical summarization for high\\-level synthesis, and function calling to route queries to the appropriate tool. When you're done, you'll have a runnable notebook that handles cross\\-document Q\\&A, enforces consistent citations in \\[file\\_name p.page\\_label\\] format, and includes a minimal validation suite.\n\n**Prerequisites:**\n\n* Python 3\\.10\\+\n* OpenAI API key\n* 2 to 3 sample PDFs (research papers, reports, or technical documents)\n* Expected cost: roughly $0\\.10 to $0\\.50 per summary\\-heavy query depending on document size\n\n## Why This Approach Works\n\n**Per\\-Document Tool Isolation**\n\nEach PDF gets its own vector and summary tool. This prevents cross\\-contamination, enables precise citations, and lets the agent reason about which document to query for any given question. It's a clean separation that makes debugging much easier too.\n\n**Semantic Tool Retrieval**\n\nAn object index embeds tool descriptions and retrieves the top\\-k relevant tools per query. This scales to dozens of documents without overwhelming the agent's context window. Actually, this is one of those things that sounds complicated but works beautifully in practice.\n\n**Dual Retrieval Strategy**\n\nVector tools handle narrow, fact\\-based queries like \"What dataset did the authors use?\" Summary tools handle broad synthesis questions like \"Compare the main contributions across papers.\" The agent picks the right mode based on query semantics. Simple but effective.\n\n**Citation Enforcement**\n\nEvery tool attaches file name and page metadata to results. The system prompt instructs the agent to cite sources after each claim, and you can post\\-process responses to format citations programmatically. No more vague \"according to the document\" references.\n\n## How It Works (High\\-Level Overview)\n\n1. **Load and chunk PDFs** â€“ Extract text, split into sentence\\-aware chunks, normalize metadata for citations.\n\n2. **Build per\\-document tools** â€“ Create vector and summary tools for each PDF, wrap them with clear descriptions.\n\n3. **Index tools semantically** â€“ Embed tool descriptions in an object index for dynamic retrieval.\n\n4. **Assemble the agent** â€“ Use function calling with a strict system prompt to route queries and enforce citations.\n\n5. **Validate and iterate** â€“ Run test queries, inspect tool selection, tune retrieval thresholds and temperature.\n\n## Setup \\& Installation\n\nFirst, run this cell to install all required packages with pinned versions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install llama-index llama-index-llms-openai  llama-index-embeddings-openai pypdf nest_asyncio python-dotenv numpy pandas jedi>=0.16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next up, configure your OpenAI API key. If you're running in Colab, add your key to Secrets (Settings, then Secrets, then OPENAI\\_API\\_KEY). Otherwise, just create a .env file with OPENAI\\_API\\_KEY\\=your\\_key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Fail early if key is missing\nassert os.getenv(\"OPENAI_API_KEY\"), \"Set OPENAI_API_KEY in .env or Colab Secrets\"\nprint(\"API key loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up logging, suppress warnings, and enable async support for a clean notebook environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nimport warnings\nimport nest_asyncio\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO)\nnest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the LLM and embedding model globally for all LlamaIndex operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# Use GPT-4o for reliable function calling; fallback to gpt-4o-mini if needed\nSettings.llm = OpenAI(model=\"gpt-4o\", temperature=0.1)\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a data directory and download sample PDFs programmatically so the notebook runs end\\-to\\-end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n\nDATA_DIR = \"data\"\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Example: download public arXiv papers (replace with your own PDFs)\nsample_urls = [\n    (\"https://arxiv.org/pdf/2005.11401.pdf\", \"paper1.pdf\"),  # GPT-3 paper\n    (\"https://arxiv.org/pdf/2303.08774.pdf\", \"paper2.pdf\"),  # GPT-4 paper\n]\n\nfor url, fname in sample_urls:\n    fpath = os.path.join(DATA_DIR, fname)\n    if not os.path.exists(fpath):\n        print(f\"Downloading {fname}...\")\n        urllib.request.urlretrieve(url, fpath)\n\npdf_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".pdf\")]\nprint(f\"Found {len(pdf_files)} PDFs:\", pdf_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Step 1: Load and Chunk PDFs\n\nLoad documents from the data directory. The PDF reader attaches page metadata automatically, which is exactly what we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, Document\n\ndocs = SimpleDirectoryReader(DATA_DIR, recursive=False).load_data()\nprint(f\"Loaded {len(docs)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split documents into sentence\\-aware chunks for semantic retrieval. Here's the thing about sentence\\-aware splitting: it avoids fragmenting thoughts mid\\-sentence, giving the vector index better semantic units. This directly improves retrieval quality, especially for dense technical writing like research papers or legal clauses. Actually, if you want more strategies to boost retrieval accuracy in RAG systems, check out our guide on [retrieval tricks to boost answer accuracy](/article/rag-application-7-retrieval-tricks-to-boost-answer-accuracy-2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n\nsplitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\nnodes = splitter.get_nodes_from_documents(docs, show_progress=True)\nprint(f\"Total chunks: {len(nodes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalize metadata for accurate citations. You need to ensure every node has file\\_name and page\\_label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for n in nodes:\n    meta = n.metadata or {}\n    if \"file_name\" not in meta:\n        file_path = meta.get(\"file_path\", meta.get(\"source\", \"unknown\"))\n        meta[\"file_name\"] = os.path.basename(file_path) if isinstance(file_path, str) else \"unknown\"\n    if \"page_label\" not in meta:\n        meta[\"page_label\"] = str(meta.get(\"page_number\", \"N/A\"))\n    n.metadata = meta\n\nprint(\"Sample chunk metadata:\", nodes[0].metadata)\nprint(\"Sample chunk text:\", nodes[0].text[:300], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Group nodes by document for per\\-document tool creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nnodes_by_file = defaultdict(list)\nfor n in nodes:\n    nodes_by_file[n.metadata[\"file_name\"]].append(n)\n\nprint({k: len(v) for k, v in nodes_by_file.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Build Per\\-Document Vector Tools\n\nCreate a vector index for each document to enable precise passage retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\nfrom llama_index.core.tools import QueryEngineTool\n\nvector_tools = {}\n\nfor fname, doc_nodes in nodes_by_file.items():\n    v_index = VectorStoreIndex(doc_nodes, show_progress=True)\n    v_engine = v_index.as_query_engine(similarity_top_k=5)\n    v_tool = QueryEngineTool.from_defaults(\n        name=f\"vector_{fname.replace('.', '_')}\",\n        query_engine=v_engine,\n        description=(\n            f\"Semantic vector search for {fname}. \"\n            \"Use for targeted, specific questions that require exact passages and citations.\"\n        )\n    )\n    vector_tools[fname] = v_tool\n\nprint(f\"Vector tools created: {len(vector_tools)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test a vector tool to verify retrieval quality. Always good to sanity check these things:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = next(iter(vector_tools.keys()))\nresp = vector_tools[sample_file].query_engine.query(\"What problem does this paper address?\")\nprint(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Build Per\\-Document Summary Tools\n\nCreate a summary index for each document to enable hierarchical summarization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex\n\nsummary_tools = {}\n\nfor fname, doc_nodes in nodes_by_file.items():\n    s_index = SummaryIndex(doc_nodes)\n    s_engine = s_index.as_query_engine(\n        response_mode=\"tree_summarize\",\n        use_async=True\n    )\n    s_tool = QueryEngineTool.from_defaults(\n        name=f\"summary_{fname.replace('.', '_')}\",\n        query_engine=s_engine,\n        description=(\n            f\"Hierarchical summarization for {fname}. \"\n            \"Use for overviews, key contributions, limitations, and document-wide synthesis.\"\n        )\n    )\n    summary_tools[fname] = s_tool\n\nprint(f\"Summary tools created: {len(summary_tools)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test a summary tool to verify synthesis quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = next(iter(summary_tools.keys()))\nresp = summary_tools[sample_file].query_engine.query(\"Provide a 5-bullet executive summary.\")\nprint(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Index Tools Semantically\n\nBuild an object index over all tools for semantic tool selection. This embeds tool descriptions and retrieves the top\\-k relevant tools per query. It's actually pretty clever how this works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.objects import ObjectIndex\n\nall_tools = list(vector_tools.values()) + list(summary_tools.values())\n\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    index_cls=VectorStoreIndex,\n    show_progress=True\n)\n\ntool_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect which tools are retrieved for different queries to debug tool selection. This step is crucial for understanding what's happening under the hood:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n\ndef inspect_tools(query: str):\n    retrieved_results = tool_retriever.retrieve(query)\n    print(f\"Query: {query}\")\n    for i, res in enumerate(retrieved_results, 1):\n        tool_obj = None\n        # Check if res is a NodeWithScore object (standard behavior for ObjectRetriever)\n        if hasattr(res, 'node') and hasattr(res.node, 'obj'):\n            tool_obj = res.node.obj\n        # Otherwise, assume res is the QueryEngineTool object directly\n        else:\n            tool_obj = res\n\n        tool_name = getattr(getattr(tool_obj, 'metadata', None), 'name', None)\n\n        if tool_name:\n            name_parts = tool_name.split('_', 1)\n            tool_type = name_parts[0] if len(name_parts) > 0 else \"unknown\"\n            file_name = name_parts[1] if len(name_parts) > 1 else \"unknown\"\n            print(f\"#{i} -> {tool_type} | {file_name} | {tool_name}\")\n        else:\n            print(f\"#{i} -> Could not determine tool properties: tool object {tool_obj} has no valid 'name' attribute in its metadata or it's empty/None.\")\n        print(\"-\" * 30)\n\n# Test calls for inspect_tools\ninspect_tools(\"Provide an executive summary across all documents.\")\ninspect_tools(\"Which sections discuss model architecture details?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Assemble the Agent\n\nCreate the agent using function calling and a strict system prompt that enforces citation format. Now, while frameworks like LangChain and CrewAI are solid choices, LlamaIndex specializes in document workflows with first\\-class support for indexing, retrieval, summarization, and agentic tool use that map cleanly to this problem. If you're interested in foundational agent patterns, you might want to check out our step\\-by\\-step tutorial on [building an LLM agent from scratch with GPT\\-4 ReAct](/article/how-to-build-an-llm-agent-from-scratch-with-gpt-4-react-5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.core import Settings\n\nSYSTEM_PROMPT = \"\"\"You are a multi-document research assistant.\n- Use only the provided tools.\n- Prefer vector tools for specific, narrow questions.\n- Prefer summary tools for high-level synthesis.\n- Always cite sources as [file_name p.page_label] after each relevant sentence.\n- If you cannot find relevant evidence, say so explicitly.\"\"\"\n\n# If you previously used a tool retriever, see Option B below.\nagent = FunctionAgent(\n    tools=all_tools,          # same list you used before\n    llm=Settings.llm,         # your configured LLM\n    system_prompt=SYSTEM_PROMPT,\n    verbose=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\nRun a cross\\-document query and verify the agent synthesizes answers with citations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\nresponse = asyncio.run(\n    agent.run(\"Compare the main challenges and proposed collaboration mechanisms across the papers.\")\n)\nprint(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a suite of test queries to validate agent routing, retrieval, and summarization. This is where you really see if everything's working together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n\ntests = [\n    \"List the datasets used by each paper and compare evaluation metrics.\",\n    \"Provide a high-level summary of the main contributions across documents.\",\n    \"According to the authors, what are the primary limitations?\"\n]\n\nasync def main():\n    for q in tests:\n        print(\"\\nQ:\", q)\n        resp = await agent.run(q)\n        print(\"A:\", str(resp))\n\nasyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nYou've built a multi\\-document research assistant that routes queries to the right tool, retrieves precise passages, and enforces consistent citations. The key decisions here include per\\-document tool isolation for clean attribution, semantic tool retrieval for scalability, and dual retrieval modes. Vector for specifics, summary for synthesis.\n\nNext steps to harden this for production:\n\n1. **Persist indices** â€“ Save vector and summary indices to disk or a vector database like pgvector or Pinecone to avoid re\\-embedding on every run. Trust me, this saves a lot of time and money.\n\n2. **Add retries and rate limits** â€“ Wrap LLM calls with exponential backoff and timeout handling for robustness. Things will fail occasionally, better to handle it gracefully.\n\n3. **Implement structured logging** â€“ Use LlamaIndex callbacks or a logging framework to trace tool calls, latency, and token usage. You'll thank yourself later when debugging.\n\n4. **Cache answers** â€“ Use an in\\-memory LRU cache or a persistent store like Redis for repeated queries. For a deep dive into implementing semantic caching with Redis Vector to optimize LLM costs, see [how to implement semantic cache with Redis Vector](/article/semantic-cache-llm-how-to-implement-with-redis-vector-to-cut-costs-6).\n\n5. **Post\\-process citations** â€“ Extract source\\_nodes from responses and format citations programmatically to ensure consistency beyond prompt\\-based enforcement. Prompt engineering only gets you so far.\n\nAnd that's it. You now have a working multi\\-document research assistant that actually knows where its information comes from. Pretty useful for any serious document analysis work."
      ]
    }
  ],
  "metadata": {
    "title": "Multi-Document Agent with LlamaIndex: The Ultimate Guide [2025]",
    "description": "Build a production-ready multi-document agent with LlamaIndex, turning PDFs into retrieval and summarization tools using semantic selection for accurate answers.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}