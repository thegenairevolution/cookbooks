{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Deploy DeepSeek-R1 Locally with Ollama, MongoDB, and a Chat UI\n\n**Description:** Build a private DeepSeek-R1 chatbot with Ollama, MongoDB, and chat UIâ€”no external APIs. Deployment steps for local setups or AWS.\n\n**ðŸ“– Read the full article:** [How to Deploy DeepSeek-R1 Locally with Ollama, MongoDB, and a Chat UI](https://blog.thegenairevolution.com/article/how-to-deploy-deepseek-r1-locally-with-ollama-mongodb-and-a-chat-ui-2)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So you've probably heard about DeepSeek R1 by now, that open source large language model from the Chinese startup DeepSeek. The release made headlines and actually managed to spook the U.S. stock market, with several AI stocks taking a hit. Look, there are already tons of reviews out there telling you how impressive this model is, so I won't pile on. What I want to talk about instead is something actually useful. You can download this thing and run it on your own machine. For a quick primer on the foundations behind models like R1, [see how transformer architectures power large language models like DeepSeek R1\\.](/article/transformers-demystifying-the-magic-behind-large-language-models-2)\n\nWhy would you want to do that? Well, maybe you're not comfortable sending your data to some third party API. Or maybe you're trying to keep costs under control. I've been there. Running locally means you can fine tune the model and customize everything to fit your specific stack. Plus, you can learn [how in\\-context learning techniques can further boost your model's accuracy and control](/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3).\n\nHere's the good news. Getting DeepSeek R1 running on your own hardware is actually pretty straightforward. Let me walk you through exactly how I did it.\n\n## Get a machine: AWS EC2 instance\n\nFirst things first, you need a machine to run DeepSeek R1\\. If you're just experimenting or building a personal chatbot, honestly, your local computer might be enough. But if you're thinking about production, you'll probably want dedicated servers. And if you just want to get started quickly without any fuss, a cloud instance is your fastest bet.\n\nFor a lightweight start, an AWS EC2 CPU instance will handle the 1\\.5B parameter variant just fine. When I first tested this, I used an m5\\.2xlarge. You can still use that one. But actually, you might want to consider newer generation instances like m7i.2xlarge or m7g.2xlarge for better price performance. I've noticed the newer ones run cooler too.\n\nNow, if you want faster responses or you're planning to try the larger variants, go with a GPU instance. A g6\\.xlarge or g5\\.xlarge makes a good baseline. These give you an NVIDIA GPU with enough VRAM to handle 7B class models at practical quantization levels. Trust me, the speed difference is worth it if you're doing anything beyond basic testing.\n\nTo launch an EC2 instance, just follow the official AWS guide: [AWS EC2 Getting Started Guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html)\n\n## Set up your machine\n\nNext, you'll need to install the essentials to run DeepSeek R1\\. I always start with a fresh Ubuntu LTS image to keep things clean. Ubuntu 24\\.04 LTS is what I'm using these days, works great.\n\nConnect to your instance over SSH using AWS's official steps: [Connecting to Your Linux Instance Using SSH](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-ssh.html)\n\n### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update system packages\nsudo apt update && sudo apt upgrade -y\n\n# Install dependencies\nsudo apt install -y curl git\n\n# Install Ollama\n# This script will create, enable, and start the Ollama systemd service\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Install Node.js and npm (for Chat UI)\ncurl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\nsudo apt install -y nodejs\n\n# Restart shell session to apply changes\nexec bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download and serve DeepSeek R1 on Ollama\n\nAlright, time to choose your DeepSeek R1 model variant based on what resources you have and what performance you need. In this walkthrough, I'm using DeepSeek R1 1\\.5B, which is the smallest version you'll find in most community runtimes.\n\nThe 1\\.5B model has, as the name suggests, 1\\.5 billion parameters. It runs really well on consumer hardware or modest cloud instances. Lower compute demands, but still delivers solid results for everyday chat and coding tasks. I was actually surprised how capable it is for its size.\n\nAs of late 2025, you've got options for larger variants in Ollama and similar runtimes too. There are several 7B and 8B options available, for instance. The larger models respond more coherently and reason better. But here's the thing, they need more memory. Let me give you a quick rule of thumb that I've learned the hard way:\n\n* CPU only with 16 to 32 GB RAM. Stick with 1\\.5B or a quantized 7B model.\n* Single mid range GPU with 16 to 24 GB VRAM. You can run 7B or 8B quantized models comfortably.\n* High VRAM GPUs. Consider the larger models if you need stronger reasoning and can handle the higher cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.5B version (smallest, lightweight, suitable for low-resource setups)\nollama pull deepseek-r1:1.5b\n\n# 8B version (mid-range, balances performance and resource usage)\nollama pull deepseek-r1:8b\n\n# 14B version (higher accuracy, requires more compute power)\nollama pull deepseek-r1:14b\n\n# 32B version (powerful, best for advanced tasks, needs high-end hardware)\nollama pull deepseek-r1:32b\n\n# 70B version (largest, highest performance, very resource-intensive)\nollama pull deepseek-r1:70b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the download finishes, list your installed models to make sure everything loaded properly. I always do this, learned my lesson after a corrupted download once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "$ ollama list\nNAME                ID              SIZE      MODIFIED\ndeepseek-r1:1.5b    a42b25d8c10a    1.1 GB    2 seconds ago"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ollama serves on [http://127\\.0\\.0\\.1:11434](http://127.0.0.1:11434) by default. Check that the service is healthy with these commands. Keep this API URL handy, you'll need it when you configure your chat UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Ollama is running and list downloaded models\ncurl http://127.0.0.1:11434/api/tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see output listing all the models available on your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n   \"models\":[\n      {\n         \"name\":\"deepseek-r1:1.5b\",\n         \"model\":\"deepseek-r1:1.5b\",\n         \"modified_at\":\"2025-02-01T17:05:07.520024256Z\",\n         \"size\":1117322599,\n         \"digest\":\"a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96\",\n         \"details\":{\n            \"parent_model\":\"\",\n            \"format\":\"gguf\",\n            \"family\":\"qwen2\",\n            \"families\":[\n               \"qwen2\"\n            ],\n            \"parameter_size\":\"1.8B\",\n            \"quantization_level\":\"Q4_K_M\"\n         }\n      }\n   ]\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the model with a simple generate call. This is where you'll know if everything's working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curl -X POST http://127.0.0.1:11434/api/generate -d '{\n  \"model\": \"deepseek-r1:1.5b\",\n  \"prompt\": \"What is Ollama?\",\n  \"num_predict\": 100,\n  \"stream\": false\n}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up the chat interface\n\nAlright, DeepSeek R1 is running. Next step is adding a chat UI so you can actually talk to your model from a browser. When you're ready to go beyond a basic interface, you might want to explore [advanced chatbot architectures that integrate knowledge graphs for richer, more accurate responses](/article/how-to-build-a-knowledge-graph-chatbot-with-neo4j-chainlit-gpt-4o-3).\n\n### Install MongoDB\n\nThe chat UI needs MongoDB to store conversation history. It won't work without it. I tried skipping this step once, doesn't work. The simplest approach is running a local MongoDB container with a persistent volume. Docker makes this easy and repeatable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sudo snap install docker\nsudo docker run -d -p 27017:27017 -v mongo-chat-ui:/data --name mongo-chat-ui mongo:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When MongoDB is running, you can access the database at: mongodb://localhost:27017\n\nYou'll add this URL to your chat UI configuration file (.env.local). Don't forget this step or you'll be scratching your head wondering why nothing's saving.\n\n### Download and install Clone Chat UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Clone Chat UI\ngit clone https://github.com/huggingface/chat-ui.git\ncd chat-ui\n\n#Install Dependencies\nnpm install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Chat UI\n\nUpdate your .env.local file with these values:\n\n* MongoDB URL: mongodb://localhost:27017\\. This is where your chat history gets stored.\n* Ollama Endpoint: [http://127\\.0\\.0\\.1:11434](http://127.0.0.1:11434). This is your local Ollama API.\n* Ollama Model Name: deepseek\\-r1:1\\.5b. Replace this with whatever model tag you actually installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a .env.local file:\nnano .env.local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can tweak these parameters to match your hardware and latency goals. I usually start conservative and then bump things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MONGODB_URL=mongodb://localhost:27017\nMODELS=`[\n  {\n    \"name\": \"DeepSeek-R1\",\n    \"chatPromptTemplate\": \"<s>{{#each messages}}{{#ifUser}}[INST] {{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s> {{/ifAssistant}}{{/each}}\",\n    \"parameters\": {\n      \"temperature\": 0.3,\n      \"top_p\": 0.95,\n      \"max_new_tokens\": 1024,\n      \"stop\": [\"</s>\"]\n    },\n    \"endpoints\": [\n      {\n        \"type\": \"ollama\",\n        \"url\" : \"http://127.0.0.1:11434\",\n        \"ollamaName\" : \"deepseek-r1:1.5b\" \n      }\n    ]\n  }\n]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you're done, save and exit. Use CTRL\\+X, then Y, then ENTER.\n\n## Use your very own DeepSeek R1 chatbot\n\nYou're ready to use your DeepSeek R1 chatbot. This is the fun part.\n\n### Start Chat UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the Chat UI in development mode, making it accessible on the network\n$ npm run dev -- --host 0.0.0.0\n\n# The output confirms the server is running and displays the accessible port\n> chat-ui@0.9.4 dev\n> vite dev --host 0.0.0.0\n\n\n  VITE v5.4.14  ready in 1122 ms\n\n  âžœ  Local:   http://localhost:5173/\n  âžœ  Network: http://100.00.00.000:5173/\n  âžœ  Network: http://100.00.0.0:5173/\n  âžœ  press h + enter to show help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're running on an AWS EC2 instance, remember to open the UI port in the instance security group. You can do this in the AWS Console under EC2, then Security Groups, then Inbound Rules. Or you can use the AWS CLI if you prefer. I always forget this step and then wonder why I can't connect.\n\nFor a public deployment, you should really consider adding a reverse proxy with HTTPS and enabling authentication. You want to protect both your Ollama endpoint and the chat UI. Seriously, don't skip this if you're going to production.\n\n### Access your chatbot\n\nOpen your machine's public address and port in a browser. You should see the chat interface.\n\n<img src='http://thegenairevolution.com/wp-content/uploads/2025/01/image-2-1024x644.png' alt='' title='' width='1024' height='644' /><img src='http://thegenairevolution.com/wp-content/uploads/2025/01/image-3-1024x534.png' alt='' title='' width='1024' height='534' />\n\n## Conclusion\n\nAnd there you have it. You now have a powerful AI model running under your control, on your own machine, inside your own security perimeter. Pretty cool, right?\n\n### Recap\n\n* You set up the environment by installing Ollama, MongoDB, and all the required dependencies.\n* You downloaded and configured DeepSeek R1 to run locally.\n* You set up a Chat UI and connected it to MongoDB and Ollama.\n* You made sure network access was working by opening the needed ports on AWS.\n* You accessed the chatbot from your browser.\n* Optional. You picked a GPU instance for faster responses and larger models.\n\nYour locally hosted DeepSeek R1 chatbot is now up and running. The whole process took me about 30 minutes the first time, and now I can spin one up in under 10\\. If you want to keep building your skills and plan your next projects, check out our [practical roadmap for aspiring GenAI developers](/article/practical-roadmap-for-aspiring-genai-developers)."
      ]
    }
  ],
  "metadata": {
    "title": "How to Deploy DeepSeek-R1 Locally with Ollama, MongoDB, and a Chat UI",
    "description": "Build a private DeepSeek-R1 chatbot with Ollama, MongoDB, and chat UIâ€”no external APIs. Deployment steps for local setups or AWS.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}