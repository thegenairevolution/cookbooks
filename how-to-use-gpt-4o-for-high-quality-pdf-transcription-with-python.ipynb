{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Use GPT-4o for High-Quality PDF Transcription with Python\n\n**Description:** Build a reliable, layout-preserving PDF transcription pipeline in Python with PyMuPDF and GPT-4o: step-by-step code, hybrid image+text, clean Markdown outputs.\n\n**ðŸ“– Read the full article:** [How to Use GPT-4o for High-Quality PDF Transcription with Python](https://blog.thegenairevolution.com/article/how-to-use-gpt-4o-for-high-quality-pdf-transcription-with-python)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You'll Build\n\nA Python pipeline that converts any PDF into clean, structured Markdown by combining PyMuPDF for rendering and text extraction with GPT\\-4o's vision capabilities. You'll preserve headings, tables, lists, and multi\\-column layoutsâ€”no broken OCR output.\n\n**Prerequisites:** Python 3\\.10\\+, an OpenAI API key, and a Colab or local environment. Expect around $0\\.01â€“0\\.05 per page at 200 DPI depending on content complexity. Runtime is roughly 5â€“10 seconds per page.\n\n**Scope note:** This pipeline works best with digital\\-native PDFs. Scanned PDFs with no embedded text will rely entirely on vision, which may reduce accuracy for dense or low\\-quality scans.\n\n## Why This Approach Works\n\nPure OCR tools like Tesseract miss layout and semanticsâ€”tables break, headings flatten, and multi\\-column text scrambles. Text\\-only extraction loses visual cues like borders and column flow.\n\nThis hybrid approach feeds GPT\\-4o both the embedded text (for accuracy) and the page image (for layout), letting the model reconstruct structure faithfully. The result: Markdown that mirrors the original document's hierarchy and reading order. If you've struggled with invisible characters or tokenization quirks breaking your pipeline, see our guide on tokenization pitfalls and invisible characters in prompts and RAG for normalization strategies.\n\nFor readers interested in extracting structured data directly from documents (e.g., invoices, forms), check out our walkthrough on building a structured data extraction pipeline with LLMs.\n\n**Trade\\-offs:** GPT\\-4o costs more than OCR but delivers higher fidelity. At 200 DPI, a 10\\-page document costs around $0\\.10â€“0\\.50\\. For large batches, consider caching and parallel processing (covered in Next Steps).\n\n## How It Works\n\n1. **Render pages to images** â€“ PyMuPDF converts each page to PNG at 200 DPI, preserving visual layout.\n2. **Extract embedded text** â€“ PyMuPDF pulls any text layer from the PDF for accuracy.\n3. **Transcribe with GPT\\-4o** â€“ Send both image and text to GPT\\-4o with a structured prompt. The model outputs Markdown.\n4. **Assemble final document** â€“ Concatenate per\\-page Markdown into one file.\n\n## Setup\n\nInstall dependencies in a single cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --quiet pymupdf pillow \"openai>=1.40.0,<2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key. For Colab, use this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For local environments, export the key in your shell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export OPENAI_API_KEY='your-key-here'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\nCentralize settings in a configuration cell for easy tuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n    \"dpi\": 200,\n    \"model\": \"gpt-4o\",\n    \"temperature\": 0.0,\n    \"max_retries\": 5,\n    \"initial_backoff\": 2.0,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Render Pages to Images\n\nPyMuPDF renders each page as a PNG at 200 DPI. This resolution balances quality and payload sizeâ€”higher DPI increases cost and latency without much fidelity gain for most documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nfrom pathlib import Path\nfrom typing import List\n\nimport fitz  # PyMuPDF\nfrom PIL import Image\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n\ndef ensure_dir(path: Path) -> None:\n    \"\"\"Create directory if it doesn't exist.\"\"\"\n    path.mkdir(parents=True, exist_ok=True)\n\ndef prepare_output_dirs(pdf_path: Path):\n    \"\"\"Set up output directories for images, text, and cache.\"\"\"\n    pdf_stem = pdf_path.stem\n    base_dir = Path(\"output\") / pdf_stem\n    images_dir = base_dir / \"images\"\n    txt_dir = base_dir / \"txt\"\n    cache_dir = base_dir / \".cache\"\n    ensure_dir(images_dir)\n    ensure_dir(txt_dir)\n    ensure_dir(cache_dir)\n    return base_dir, images_dir, txt_dir, cache_dir\n\ndef convert_pages_to_images(pdf_path: Path, images_dir: Path, dpi: int = 200) -> List[Path]:\n    \"\"\"Render each PDF page as a PNG at the specified DPI.\"\"\"\n    doc = fitz.open(pdf_path)\n    images = []\n    scale = dpi / 72  # PDF default is 72 DPI\n    matrix = fitz.Matrix(scale, scale)\n    \n    for page_index in range(len(doc)):\n        page = doc[page_index]\n        # Render without alpha channel to reduce payload size\n        pix = page.get_pixmap(matrix=matrix, alpha=False)\n        \n        # Guard against blank or failed renders\n        if pix.width == 0 or pix.height == 0:\n            logging.warning(f\"Page {page_index+1}: Rendering failed or empty page, skipping.\")\n            continue\n        \n        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n        out_path = images_dir / f\"page_{page_index + 1:03d}.png\"\n        img.save(out_path, format=\"PNG\", optimize=True)\n        images.append(out_path)\n        logging.info(f\"Saved image: {out_path} ({out_path.stat().st_size // 1024} KB)\")\n    \n    doc.close()\n    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract Embedded Text\n\nExtract any text layer from the PDF. This gives GPT\\-4o accurate character data to anchor its transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_page_texts(pdf_path: Path, txt_dir: Path) -> List[Path]:\n    \"\"\"Extract embedded text from each page and save as .txt files.\"\"\"\n    doc = fitz.open(pdf_path)\n    txt_files = []\n    \n    for page_index in range(len(doc)):\n        page = doc[page_index]\n        text = page.get_text(\"text\") or \"\"\n        text = text.replace(\"\\r\\n\", \"\\n\").strip()\n        \n        # Avoid polluting the prompt with placeholder text\n        if not text:\n            text = \"\"\n        \n        out_path = txt_dir / f\"page_{page_index + 1:03d}.txt\"\n        out_path.write_text(text, encoding=\"utf-8\")\n        txt_files.append(out_path)\n        logging.info(f\"Saved text: {out_path}\")\n    \n    doc.close()\n    return txt_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Transcribe with GPT\\-4o\n\nSend both the page image and extracted text to GPT\\-4o. The model uses the image for layout and the text for accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\nimport time\nfrom openai import OpenAI\n\n# Initialize client with environment variable\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef encode_image_to_data_url(image_path: Path) -> str:\n    \"\"\"Encode PNG as base64 data URL for API input.\"\"\"\n    with open(image_path, \"rb\") as f:\n        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n    return f\"data:image/png;base64,{b64}\"\n\nSYSTEM_PROMPT = (\n    \"You are a meticulous document transcription engine. \"\n    \"Transcribe each page into clean, well-structured Markdown. \"\n    \"Preserve headings, lists, tables, and reading order. \"\n    \"Use the extracted text for accuracy but follow the visual layout from the image. \"\n    \"Do not hallucinate content. If content is illegible, mark it clearly. \"\n    \"If extracted text is empty or indicates no embedded text, rely solely on the image.\"\n)\n\nUSER_TEMPLATE = (\n    \"Use both the page image and the extracted text below. \"\n    \"Reconstruct the document faithfully into Markdown.\\n\\n\"\n    \"Extracted text:\\n\\n{page_text}\"\n)\n\ndef transcribe_page_with_gpt4o(\n    client: OpenAI,\n    image_path: Path,\n    text_path: Path,\n    model: str = \"gpt-4o\",\n    temperature: float = 0.0,\n    max_retries: int = 5,\n    initial_backoff: float = 2.0,\n) -> str:\n    \"\"\"Send multimodal request to GPT-4o for Markdown transcription with retry logic.\"\"\"\n    data_url = encode_image_to_data_url(image_path)\n    page_text = text_path.read_text(encoding=\"utf-8\")\n    user_text = USER_TEMPLATE.format(page_text=page_text if page_text else \"[No embedded text]\")\n    \n    for attempt in range(max_retries):\n        try:\n            resp = client.chat.completions.create(\n                model=model,\n                temperature=temperature,\n                messages=[\n                    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": user_text},\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}},\n                        ],\n                    },\n                ],\n            )\n            content = resp.choices[0].message.content\n            return content.strip()\n        except Exception as e:\n            wait = initial_backoff * (2 ** attempt)\n            logging.warning(\n                f\"GPT-4o request failed (attempt {attempt+1}/{max_retries}): {type(e).__name__} - {e}. \"\n                f\"Retrying in {wait:.1f}s.\"\n            )\n            if attempt == max_retries - 1:\n                logging.error(\"Max retries reached. Raising exception.\")\n                raise\n            time.sleep(wait)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Assemble the Final Document\n\nLoop through all pages, transcribe each, and concatenate into one Markdown file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_pdf(pdf_path_str: str, dpi: int = 200) -> Path:\n    \"\"\"End-to-end pipeline: render, extract, transcribe, assemble.\"\"\"\n    pdf_path = Path(pdf_path_str).expanduser().resolve()\n    base_dir, images_dir, txt_dir, _ = prepare_output_dirs(pdf_path)\n    \n    # Render and extract\n    image_files = convert_pages_to_images(pdf_path, images_dir, dpi=dpi)\n    text_files = extract_page_texts(pdf_path, txt_dir)\n    \n    if len(image_files) != len(text_files):\n        raise RuntimeError(\"Mismatch between number of images and text files.\")\n    \n    # Transcribe each page\n    page_markdowns = []\n    for idx, (img, txt) in enumerate(zip(image_files, text_files), start=1):\n        logging.info(f\"Transcribing page {idx}/{len(image_files)}: {img.name}\")\n        try:\n            md = transcribe_page_with_gpt4o(client, img, txt, model=CONFIG[\"model\"], temperature=CONFIG[\"temperature\"])\n        except Exception as e:\n            logging.error(f\"Transcription failed for page {idx}: {e}\")\n            md = \"[Transcription failed for this page.]\"\n        page_markdowns.append(f\"---\\n\\n## Page {idx}\\n\\n{md}\\n\")\n    \n    # Write final transcript\n    transcript_path = base_dir / \"transcript.md\"\n    transcript_path.write_text(\"\\n\".join(page_markdowns), encoding=\"utf-8\")\n    logging.info(f\"Wrote transcript: {transcript_path}\")\n    return transcript_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\nUpload a sample PDF or use a local file. For Colab, upload with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\nuploaded = files.upload()\npdf_path = list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcript = process_pdf(pdf_path, dpi=CONFIG[\"dpi\"])\nprint(f\"Transcript saved to: {transcript}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import islice\n\ndef list_dir(p: Path, limit: int = 10) -> None:\n    \"\"\"List up to `limit` files in a directory.\"\"\"\n    files_list = sorted(p.glob(\"*\"))\n    for f in islice(files_list, 0, limit):\n        print(f.name)\n    if len(files_list) > limit:\n        print(f\"... and {len(files_list) - limit} more\")\n\nbase = Path(\"output\") / Path(pdf_path).stem\nprint(\"Images:\")\nlist_dir(base / \"images\")\nprint(\"\\nText files:\")\nlist_dir(base / \"txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display a rendered page:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display\n\nimg_path = base / \"images\" / \"page_001.png\"\nimg = Image.open(img_path)\ndisplay(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print extracted text sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_path = base / \"txt\" / \"page_001.txt\"\nprint(txt_path.read_text(encoding=\"utf-8\")[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the first 80 lines of the final Markdown:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcript_md = (base / \"transcript.md\").read_text(encoding=\"utf-8\")\nprint(\"\\n\".join(transcript_md.splitlines()[:80]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add lightweight validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assert at least one heading is present\nassert \"##\" in transcript_md or \"#\" in transcript_md, \"No headings found in transcript\"\nlogging.info(\"Validation passed: headings detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add Caching to Reduce Costs\n\nCache transcriptions to avoid redundant API calls during re\\-runs or iterative testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\nimport json\n\ndef page_cache_key(image_path: Path, text_path: Path, model: str, prompt_hash: str) -> str:\n    \"\"\"Generate cache key from image, text, model, and prompt.\"\"\"\n    h = hashlib.sha256()\n    h.update(image_path.read_bytes())\n    h.update(text_path.read_text(encoding=\"utf-8\").encode(\"utf-8\"))\n    h.update(model.encode(\"utf-8\"))\n    h.update(prompt_hash.encode(\"utf-8\"))\n    return h.hexdigest()\n\ndef transcribe_with_cache(\n    client: OpenAI,\n    image_path: Path,\n    text_path: Path,\n    cache_dir: Path,\n    model: str = \"gpt-4o\",\n    prompt_hash: str = \"\",\n) -> str:\n    \"\"\"Transcribe with caching to avoid redundant API calls.\"\"\"\n    key = page_cache_key(image_path, text_path, model, prompt_hash)\n    cache_file = cache_dir / f\"{key}.json\"\n    \n    if cache_file.exists():\n        logging.info(f\"Cache hit for {image_path.name}\")\n        return json.loads(cache_file.read_text(encoding=\"utf-8\"))[\"markdown\"]\n    \n    md = transcribe_page_with_gpt4o(client, image_path, text_path, model=model)\n    cache_file.write_text(json.dumps({\"markdown\": md}), encoding=\"utf-8\")\n    logging.info(f\"Cache saved for {image_path.name}\")\n    return md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update the pipeline to use caching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_pdf_with_cache(pdf_path_str: str, dpi: int = 200) -> Path:\n    \"\"\"Pipeline with per-page caching.\"\"\"\n    pdf_path = Path(pdf_path_str).expanduser().resolve()\n    base_dir, images_dir, txt_dir, cache_dir = prepare_output_dirs(pdf_path)\n    \n    # Hash prompts to invalidate cache if prompts change\n    prompt_hash = hashlib.sha256((SYSTEM_PROMPT + USER_TEMPLATE).encode(\"utf-8\")).hexdigest()\n    \n    image_files = convert_pages_to_images(pdf_path, images_dir, dpi=dpi)\n    text_files = extract_page_texts(pdf_path, txt_dir)\n    \n    if len(image_files) != len(text_files):\n        raise RuntimeError(\"Mismatch between number of images and text files.\")\n    \n    page_markdowns = []\n    for idx, (img, txt) in enumerate(zip(image_files, text_files), start=1):\n        logging.info(f\"Transcribing page {idx}/{len(image_files)}: {img.name}\")\n        try:\n            md = transcribe_with_cache(client, img, txt, cache_dir, model=CONFIG[\"model\"], prompt_hash=prompt_hash)\n        except Exception as e:\n            logging.error(f\"Transcription failed for page {idx}: {e}\")\n            md = \"[Transcription failed for this page.]\"\n        page_markdowns.append(f\"---\\n\\n## Page {idx}\\n\\n{md}\\n\")\n    \n    transcript_path = base_dir / \"transcript.md\"\n    transcript_path.write_text(\"\\n\".join(page_markdowns), encoding=\"utf-8\")\n    logging.info(f\"Wrote transcript: {transcript_path}\")\n    return transcript_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run with caching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcript = process_pdf_with_cache(pdf_path, dpi=CONFIG[\"dpi\"])\nprint(f\"Transcript saved to: {transcript}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You Get\n\nA per\\-page Markdown transcription assembled into one document. Headings map to original styles, lists are preserved, tables stay coherent, and multi\\-column content reads in the right order. The output is ready for indexing, search, or publishing.\n\nFor more on how LLMs handle context and why managing memory is crucial for large documents, see our article on context rot and why LLMs \"forget\" as their memory grows.\n\n## Next Steps\n\n* **Parallel processing:** Use concurrent.futures.ThreadPoolExecutor to transcribe multiple pages simultaneously and reduce total runtime.\n* **Payload optimization:** Downscale images above a byte\\-size threshold to stay under API limits and reduce costs.\n* **Observability:** Add structured logging with error types, status codes, and per\\-page cost estimates for production monitoring.\n* **Deployment:** Wrap the pipeline in a FastAPI endpoint or Streamlit app for team use."
      ]
    }
  ],
  "metadata": {
    "title": "How to Use GPT-4o for High-Quality PDF Transcription with Python",
    "description": "Build a reliable, layout-preserving PDF transcription pipeline in Python with PyMuPDF and GPT-4o: step-by-step code, hybrid image+text, clean Markdown outputs.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}