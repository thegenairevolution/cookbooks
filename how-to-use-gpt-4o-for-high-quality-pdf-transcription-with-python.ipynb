{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Use GPT-4o for High-Quality PDF Transcription with Python\n\n**Description:** Build a reliable, layout-preserving PDF transcription pipeline in Python with PyMuPDF and GPT-4o: step-by-step code, hybrid image+text, clean Markdown outputs.\n\n**ðŸ“– Read the full article:** [How to Use GPT-4o for High-Quality PDF Transcription with Python](https://blog.thegenairevolution.com/article/how-to-use-gpt-4o-for-high-quality-pdf-transcription-with-python-3)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I've been working with PDF extraction for a while now, and let me tell youâ€”it's one of those problems that seems simple until you actually try to solve it. You know what I mean? You think \"just extract the text\" and then you end up with scrambled tables and paragraphs that jump between columns like they're playing hopscotch.\n\nSo here's what I figured out: combine PyMuPDF for rendering with GPT\\-4o's vision capabilities. This approach actually preserves your headings, tables, lists, even those annoying multi\\-column layouts that usually turn into word soup.\n\nPrerequisites are pretty straightforward: Python 3\\.10\\+, an OpenAI API key, and either Colab or a local environment. Cost\\-wise, you're looking at about $0\\.01â€“0\\.05 per page at 200 DPI. Processing time runs around 5â€“10 seconds per page, depending on how complex your content is.\n\nOne thing to noteâ€”this works best with digital\\-native PDFs. If you're dealing with scanned documents that have no embedded text, the whole thing relies on vision capabilities, which... well, let's just say the accuracy takes a hit with dense or low\\-quality scans.\n\n## Why This Approach Works\n\nPure OCR tools like Tesseract? They completely miss the point. I learned this the hard way after spending hours trying to fix broken table outputs. Tables break apart, headings get flattened into regular text, and don't even get me started on multi\\-column layoutsâ€”they scramble into this unreadable mess that makes you question your life choices.\n\nAnd text\\-only extraction isn't much better. Sure, you get the words, but you lose all those visual cues. The borders, the column flow, the actual structure that makes a document readable.\n\nWhat I discovered is that this hybrid approach feeds GPT\\-4o both pieces of the puzzleâ€”the embedded text for accuracy and the page image for layout understanding. The model then reconstructs everything faithfully. You actually get Markdown that mirrors the original document's hierarchy and reading order. If you've been struggling with invisible characters or weird tokenization issues breaking your pipeline, check out our guide on [tokenization pitfalls and invisible characters in prompts and RAG](link) for some normalization strategies that saved my sanity.\n\nFor those of you looking to extract structured data directly from documents (invoices, forms, that sort of thing), we've got a walkthrough on [building a structured data extraction pipeline with LLMs](link) that might be helpful.\n\nNow, about trade\\-offsâ€”GPT\\-4o definitely costs more than traditional OCR. At 200 DPI, a 10\\-page document will run you somewhere between $0\\.10â€“0\\.50\\. But honestly? The fidelity is worth it. For large batches though, you'll want parallel processing, which I'll cover later.\n\n## How It Works\n\nThe process breaks down into four main steps:\n\n1. **Render pages to images** â€“ PyMuPDF converts each page to PNG at 200 DPI. This preserves the visual layout exactly as it appears.\n2. **Extract embedded text** â€“ PyMuPDF pulls any text layer from the PDF. This gives us the accurate character data we need.\n3. **Transcribe with GPT\\-4o** â€“ We send both the image and text to GPT\\-4o with a structured prompt. The model spits out clean Markdown.\n4. **Assemble final document** â€“ Take all those per\\-page Markdown chunks and concatenate them into one file.\n\n## Setup\n\nFirst, let's get our dependencies sorted. Install everything in one go:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --quiet pymupdf pillow \"openai>=1.40.0,<2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next up, your OpenAI API key. If you're using Colab, here's what you need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Working locally? Just export the key in your shell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export OPENAI_API_KEY='your-key-here'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\nI like to keep all my settings in one place. Makes it easier to tweak things later without hunting through the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n    \"dpi\": 200,\n    \"model\": \"gpt-4o\",\n    \"temperature\": 0.0,\n    \"max_retries\": 5,\n    \"initial_backoff\": 2.0,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Render Pages to Images\n\nPyMuPDF renders each page as a PNG at 200 DPI. Why 200 DPI? I tested a bunch of different resolutions, and this one hits the sweet spot between quality and payload size. Go higher and you're just burning money without much improvement. Trust me on this one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nfrom pathlib import Path\nfrom typing import List\n\nimport fitz  # PyMuPDF\nfrom PIL import Image\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n\ndef ensure_dir(path: Path) -> None:\n    \"\"\"Create directory if it doesn't exist.\"\"\"\n    path.mkdir(parents=True, exist_ok=True)\n\ndef prepare_output_dirs(pdf_path: Path):\n    \"\"\"Set up output directories for images, text, and cache.\"\"\"\n    pdf_stem = pdf_path.stem\n    base_dir = Path(\"output\") / pdf_stem\n    images_dir = base_dir / \"images\"\n    txt_dir = base_dir / \"txt\"\n    cache_dir = base_dir / \".cache\"\n    ensure_dir(images_dir)\n    ensure_dir(txt_dir)\n    ensure_dir(cache_dir)\n    return base_dir, images_dir, txt_dir, cache_dir\n\ndef convert_pages_to_images(pdf_path: Path, images_dir: Path, dpi: int = 200) -> List[Path]:\n    \"\"\"Render each PDF page as a PNG at the specified DPI.\"\"\"\n    doc = fitz.open(pdf_path)\n    images = []\n    scale = dpi / 72  # PDF default is 72 DPI\n    matrix = fitz.Matrix(scale, scale)\n    \n    for page_index in range(len(doc)):\n        page = doc[page_index]\n        # Render without alpha channel to reduce payload size\n        pix = page.get_pixmap(matrix=matrix, alpha=False)\n        \n        # Guard against blank or failed renders\n        if pix.width == 0 or pix.height == 0:\n            logging.warning(f\"Page {page_index+1}: Rendering failed or empty page, skipping.\")\n            continue\n        \n        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n        out_path = images_dir / f\"page_{page_index + 1:03d}.png\"\n        img.save(out_path, format=\"PNG\", optimize=True)\n        images.append(out_path)\n        logging.info(f\"Saved image: {out_path} ({out_path.stat().st_size // 1024} KB)\")\n    \n    doc.close()\n    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract Embedded Text\n\nThis step pulls any text layer from the PDF. It gives GPT\\-4o the accurate character data it needs to anchor its transcription. Skip this and you're basically asking the model to work blindfoldedâ€”not ideal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_page_texts(pdf_path: Path, txt_dir: Path) -> List[Path]:\n    \"\"\"Extract embedded text from each page and save as .txt files.\"\"\"\n    doc = fitz.open(pdf_path)\n    txt_files = []\n    \n    for page_index in range(len(doc)):\n        page = doc[page_index]\n        text = page.get_text(\"text\") or \"\"\n        text = text.replace(\"\\r\\n\", \"\\n\").strip()\n        \n        # Avoid polluting the prompt with placeholder text\n        if not text:\n            text = \"\"\n        \n        out_path = txt_dir / f\"page_{page_index + 1:03d}.txt\"\n        out_path.write_text(text, encoding=\"utf-8\")\n        txt_files.append(out_path)\n        logging.info(f\"Saved text: {out_path}\")\n    \n    doc.close()\n    return txt_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Transcribe with GPT\\-4o\n\nHere's where the magic happens. We send both the page image and the extracted text to GPT\\-4o. The model uses the image to understand layout and the text for accuracy. It's actually pretty clever how it combines both signalsâ€”like having two different perspectives on the same problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\nimport time\nfrom openai import OpenAI\n\n# Initialize client with environment variable\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef encode_image_to_data_url(image_path: Path) -> str:\n    \"\"\"Encode PNG as base64 data URL for API input.\"\"\"\n    with open(image_path, \"rb\") as f:\n        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n    return f\"data:image/png;base64,{b64}\"\n\nSYSTEM_PROMPT = (\n    \"You are a meticulous document transcription engine. \"\n    \"Transcribe each page into clean, well-structured Markdown. \"\n    \"Preserve headings, lists, tables, and reading order. \"\n    \"Use the extracted text for accuracy but follow the visual layout from the image. \"\n    \"Do not hallucinate content. If content is illegible, mark it clearly. \"\n    \"If extracted text is empty or indicates no embedded text, rely solely on the image.\"\n)\n\nUSER_TEMPLATE = (\n    \"Use both the page image and the extracted text below. \"\n    \"Reconstruct the document faithfully into Markdown.\\n\\n\"\n    \"Extracted text:\\n\\n{page_text}\"\n)\n\ndef transcribe_page_with_gpt4o(\n    client: OpenAI,\n    image_path: Path,\n    text_path: Path,\n    model: str = \"gpt-4o\",\n    temperature: float = 0.0,\n    max_retries: int = 5,\n    initial_backoff: float = 2.0,\n) -> str:\n    \"\"\"Send multimodal request to GPT-4o for Markdown transcription with retry logic.\"\"\"\n    data_url = encode_image_to_data_url(image_path)\n    page_text = text_path.read_text(encoding=\"utf-8\")\n    user_text = USER_TEMPLATE.format(page_text=page_text if page_text else \"[No embedded text]\")\n    \n    for attempt in range(max_retries):\n        try:\n            resp = client.chat.completions.create(\n                model=model,\n                temperature=temperature,\n                messages=[\n                    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": user_text},\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}},\n                        ],\n                    },\n                ],\n            )\n            content = resp.choices[0].message.content\n            return content.strip()\n        except Exception as e:\n            wait = initial_backoff * (2 ** attempt)\n            logging.warning(\n                f\"GPT-4o request failed (attempt {attempt+1}/{max_retries}): {type(e).__name__} - {e}. \"\n                f\"Retrying in {wait:.1f}s.\"\n            )\n            if attempt == max_retries - 1:\n                logging.error(\"Max retries reached. Raising exception.\")\n                raise\n            time.sleep(wait)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Assemble the Final Document\n\nNow we just loop through all the pages, transcribe each one, and stitch them together into one Markdown file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_pdf(pdf_path_str: str, dpi: int = 200) -> Path:\n    \"\"\"End-to-end pipeline: render, extract, transcribe, assemble.\"\"\"\n    pdf_path = Path(pdf_path_str).expanduser().resolve()\n    base_dir, images_dir, txt_dir, _ = prepare_output_dirs(pdf_path)\n    \n    # Render and extract\n    image_files = convert_pages_to_images(pdf_path, images_dir, dpi=dpi)\n    text_files = extract_page_texts(pdf_path, txt_dir)\n    \n    if len(image_files) != len(text_files):\n        raise RuntimeError(\"Mismatch between number of images and text files.\")\n    \n    # Transcribe each page\n    page_markdowns = []\n    for idx, (img, txt) in enumerate(zip(image_files, text_files), start=1):\n        logging.info(f\"Transcribing page {idx}/{len(image_files)}: {img.name}\")\n        try:\n            md = transcribe_page_with_gpt4o(client, img, txt, model=CONFIG[\"model\"], temperature=CONFIG[\"temperature\"])\n        except Exception as e:\n            logging.error(f\"Transcription failed for page {idx}: {e}\")\n            md = \"[Transcription failed for this page.]\"\n        page_markdowns.append(f\"---\\n\\n## Page {idx}\\n\\n{md}\\n\")\n    \n    # Write final transcript\n    transcript_path = base_dir / \"transcript.md\"\n    transcript_path.write_text(\"\\n\".join(page_markdowns), encoding=\"utf-8\")\n    logging.info(f\"Wrote transcript: {transcript_path}\")\n    return transcript_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\nTime to test this thing out. If you're in Colab, upload a sample PDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\nuploaded = files.upload()\npdf_path = list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then run the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcript = process_pdf(pdf_path, dpi=CONFIG[\"dpi\"])\nprint(f\"Transcript saved to: {transcript}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's inspect what we got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import islice\n\ndef list_dir(p: Path, limit: int = 10) -> None:\n    \"\"\"List up to `limit` files in a directory.\"\"\"\n    files_list = sorted(p.glob(\"*\"))\n    for f in islice(files_list, 0, limit):\n        print(f.name)\n    if len(files_list) > limit:\n        print(f\"... and {len(files_list) - limit} more\")\n\nbase = Path(\"output\") / Path(pdf_path).stem\nprint(\"Images:\")\nlist_dir(base / \"images\")\nprint(\"\\nText files:\")\nlist_dir(base / \"txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Want to see one of the rendered pages?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display\n\nimg_path = base / \"images\" / \"page_001.png\"\nimg = Image.open(img_path)\ndisplay(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check out a sample of the extracted text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_path = base / \"txt\" / \"page_001.txt\"\nprint(txt_path.read_text(encoding=\"utf-8\")[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here's the first 80 lines of your final Markdown:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcript_md = (base / \"transcript.md\").read_text(encoding=\"utf-8\")\nprint(\"\\n\".join(transcript_md.splitlines()[:80]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I always add some basic validation too. Better to catch issues early:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assert at least one heading is present\nassert \"##\" in transcript_md or \"#\" in transcript_md, \"No headings found in transcript\"\nlogging.info(\"Validation passed: headings detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You Get\n\nYou end up with a per\\-page Markdown transcription assembled into one clean document. Headings map correctly to their original styles, lists stay intact, tables remain coherent, andâ€”this is the big oneâ€”multi\\-column content actually reads in the right order.\n\nLet me be more specific here. You get clean Markdown that you can immediately drop into your documentation system, knowledge base, whatever you're building. No post\\-processing, no cleanup, no fixing broken formatting. It just works.\n\nActually, if you're curious about how LLMs handle context and why managing memory matters for large documents, our article on [context rot and why LLMs \"forget\" as their memory grows](link) dives into that whole mess.\n\n## Next Steps\n\n* **Parallel processing**: Use `concurrent.futures.ThreadPoolExecutor` to transcribe multiple pages at once. In my experience, this cuts processing time by 60\\-70%. Game changer for larger documents.\n* **Payload optimization**: Downscale images that are above your byte\\-size threshold. Keeps you under API limits and saves money.\n* **Observability**: Add structured loggingâ€”error types, status codes, per\\-page cost estimates. Trust me, when something breaks at 2 AM, you'll be glad you have proper monitoring.\n* **Deployment**: Wrap this whole thing in a FastAPI endpoint or throw together a Streamlit app. Makes it way easier for your team to use without having to touch the code."
      ]
    }
  ],
  "metadata": {
    "title": "How to Use GPT-4o for High-Quality PDF Transcription with Python",
    "description": "Build a reliable, layout-preserving PDF transcription pipeline in Python with PyMuPDF and GPT-4o: step-by-step code, hybrid image+text, clean Markdown outputs.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}