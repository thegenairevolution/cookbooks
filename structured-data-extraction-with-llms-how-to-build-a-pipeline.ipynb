{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Structured Data Extraction with LLMs: How to Build a Pipeline\n\n**Description:** Build a reliable structured data extraction pipeline using LLMs, LangChain, and OpenAI functionsâ€”JSON schemas, deterministic outputs, zero hallucinations, for production.\n\n**ðŸ“– Read the full article:** [Structured Data Extraction with LLMs: How to Build a Pipeline](https://blog.thegenairevolution.com/article/structured-data-extraction-with-llms-how-to-build-a-pipeline)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "High\\-quality structured data unlocks downstream analytics, automation, and search. This guide shows you exactly how to turn messy text into validated JSON using LLMs, OpenAI function calling, LangChain, and Pydanticâ€”no training data, no brittle regex. You'll build and run a Colab\\-ready, deterministic extraction pipeline that converts raw text into validated JSON. If you're dealing with unpredictable input, understanding tokenization pitfalls and invisible characters can help you avoid subtle extraction bugs.\n\n## Why This Approach Works\n\n**Function Calling Enforces Structure** OpenAI function calling forces the model to return JSON matching your schema. No free\\-form text, no hallucinated fields. The model outputs only what you define.\n\n**Pydantic Validates at the Boundary** Pydantic models enforce types, required fields, and constraints at runtime. Invalid payloads fail fast with clear error messages, ensuring downstream systems receive clean data.\n\n**LangChain Orchestrates Composable Pipelines** LangChain chains combine prompts, models, and parsers into testable, reusable pipelines. You can swap models, extend schemas, or add retry logic without rewriting core extraction logic.\n\n**Deterministic Output with Temperature Zero** Setting temperature to zero eliminates randomness. The same input produces the same output every time, making extraction predictable and testable.\n\n## How It Works\n\n1. **Define Schema**: Use Pydantic models to specify the structure of extracted data (e.g., event name, date, outcome).\n2. **Convert to Function Spec**: Transform the Pydantic model into an OpenAI function definition.\n3. **Bind Function to Model**: Attach the function spec to the LLM so it knows to return structured JSON.\n4. **Create Prompt**: Write a strict system prompt instructing the model to extract only explicit information.\n5. **Build Chain**: Compose prompt, model, and output parser into a LangChain pipeline.\n6. **Invoke and Validate**: Run the chain on input text and validate the output with Pydantic.\n\n## Setup \\& Installation\n\nRun this cell at the top of your Colab notebook to install all required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U \"langchain>=0.2\" \"langchain-openai>=0.1\" \"langchain-community>=0.2\" \"langchain-text-splitters>=0.0.1\" pydantic python-dotenv beautifulsoup4 html2text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key as an environment variable before running the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\nrequired_keys = [\"OPENAI_API_KEY\"]\nmissing = [k for k in required_keys if not os.getenv(k)]\nif missing:\n    raise EnvironmentError(\n        f\"Missing required environment variables: {', '.join(missing)}\\n\"\n        \"Please set them before running the notebook. Example:\\n\"\n        \"  export OPENAI_API_KEY='your-key-here'\"\n    )\n\nprint(\"All required API keys found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Step 1: Initialize the LLM\n\nLoad environment variables and initialize the OpenAI model with temperature zero for deterministic output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n\n# Use gpt-4o-mini for cost-effective, fast extraction with function calling support\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nprint(\"LLM ready:\", llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define Pydantic Models\n\nCreate Pydantic models to define the structure of extracted events. Field descriptions guide the LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass Event(BaseModel):\n    \"\"\"\n    Represents a single event extracted from text.\n    \"\"\"\n    name: str = Field(..., description=\"The explicit event name or title extracted verbatim from the text.\")\n    date: Optional[str] = Field(None, description=\"The explicit date as written in the text (ISO if present, else raw).\")\n    outcome: Optional[str] = Field(None, description=\"The explicit outcome/result stated in the text, if any.\")\n\nclass Extracted(BaseModel):\n    \"\"\"\n    Wrapper model for a list of extracted events.\n    \"\"\"\n    events: List[Event] = Field(default_factory=list, description=\"All events explicitly mentioned in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Convert Schema to OpenAI Function Spec\n\nTransform the Pydantic model into an OpenAI function definition so the model knows to return structured JSON:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n\nextract_fn = convert_to_openai_function(Extracted)\n\nfunctions = [extract_fn]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Create a Strict System Prompt\n\nWrite a system prompt that instructs the model to extract only explicit information and avoid hallucination:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n\nSYSTEM_PROMPT = \"\"\"You are a precise information extractor.\n- Extract only information explicitly present in the text.\n- Do not infer, guess, or add missing details.\n- If a field is not explicitly present, set it to null.\n- If no events are present, return an empty list.\n- Preserve original wording where reasonable.\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", SYSTEM_PROMPT),\n        (\"human\", \"{text}\")\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Bind Function to Model and Set Up Parsers\n\nBind the function spec to the LLM and configure output parsers to extract structured data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.openai_functions import (\n    JsonKeyOutputFunctionsParser,\n    JsonOutputFunctionsParser,\n)\n\n# Bind the function spec to the LLM\nmodel_with_fn = llm.bind(functions=functions)\n\n# Parser that returns only the \"events\" key from the function arguments\nevents_only_parser = JsonKeyOutputFunctionsParser(key_name=\"events\")\n\n# Parser that returns the entire function arguments payload\nfull_args_parser = JsonOutputFunctionsParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Build LangChain Pipelines\n\nCompose the prompt, model, and parsers into reusable chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain that returns only the list of events\nevents_chain = prompt | model_with_fn | events_only_parser\n\n# Chain that returns the full structured payload for validation\nfull_chain = prompt | model_with_fn | full_args_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\n### Basic Extraction\n\nTest the pipeline on a simple input string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"I attended a music festival on June 15th and a tech conference on July 20th.\"\nevents = events_chain.invoke({\"text\": text})\nprint(events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate with Pydantic\n\nUse the full chain to extract the payload and validate it with Pydantic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "payload = full_chain.invoke({\"text\": text})\nvalidated = Extracted.model_validate(payload)\nprint(validated)\nfor ev in validated.events:\n    print(ev.name, ev.date, ev.outcome)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edge Case: No Events\n\nVerify the extractor returns an empty list when no events are present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_irrelevant = \"This is irrelevant text with no events.\"\nempty = events_chain.invoke({\"text\": text_irrelevant})\nprint(empty)\nassert empty == [], \"Extractor should not hallucinate events.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edge Case: Missing Fields\n\nTest extraction when some fields are missing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_partial = \"Our team hosted Launch Day and later Demo Night.\"\npartial = events_chain.invoke({\"text\": text_partial})\nprint(partial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Determinism Check\n\nRun the same input multiple times and confirm outputs are identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "same1 = events_chain.invoke({\"text\": text})\nsame2 = events_chain.invoke({\"text\": text})\nassert same1 == same2, \"Outputs should be identical with temperature=0.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real\\-World Data Extraction\n\nLoad a Wikipedia page and extract events from real content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Apollo_program\")\ndocs = loader.load()\npage_text = docs[0].page_content[:10000]\n\nreal_events = events_chain.invoke({\"text\": page_text})\nprint(f\"Extracted {len(real_events)} events\")\nfor e in real_events[:5]:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chunking Long Documents\n\nFor longer documents, split text into overlapping chunks, extract per chunk, then merge and deduplicate events. If you notice models missing details or hallucinating as context grows, our deep dive on context rot and LLM memory limits explains why this happens and how to mitigate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=400)\nchunks = splitter.split_text(docs[0].page_content)\n\nall_events = []\nfor ch in chunks:\n    all_events.extend(events_chain.invoke({\"text\": ch}))\n\n# Deduplicate by (name, date) tuple\nunique = {(e[\"name\"], e.get(\"date\")): e for e in all_events}\nchr = list(unique.values())\nprint(f\"Merged events: {len(merged_events)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constraints and Performance\n\n**Token Limits**: gpt\\-4o\\-mini supports up to 128k tokens input. For documents over 4k characters, chunk the text to avoid context window issues.\n\n**Cost**: gpt\\-4o\\-mini costs approximately $0\\.15 per 1M input tokens and $0\\.60 per 1M output tokens. A typical extraction call uses 500â€“2000 tokens.\n\n**Latency**: Expect 1â€“3 seconds per extraction call depending on input size and API load.\n\nFor high\\-volume jobs, control prompt size, reduce chunk overlap, and prefer cheaper models like gpt\\-4o\\-mini for extraction. If you're evaluating which model best fits your pipeline's speed, cost, and reliability needs, see our practical guide on how to choose an LLM for your application.\n\n## Conclusion\n\nYou've built a deterministic, validated extraction pipeline that converts raw text into structured JSON using OpenAI function calling, Pydantic, and LangChain. The system enforces schema compliance, eliminates hallucination, and produces repeatable results.\n\n**Key Design Choices:**\n\n* Function calling forces the model to return only schema\\-compliant JSON.\n* Pydantic validation catches invalid payloads at runtime.\n* LangChain orchestration makes the pipeline composable, testable, and extensible.\n* Temperature zero ensures deterministic output.\n\n\n**Next Steps:**\n\n* Add retry logic with exponential backoff for production reliability.\n* Extend the schema with new fields like location or confidence scores.\n* Deploy the pipeline as a REST API using FastAPI.\n* Parallelize extraction across multiple documents with asyncio and rate limiting.\n* Add observability with structured logging and input/output hashing to track performance and avoid logging PII."
      ]
    }
  ],
  "metadata": {
    "title": "Structured Data Extraction with LLMs: How to Build a Pipeline",
    "description": "Build a reliable structured data extraction pipeline using LLMs, LangChain, and OpenAI functionsâ€”JSON schemas, deterministic outputs, zero hallucinations, for production.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}