{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Structured Data Extraction with LLMs: How to Build a Pipeline\n\n**Description:** Build a reliable structured data extraction pipeline using LLMs, LangChain, and OpenAI functions: JSON schemas, deterministic outputs, zero hallucinations, for production.\n\n**ðŸ“– Read the full article:** [Structured Data Extraction with LLMs: How to Build a Pipeline](https://blog.thegenairevolution.com/article/structured-data-extraction-with-llms-how-to-build-a-pipeline-2)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "High\\-quality structured data unlocks downstream analytics, automation, and search. Here's the thing \\- if you're dealing with messy text and need clean JSON, you don't need training data or brittle regex patterns anymore. I'll show you exactly how to build a deterministic extraction pipeline using LLMs, OpenAI function calling, LangChain, and Pydantic. This approach works reliably, and you can run it right in Colab.\n\nActually, before we dive in, let me mention something important: if you're dealing with unpredictable input, understanding tokenization pitfalls and invisible characters can save you hours of debugging. These subtle extraction bugs can drive you crazy if you don't know what to look for.\n\n## Why This Approach Works\n\n**Function Calling Enforces Structure** OpenAI function calling is honestly a game\\-changer. It forces the model to return JSON that matches your exact schema \\- no free\\-form text, no hallucinated fields. The model outputs only what you define, period.\n\n**Pydantic Validates At The Boundary** I've come to really appreciate Pydantic models. They enforce types, required fields, and constraints at runtime. When something's wrong, you get clear error messages immediately. Your downstream systems receive clean data, or they don't receive anything at all \\- which is exactly what you want.\n\n**LangChain Orchestrates Composable Pipelines** Here's what makes LangChain particularly useful: you can combine prompts, models, and parsers into testable, reusable pipelines. Need to swap models? Extend schemas? Add retry logic? You can do all that without touching your core extraction logic.\n\n**Deterministic Output With Temperature Zero** Setting temperature to zero eliminates randomness completely. Same input, same output, every single time. This makes extraction predictable and, more importantly, testable.\n\n## How It Works\n\n1. **Define Schema**: Use Pydantic models to specify your data structure (event name, date, outcome \\- whatever you need).\n2. **Convert To Function Spec**: Transform that Pydantic model into an OpenAI function definition.\n3. **Bind Function To Model**: Attach the function spec to the LLM so it knows to return structured JSON.\n4. **Create Prompt**: Write a strict system prompt that tells the model to extract only explicit information.\n5. **Build Chain**: Compose everything \\- prompt, model, and output parser \\- into a LangChain pipeline.\n6. **Invoke And Validate**: Run the chain on your input text and validate the output with Pydantic.\n\n## Setup \\& Installation\n\nFirst thing \\- run this cell at the top of your Colab notebook to install all the dependencies you'll need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U \"langchain>=0.2\" \"langchain-openai>=0.1\" \"langchain-community>=0.2\" \"langchain-text-splitters>=0.0.1\" pydantic python-dotenv beautifulsoup4 html2text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And don't forget to set your OpenAI API key as an environment variable before running anything:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\nrequired_keys = [\"OPENAI_API_KEY\"]\nmissing = [k for k in required_keys if not os.getenv(k)]\nif missing:\n    raise EnvironmentError(\n        f\"Missing required environment variables: {', '.join(missing)}\\n\"\n        \"Please set them before running the notebook. Example:\\n\"\n        \"  export OPENAI_API_KEY='your-key-here'\"\n    )\n\nprint(\"All required API keys found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step\\-by\\-Step Implementation\n\n### Step 1: Initialize The LLM\n\nLet's start by loading environment variables and initializing the OpenAI model. Notice we're setting temperature to zero for deterministic output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n\n# Use gpt-4o-mini for cost-effective, fast extraction with function calling support\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nprint(\"LLM ready:\", llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define Pydantic Models\n\nNow we create Pydantic models to define our extracted event structure. The field descriptions are actually important here \\- they guide the LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional\nfrom pydantic import BaseModel, Field\n\nclass Event(BaseModel):\n    \"\"\"\n    Represents a single event extracted from text.\n    \"\"\"\n    name: str = Field(..., description=\"The explicit event name or title extracted verbatim from the text.\")\n    date: Optional[str] = Field(None, description=\"The explicit date as written in the text (ISO if present, else raw).\")\n    outcome: Optional[str] = Field(None, description=\"The explicit outcome/result stated in the text, if any.\")\n\nclass Extracted(BaseModel):\n    \"\"\"\n    Wrapper model for a list of extracted events.\n    \"\"\"\n    events: List[Event] = Field(default_factory=list, description=\"All events explicitly mentioned in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Convert Schema To OpenAI Function Spec\n\nThis step transforms your Pydantic model into something OpenAI understands \\- a function definition that tells the model to return structured JSON:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n\nextract_fn = convert_to_openai_function(Extracted)\n\nfunctions = [extract_fn]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Create A Strict System Prompt\n\nThe prompt is crucial. You need to be explicit about extracting only what's actually in the text, no hallucination:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n\nSYSTEM_PROMPT = \"\"\"You are a precise information extractor.\n- Extract only information explicitly present in the text.\n- Do not infer, guess, or add missing details.\n- If a field is not explicitly present, set it to null.\n- If no events are present, return an empty list.\n- Preserve original wording where reasonable.\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", SYSTEM_PROMPT),\n        (\"human\", \"{text}\")\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Bind Function To Model And Set Up Parsers\n\nHere we bind the function spec to the LLM and configure our output parsers to extract structured data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.openai_functions import (\n    JsonKeyOutputFunctionsParser,\n    JsonOutputFunctionsParser,\n)\n\n# Bind the function spec to the LLM\nmodel_with_fn = llm.bind(functions=functions)\n\n# Parser that returns only the \"events\" key from the function arguments\nevents_only_parser = JsonKeyOutputFunctionsParser(key_name=\"events\")\n\n# Parser that returns the entire function arguments payload\nfull_args_parser = JsonOutputFunctionsParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Build LangChain Pipelines\n\nNow let's compose everything into reusable chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain that returns only the list of events\nevents_chain = prompt | model_with_fn | events_only_parser\n\n# Chain that returns the full structured payload for validation\nfull_chain = prompt | model_with_fn | full_args_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run And Validate\n\n### Basic Extraction\n\nLet's test the pipeline on something simple first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"I attended a music festival on June 15th and a tech conference on July 20th.\"\nevents = events_chain.invoke({\"text\": text})\nprint(events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate With Pydantic\n\nUse the full chain to extract and validate with Pydantic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "payload = full_chain.invoke({\"text\": text})\nvalidated = Extracted.model_validate(payload)\nprint(validated)\nfor ev in validated.events:\n    print(ev.name, ev.date, ev.outcome)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edge Case: No Events\n\nAlways verify your extractor returns an empty list when there's nothing to extract:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_irrelevant = \"This is irrelevant text with no events.\"\nempty = events_chain.invoke({\"text\": text_irrelevant})\nprint(empty)\nassert empty == [], \"Extractor should not hallucinate events.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edge Case: Missing Fields\n\nWhat happens when some fields are missing? Let's test that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_partial = \"Our team hosted Launch Day and later Demo Night.\"\npartial = events_chain.invoke({\"text\": text_partial})\nprint(partial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Determinism Check\n\nThis is important \\- run the same input multiple times and make sure you get identical outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "same1 = events_chain.invoke({\"text\": text})\nsame2 = events_chain.invoke({\"text\": text})\nassert same1 == same2, \"Outputs should be identical with temperature=0.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real\\-World Data Extraction\n\nNow for something more interesting \\- let's load a Wikipedia page and extract events from actual content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Apollo_program\")\ndocs = loader.load()\npage_text = docs[0].page_content[:10000]\n\nreal_events = events_chain.invoke({\"text\": page_text})\nprint(f\"Extracted {len(real_events)} events\")\nfor e in real_events[:5]:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunking Long Documents\n\nFor longer documents, you'll need to split text into overlapping chunks, extract from each chunk, then merge and deduplicate events. Actually, if you notice models missing details or hallucinating as context grows, our deep dive on context rot and LLM memory limits explains exactly why this happens and how to work around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=400)\nchunks = splitter.split_text(docs[0].page_content)\n\nall_events = []\nfor ch in chunks:\n    all_events.extend(events_chain.invoke({\"text\": ch}))\n\n# Deduplicate by (name, date) tuple\nunique = {(e[\"name\"], e.get(\"date\")): e for e in all_events}\nchr = list(unique.values())\nprint(f\"Merged events: {len(merged_events)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constraints And Performance\n\n**Token Limits**: gpt\\-4o\\-mini supports up to 128k tokens input. But here's the thing \\- for documents over 4k characters, you should chunk the text anyway to avoid context window issues.\n\n**Cost**: gpt\\-4o\\-mini runs about $0\\.15 per 1M input tokens and $0\\.60 per 1M output tokens. A typical extraction call uses somewhere between 500 and 2000 tokens.\n\n**Latency**: Expect 1\\-3 seconds per extraction call. This varies based on input size and API load.\n\nFor high\\-volume jobs, you'll want to control prompt size, reduce chunk overlap, and honestly, just stick with cheaper models like gpt\\-4o\\-mini for extraction. If you're trying to figure out which model best fits your pipeline's speed, cost, and reliability needs, check out our practical guide on how to choose an LLM for your application.\n\n## Conclusion\n\nYou've built a deterministic, validated extraction pipeline that converts raw text into structured JSON using OpenAI function calling, Pydantic, and LangChain. The system enforces schema compliance, eliminates hallucination, and \\- this is key \\- produces repeatable results.\n\n**Key Design Choices:**\n\n* Function calling forces the model to return only schema\\-compliant JSON\n* Pydantic validation catches invalid payloads at runtime\n* LangChain orchestration makes the pipeline composable, testable, and extensible\n* Temperature zero ensures deterministic output\n\n\n**Next Steps:**\n\n* Add retry logic with exponential backoff for production reliability\n* Extend the schema with new fields like location or confidence scores\n* Deploy the pipeline as a REST API using FastAPI\n* Parallelize extraction across multiple documents with asyncio and rate limiting\n* Add observability with structured logging and input/output hashing to track performance (and avoid logging PII \\- that's important)"
      ]
    }
  ],
  "metadata": {
    "title": "Structured Data Extraction with LLMs: How to Build a Pipeline",
    "description": "Build a reliable structured data extraction pipeline using LLMs, LangChain, and OpenAI functions: JSON schemas, deterministic outputs, zero hallucinations, for production.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}