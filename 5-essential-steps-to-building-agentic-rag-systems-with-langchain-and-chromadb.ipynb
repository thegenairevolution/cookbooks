{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** 5 Essential Steps to Building Agentic RAG Systems with LangChain and ChromaDB\n\n**Description:** Unlock the power of agentic RAG systems with LangChain and ChromaDB. Follow these steps to enhance AI adaptability and relevance in real-world applications.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nI'll be honest - when I first started working with RAG systems, I thought they were pretty impressive. But then I kept running into the same frustrating problem: they were essentially just fancy search engines. They'd retrieve documents, sure, but they couldn't really *think* about when to retrieve them, or figure out if they even needed to search at all. That's where agentic RAG comes in.\n\nAgentic retrieval-augmented generation systems are a whole different beast. Unlike traditional RAG that mechanically retrieves information every single time, these systems can actually make decisions. They determine when they need to look something up, how to break down complex questions, and can even use multiple tools to get you the answer you need. It's like the difference between a library catalog and having an actual research assistant.\n\nIn this tutorial, I'm going to show you exactly how to build one of these systems using <a href=\"https://python.langchain.com/\">LangChain</a> and <a href=\"https://www.trychroma.com/\">ChromaDB</a>. By the time we're done, you'll know how to:\n\n<ul>\n- Set up LangChain agents with ChromaDB for vector storage\n- Build in the logic for autonomous decision-making\n- Create multi-step reasoning workflows (this is where it gets really interesting)\n- Optimize your retrieval with caching and reranking\n- Test everything properly\n- Get your system ready for actual production use\n</ul>\nThis guide is for AI builders who want to go beyond basic RAG. If you're interested in diving deeper into customization for specific domains, you might find our guide on <a href=\"/article/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled\">customizing LLMs for domain-specific applications</a> helpful.\n\n## Setup & Installation\nLet's start with the basics - getting everything installed and configured. We need LangChain for orchestrating our agents, ChromaDB for vector storage, and OpenAI for embeddings and the language model itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages\n!pip install langchain chromadb openai langchain-community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n\n# Set up environment variables for API keys\n# Replace these with your actual API keys\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n\n# Import necessary libraries\nimport langchain\nimport chromadb\n\n# Verify installation by printing versions\nprint(\"LangChain version:\", langchain.__version__)\nprint(\"ChromaDB version:\", chromadb.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Knowledge Base\nHere's the thing about RAG systems - they're only as good as their knowledge base. So we need to be really careful about how we set this up. We'll load documents, chunk them properly, create embeddings, and index everything in ChromaDB.\n\n### Loading and Processing Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom chromadb import Client\nfrom chromadb.config import Settings\n\n# Load documents from a PDF file\n# Replace \"sample.pdf\" with your actual document path\nloader = PyPDFLoader(\"sample.pdf\")\ndocuments = loader.load()\n\nprint(f\"Loaded {len(documents)} pages from the document\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunking Strategy\nNow, chunking is more important than you might think. Too big and you lose precision. Too small and you lose context. I've found that 500 characters with some overlap works pretty well for most use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split text into manageable chunks\n# chunk_size: Maximum characters per chunk (affects retrieval granularity)\n# chunk_overlap: Characters shared between chunks (preserves context)\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n\nchunks = text_splitter.split_documents(documents)\nprint(f\"Created {len(chunks)} text chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Indexing in ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ChromaDB client\nchroma_client = Client(Settings(\n    anonymized_telemetry=False,\n    allow_reset=True\n))\n\n# Create a collection for storing document embeddings\ncollection_name = \"knowledge_base\"\ncollection = chroma_client.create_collection(\n    name=collection_name,\n    metadata={\"description\": \"Agentic RAG knowledge base\"}\n)\n\n# Initialize OpenAI embeddings\nembeddings = OpenAIEmbeddings()\n\n# Add documents to ChromaDB with embeddings\nfor i, chunk in enumerate(chunks):\n    embedding = embeddings.embed_query(chunk.page_content)\n    collection.add(\n        embeddings=[embedding],\n        documents=[chunk.page_content],\n        metadatas=[chunk.metadata],\n        ids=[f\"doc_{i}\"]\n    )\n\nprint(f\"Indexed {len(chunks)} chunks in ChromaDB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Basic Retrieval\nBefore we get fancy, let's make sure our basic retrieval actually works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test basic similarity search to verify setup\nquery = \"What is agentic RAG?\"\nquery_embedding = embeddings.embed_query(query)\n\nresults = collection.query(\n    query_embeddings=[query_embedding],\n    n_results=3\n)\n\nprint(\"Search results:\")\nfor i, doc in enumerate(results['documents'][0]):\n    print(f\"\\nResult {i+1}:\")\n    print(doc[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing the Agentic System\nAlright, now we get to the interesting part. This is where we build the actual \"agent\" capabilities. LangChain gives us a solid framework for this, and ChromaDB handles our vector storage efficiently. Actually, if you're curious about the technical details of fine-tuning these models, check out our breakdown of <a href=\"/article/mastering-fine-tuning-of-large-language-models-with-hugging-face\">fine-tuning large language models with Hugging Face Transformers</a>.\n\n### Creating the Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.tools import Tool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain.prompts import PromptTemplate\n\ndef retrieve_documents(query: str) -> str:\n    \"\"\"\n    Retrieve relevant documents from ChromaDB based on the query.\n    \n    Args:\n        query: The search query string\n        \n    Returns:\n        Concatenated text from top retrieved documents\n    \"\"\"\n    query_embedding = embeddings.embed_query(query)\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=3\n    )\n    \n    # Combine retrieved documents\n    retrieved_text = \"\\n\\n\".join(results['documents'][0])\n    return retrieved_text\n\n# Create a LangChain tool for retrieval\nretrieval_tool = Tool(\n    name=\"Knowledge_Base_Search\",\n    func=retrieve_documents,\n    description=\"Searches the knowledge base for relevant information. Use this when you need to find specific facts or context from the documents.\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the Agent with Decision-Making Logic\nHere's where things get really interesting. We're creating an agent that can actually decide whether it needs to search for information or not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the language model\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n\n# Define the agent prompt with reasoning capabilities\nagent_prompt = PromptTemplate.from_template(\"\"\"\nYou are an intelligent assistant with access to a knowledge base. \n\nAnswer the following question by deciding whether you need to retrieve information or can answer directly.\n\nAvailable tools:\n\n\nTool names: \n\nQuestion: {input}\n\nThought: Let me think about whether I need to search the knowledge base.\n{agent_scratchpad}\n\"\"\")\n\n# Create the ReAct agent\ntools = [retrieval_tool]\nagent = create_react_agent(llm, tools, agent_prompt)\n\n# Create an agent executor\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    max_iterations=3,\n    handle_parsing_errors=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Autonomous Decision-Making\nNow, this is the part that really makes it \"agentic\" - the system decides for itself whether to retrieve information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decide_and_respond(query: str) -> str:\n    \"\"\"\n    Autonomous decision-making function that determines whether to retrieve\n    information based on query complexity and context.\n    \n    Args:\n        query: The user's question\n        \n    Returns:\n        The agent's response\n    \"\"\"\n    # Keywords that indicate need for retrieval\n    retrieval_indicators = [\"what\", \"how\", \"explain\", \"describe\", \"details\", \"specific\"]\n    \n    # Simple heuristic: check if query contains retrieval indicators\n    needs_retrieval = any(indicator in query.lower() for indicator in retrieval_indicators)\n    \n    if needs_retrieval:\n        # Use agent with retrieval capabilities\n        response = agent_executor.invoke({\"input\": query})\n        return response[\"output\"]\n    else:\n        # Direct response without retrieval\n        response = llm.predict(query)\n        return response\n\n# Test the decision-making logic\ntest_queries = [\n    \"What is agentic RAG?\",\n    \"Hello, how are you?\",\n    \"Explain the key components of the system\"\n]\n\nfor query in test_queries:\n    print(f\"\\nQuery: {query}\")\n    print(f\"Response: {decide_and_respond(query)}\")\n    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Step Reasoning Implementation\nThis is honestly my favorite part. When you get a really complex question, the system breaks it down into smaller pieces, answers each one, then synthesizes everything together. It's like watching someone actually think through a problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n\ndef multi_step_reasoning(complex_query: str) -> dict:\n    \"\"\"\n    Perform multi-step reasoning for complex queries by breaking them down\n    into sub-questions and aggregating results.\n    \n    Args:\n        complex_query: A complex question requiring multiple reasoning steps\n        \n    Returns:\n        Dictionary containing reasoning steps and final answer\n    \"\"\"\n    # Step 1: Decompose the query into sub-questions\n    decomposition_prompt = PromptTemplate(\n        input_variables=[\"query\"],\n        template=\"Break down this complex question into 2-3 simpler sub-questions:\\n{query}\\n\\nSub-questions:\"\n    )\n    \n    decomposition_chain = LLMChain(llm=llm, prompt=decomposition_prompt)\n    sub_questions = decomposition_chain.run(complex_query)\n    \n    print(\"Sub-questions identified:\")\n    print(sub_questions)\n    \n    # Step 2: Answer each sub-question\n    sub_answers = []\n    for sub_q in sub_questions.split(\"\\n\"):\n        if sub_q.strip():\n            answer = decide_and_respond(sub_q.strip())\n            sub_answers.append({\n                \"question\": sub_q.strip(),\n                \"answer\": answer\n            })\n    \n    # Step 3: Synthesize final answer\n    synthesis_prompt = PromptTemplate(\n        input_variables=[\"original_query\", \"sub_answers\"],\n        template=\"\"\"\n        Original question: {original_query}\n        \n        Sub-question answers:\n        {sub_answers}\n        \n        Provide a comprehensive final answer to the original question:\n        \"\"\"\n    )\n    \n    synthesis_chain = LLMChain(llm=llm, prompt=synthesis_prompt)\n    final_answer = synthesis_chain.run(\n        original_query=complex_query,\n        sub_answers=\"\\n\\n\".join([f\"Q: {sa['question']}\\nA: {sa['answer']}\" for sa in sub_answers])\n    )\n    \n    return {\n        \"sub_questions\": sub_questions,\n        \"sub_answers\": sub_answers,\n        \"final_answer\": final_answer\n    }\n\n# Test multi-step reasoning\ncomplex_query = \"How does agentic RAG improve upon traditional RAG systems and what are the key implementation challenges?\"\nresult = multi_step_reasoning(complex_query)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL ANSWER:\")\nprint(result[\"final_answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization, Testing, and Production Readiness\nLet me tell you - getting this stuff production-ready is a lot more complicated than it seems at first. You need to think about optimization, proper testing, error handling... the works. If you want more on optimizing AI systems, our article on <a href=\"/article/mastering-domain-specific-llm-customization-techniques-and-tools-unveiled\">customizing LLMs for domain-specific applications</a> goes pretty deep into this.\n\n### Advanced Retrieval Optimization\nOne trick I've learned is that asking the same question in different ways often gets you better results. So let's implement that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import lru_cache\nfrom typing import List, Tuple\n\ndef multi_query_retrieval(query: str, num_variations: int = 3) -> List[str]:\n    \"\"\"\n    Generate multiple query variations to improve retrieval coverage.\n    \n    Args:\n        query: Original query string\n        num_variations: Number of query variations to generate\n        \n    Returns:\n        List of unique retrieved documents\n    \"\"\"\n    # Generate query variations\n    variation_prompt = PromptTemplate(\n        input_variables=[\"query\", \"num\"],\n        template=\"Generate {num} different ways to ask this question:\\n{query}\\n\\nVariations:\"\n    )\n    \n    chain = LLMChain(llm=llm, prompt=variation_prompt)\n    variations = chain.run(query=query, num=num_variations)\n    \n    # Retrieve documents for each variation\n    all_results = []\n    for variation in variations.split(\"\\n\"):\n        if variation.strip():\n            docs = retrieve_documents(variation.strip())\n            all_results.append(docs)\n    \n    # Deduplicate results\n    unique_results = list(set(all_results))\n    return unique_results\n\ndef rerank_results(query: str, documents: List[str]) -> List[Tuple[str, float]]:\n    \"\"\"\n    Rerank retrieved documents based on relevance to the query.\n    \n    Args:\n        query: The search query\n        documents: List of retrieved documents\n        \n    Returns:\n        List of (document, score) tuples sorted by relevance\n    \"\"\"\n    scored_docs = []\n    \n    for doc in documents:\n        # Simple relevance scoring based on keyword overlap\n        # In production, use a cross-encoder model for better accuracy\n        query_terms = set(query.lower().split())\n        doc_terms = set(doc.lower().split())\n        overlap = len(query_terms.intersection(doc_terms))\n        score = overlap / len(query_terms) if query_terms else 0\n        \n        scored_docs.append((doc, score))\n    \n    # Sort by score descending\n    scored_docs.sort(key=lambda x: x[1], reverse=True)\n    return scored_docs\n\n# Implement optimized retrieval with caching\n@lru_cache(maxsize=100)\ndef cached_optimized_retrieval(query: str) -> str:\n    \"\"\"\n    Cached retrieval with multi-query and reranking optimization.\n    \n    Args:\n        query: The search query\n        \n    Returns:\n        Best retrieved document\n    \"\"\"\n    # Multi-query retrieval\n    documents = multi_query_retrieval(query)\n    \n    # Rerank results\n    ranked_docs = rerank_results(query, documents)\n    \n    # Return top result\n    return ranked_docs[0][0] if ranked_docs else \"\"\n\n# Test optimized retrieval\ntest_query = \"What are the benefits of agentic systems?\"\nresult = cached_optimized_retrieval(test_query)\nprint(\"Optimized retrieval result:\")\nprint(result[:300] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Evaluation\nYou can't improve what you don't measure, right? So here's how we track performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\nfrom typing import Dict\n\ndef evaluate_system_performance(test_queries: List[str]) -> Dict[str, float]:\n    \"\"\"\n    Evaluate system performance across multiple metrics.\n    \n    Args:\n        test_queries: List of queries to test\n        \n    Returns:\n        Dictionary of performance metrics\n    \"\"\"\n    latencies = []\n    cache_hits = 0\n    \n    for query in test_queries:\n        # Measure latency\n        start_time = time.time()\n        \n        # Check cache\n        cache_info_before = cached_optimized_retrieval.cache_info()\n        result = cached_optimized_retrieval(query)\n        cache_info_after = cached_optimized_retrieval.cache_info()\n        \n        end_time = time.time()\n        latency = end_time - start_time\n        latencies.append(latency)\n        \n        # Track cache hits\n        if cache_info_after.hits > cache_info_before.hits:\n            cache_hits += 1\n    \n    # Calculate metrics\n    avg_latency = sum(latencies) / len(latencies)\n    cache_hit_rate = cache_hits / len(test_queries)\n    \n    metrics = {\n        \"average_latency_seconds\": round(avg_latency, 3),\n        \"cache_hit_rate\": round(cache_hit_rate, 2),\n        \"total_queries\": len(test_queries)\n    }\n    \n    return metrics\n\n# Run evaluation\ntest_queries = [\n    \"What is agentic RAG?\",\n    \"How does retrieval work?\",\n    \"What is agentic RAG?\",  # Duplicate to test caching\n    \"Explain multi-step reasoning\"\n]\n\nmetrics = evaluate_system_performance(test_queries)\nprint(\"\\nPerformance Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"  {metric}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Handling and Resilience\nHere's something I learned the hard way - things will fail. APIs go down, rate limits hit, weird edge cases pop up. You need to be ready:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\nfrom typing import Optional\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef robust_query_handler(query: str, max_retries: int = 3) -> Optional[str]:\n    \"\"\"\n    Handle queries with error handling, retries, and fallback responses.\n    \n    Args:\n        query: The user query\n        max_retries: Maximum number of retry attempts\n        \n    Returns:\n        Response string or None if all attempts fail\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            # Attempt to process query\n            response = decide_and_respond(query)\n            logger.info(f\"Query processed successfully on attempt {attempt + 1}\")\n            return response\n            \n        except Exception as e:\n            logger.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n            \n            if attempt < max_retries - 1:\n                # Wait before retry (exponential backoff)\n                wait_time = 2 ** attempt\n                time.sleep(wait_time)\n            else:\n                # Final fallback\n                logger.error(\"All retry attempts exhausted\")\n                return \"I apologize, but I'm having trouble processing your request. Please try rephrasing your question or contact support.\"\n    \n    return None\n\n# Test error handling\ntest_query = \"What is agentic RAG?\"\nresponse = robust_query_handler(test_query)\nprint(f\"\\nRobust response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment with FastAPI\nWhen you're ready to deploy, FastAPI is a great choice. Here's the basic structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: This code demonstrates the deployment structure\n# In Colab, you would need to run this in a separate environment\n\n\"\"\"\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\n\napp = FastAPI(title=\"Agentic RAG API\")\n\nclass Query(BaseModel):\n    question: str\n    use_multi_step: bool = False\n\nclass Response(BaseModel):\n    answer: str\n    metadata: dict\n\n@app.post(\"/query\", response_model=Response)\nasync def process_query(query: Query):\n    try:\n        if query.use_multi_step:\n            result = multi_step_reasoning(query.question)\n            return Response(\n                answer=result[\"final_answer\"],\n                metadata={\"sub_questions\": result[\"sub_questions\"]}\n            )\n        else:\n            answer = robust_query_handler(query.question)\n            return Response(\n                answer=answer,\n                metadata={\"method\": \"single_step\"}\n            )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n# To run: uvicorn main:app --host 0.0.0.0 --port 8000\n\"\"\"\n\nprint(\"FastAPI deployment code structure defined\")\nprint(\"Deploy using: uvicorn main:app --host 0.0.0.0 --port 8000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitoring and Observability\nActually, wait - before you deploy anything, you need proper monitoring. Trust me on this one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\nfrom datetime import datetime\n\nclass SystemMonitor:\n    \"\"\"\n    Monitor agent decisions, retrieval quality, and system health.\n    \"\"\"\n    \n    def __init__(self):\n        self.query_log = []\n        self.retrieval_stats = defaultdict(int)\n        self.error_log = []\n    \n    def log_query(self, query: str, response: str, latency: float, used_retrieval: bool):\n        \"\"\"Log query details for analysis.\"\"\"\n        self.query_log.append({\n            \"timestamp\": datetime.now().isoformat(),\n            \"query\": query,\n            \"response_length\": len(response),\n            \"latency\": latency,\n            \"used_retrieval\": used_retrieval\n        })\n        \n        if used_retrieval:\n            self.retrieval_stats[\"retrieval_used\"] += 1\n        else:\n            self.retrieval_stats[\"direct_response\"] += 1\n    \n    def log_error(self, error: str, context: dict):\n        \"\"\"Log errors for debugging.\"\"\"\n        self.error_log.append({\n            \"timestamp\": datetime.now().isoformat(),\n            \"error\": error,\n            \"context\": context\n        })\n    \n    def get_statistics(self) -> dict:\n        \"\"\"Get system statistics.\"\"\"\n        total_queries = len(self.query_log)\n        avg_latency = sum(q[\"latency\"] for q in self.query_log) / total_queries if total_queries > 0 else 0\n        \n        return {\n            \"total_queries\": total_queries,\n            \"average_latency\": round(avg_latency, 3),\n            \"retrieval_usage\": dict(self.retrieval_stats),\n            \"error_count\": len(self.error_log)\n        }\n\n# Initialize monitor\nmonitor = SystemMonitor()\n\n# Example usage\nstart = time.time()\nresponse = decide_and_respond(\"What is agentic RAG?\")\nlatency = time.time() - start\n\nmonitor.log_query(\n    query=\"What is agentic RAG?\",\n    response=response,\n    latency=latency,\n    used_retrieval=True\n)\n\nprint(\"\\nSystem Statistics:\")\nprint(monitor.get_statistics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing & Validation\nLet's make sure everything actually works end-to-end. I like to test with different types of queries to really stress the system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_system_test():\n    \"\"\"\n    Run comprehensive tests across different query types and scenarios.\n    \"\"\"\n    test_cases = [\n        {\n            \"query\": \"What is agentic RAG?\",\n            \"expected_behavior\": \"Should retrieve from knowledge base\",\n            \"use_multi_step\": False\n        },\n        {\n            \"query\": \"How does agentic RAG differ from traditional RAG and what are implementation challenges?\",\n            \"expected_behavior\": \"Should use multi-step reasoning\",\n            \"use_multi_step\": True\n        },\n        {\n            \"query\": \"Hello\",\n            \"expected_behavior\": \"Should respond directly without retrieval\",\n            \"use_multi_step\": False\n        }\n    ]\n    \n    results = []\n    \n    for i, test_case in enumerate(test_cases):\n        print(f\"\\n{'='*80}\")\n        print(f\"Test Case {i+1}: {test_case['query']}\")\n        print(f\"Expected: {test_case['expected_behavior']}\")\n        print(f\"{'='*80}\")\n        \n        start_time = time.time()\n        \n        try:\n            if test_case[\"use_multi_step\"]:\n                result = multi_step_reasoning(test_case[\"query\"])\n                response = result[\"final_answer\"]\n            else:\n                response = robust_query_handler(test_case[\"query\"])\n            \n            latency = time.time() - start_time\n            \n            results.append({\n                \"test_case\": i+1,\n                \"query\": test_case[\"query\"],\n                \"response\": response[:200] + \"...\" if len(response) > 200 else response,\n                \"latency\": round(latency, 3),\n                \"status\": \"PASSED\"\n            })\n            \n            print(f\"\\nResponse: {response[:200]}...\")\n            print(f\"Latency: {latency:.3f}s\")\n            print(\"Status: PASSED âœ“\")\n            \n        except Exception as e:\n            results.append({\n                \"test_case\": i+1,\n                \"query\": test_case[\"query\"],\n                \"error\": str(e),\n                \"status\": \"FAILED\"\n            })\n            print(f\"\\nError: {str(e)}\")\n            print(\"Status: FAILED âœ—\")\n    \n    return results\n\n# Run comprehensive tests\ntest_results = comprehensive_system_test()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TEST SUMMARY\")\nprint(\"=\"*80)\npassed = sum(1 for r in test_results if r[\"status\"] == \"PASSED\")\nprint(f\"Passed: {passed}/{len(test_results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nSo there you have it - a complete agentic RAG system built from scratch. The more I think about it, the key difference between this and traditional RAG really comes down to autonomy. Your system can now:\n\n<ul>\n- Decide for itself when to retrieve information (no more unnecessary searches)\n- Break down complex questions into manageable pieces\n- Use multiple strategies to find the best information\n- Handle errors gracefully and keep running\n- Monitor its own performance\n</ul>\nAnd honestly, the performance improvements from caching and multi-query retrieval alone make this worth implementing.\n\n### Next Steps\nIf you want to take this further, here are some ideas I've been exploring:\n\n<ul>\n<li>**Better Reranking**: Try cross-encoder models like `sentence-transformers/ms-marco-MiniLM-L-12-v2`. They're slower but way more accurate.\n\n</li>\n<li>**Memory Systems**: Add conversation memory so the agent remembers what you talked about. LangChain has some great modules for this.\n\n</li>\n<li>**More Tools**: Why stop at retrieval? Add web search, calculators, maybe even database queries. The agent can figure out which to use.\n\n</li>\n<li>**Fine-tuning**: If you have domain-specific data, fine-tune your embedding model. The improvement in retrieval accuracy can be dramatic.\n\n</li>\n<li>**CI/CD**: Set up proper testing pipelines. GitHub Actions works great for this.\n\n</li>\n<li>**Better Monitoring**: Look into LangSmith or Weights & Biases for deeper insights into what your agent is actually doing.\n\n</li>\n</ul>\nOne last thing - when you deploy this to production, don't forget about authentication, rate limiting, and cost monitoring. OpenAI API calls add up quickly, trust me. And if you're dealing with serious scale, consider managed vector databases like Pinecone or Weaviate instead of ChromaDB.\n\nThe beauty of this system is that it's modular. Start simple, test everything, then gradually add complexity. Before you know it, you'll have an AI assistant that actually understands when and how to help, not just blindly retrieve documents every time."
      ]
    }
  ],
  "metadata": {
    "title": "5 Essential Steps to Building Agentic RAG Systems with LangChain and ChromaDB",
    "description": "Unlock the power of agentic RAG systems with LangChain and ChromaDB. Follow these steps to enhance AI adaptability and relevance in real-world applications.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}