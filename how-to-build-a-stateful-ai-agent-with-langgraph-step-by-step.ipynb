{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Build a Stateful AI Agent with LangGraph Step-by-Step\n\n**Description:** Build reliable, stateful AI agents with LangGraph using step-by-step patterns, visual debugging, and persistenceâ€”ready for tool use and production today.\n\n**ðŸ“– Read the full article:** [How to Build a Stateful AI Agent with LangGraph Step-by-Step](https://blog.thegenairevolution.com/article/how-to-build-a-stateful-ai-agent-with-langgraph-step-by-step)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When building AI agents that need to make decisions, call tools, and maintain context across multiple turns, orchestration becomes critical. LangGraph provides a structured way to define agent workflows as state machines, giving you fine\\-grained control over how your agent reasons, acts, and responds.\n\nIn this tutorial, you'll build a travel assistant agent that helps users plan trips by calling external tools to fetch weather data and search for flights. You'll learn how to define a stateful graph, integrate tool\\-calling logic, and trace execution step\\-by\\-step. By the end, you'll have a working agent that can handle multi\\-turn conversations and coordinate multiple tools to deliver useful results.\n\n## Why This Approach Works\n\nLangGraph treats agent workflows as explicit graphs where each node represents a step (like calling the LLM or executing a tool) and edges define transitions. This makes complex agent behavior easier to reason about, debug, and extend compared to implicit loops or callback\\-based systems.\n\nKey benefits:\n\n* **Explicit state management** â€“ You define exactly what data flows between steps, making it easier to track context and debug issues.\n* **Composable logic** â€“ Each node is a function. You can test, swap, or extend individual components without rewriting the entire agent.\n* **Built\\-in tracing** â€“ LangGraph logs every state transition, so you can inspect what the agent did at each step and why.\n\nThis approach is especially useful when your agent needs to call multiple tools, handle conditional logic, or maintain conversation history across turns.\n\n## High\\-Level Overview\n\nHere's how the system works:\n\n1. **User input** â€“ The user sends a message (e.g., \"Find me flights to Tokyo and check the weather\").\n2. **LLM reasoning** â€“ The agent calls the LLM, which decides whether to respond directly or invoke tools.\n3. **Tool execution** â€“ If tools are needed, the agent executes them (e.g., search\\_flights, get\\_weather) and collects results.\n4. **LLM synthesis** â€“ The agent sends tool results back to the LLM, which generates a final response.\n5. **Output** â€“ The user receives a natural language answer informed by real data.\n\nThe graph has three main nodes:\n\n* **Agent node** â€“ Calls the LLM to decide next actions.\n* **Tool node** â€“ Executes requested tools and returns results.\n* **Conditional edge** â€“ Routes to tools if needed, or ends if the agent is done.\n\n## Setup \\& Installation\n\nThis code runs in Google Colab or any Python 3\\.10\\+ environment. Install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU langgraph langchain-openai langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace \"your\\-openai\\-api\\-key\" with your actual key. For production, use environment variables or secret management tools instead of hardcoding keys.\n\n## Step 1: Define Tools\n\nTools are Python functions decorated with @tool. The LLM can call these functions when it needs external data.\n\nThis example defines two tools: one for searching flights and one for fetching weather. Both return mock data for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n\n@tool\ndef search_flights(origin: str, destination: str, date: str) -> str:\n    \"\"\"Search for available flights between two cities on a given date.\"\"\"\n    return f\"Found 3 flights from {origin} to {destination} on {date}: Flight A ($450), Flight B ($520), Flight C ($610).\"\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get current weather information for a city.\"\"\"\n    return f\"Weather in {city}: 22Â°C, partly cloudy, light breeze.\"\n\ntools = [search_flights, get_weather]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a real application, replace the return statements with API calls to services like Amadeus (flights) or OpenWeatherMap (weather).\n\n## Step 2: Bind Tools to the LLM\n\nThe LLM needs to know which tools are available and how to call them. Use .bind\\_tools() to attach tool schemas to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\nmodel_with_tools = model.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now when you call model\\_with\\_tools, the LLM can decide to invoke search\\_flights or get\\_weather based on the user's request.\n\n## Step 3: Define the Agent State\n\nState is a dictionary that flows through the graph. It holds the conversation history and any other data you need to track."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The add\\_messages annotation tells LangGraph to append new messages to the list rather than replacing it. This preserves conversation history across turns.\n\n## Step 4: Build the Agent Node\n\nThe agent node calls the LLM with the current message history. The LLM returns either a text response or a request to call tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_agent(state: State):\n    \"\"\"Invoke the LLM with the current conversation state.\"\"\"\n    response = model_with_tools.invoke(state[\"messages\"])\n    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function takes the state, passes state\\[\"messages\"] to the model, and returns the model's response wrapped in a dictionary so LangGraph can merge it back into the state.\n\n## Step 5: Build the Tool Node\n\nLangGraph provides a ToolNode that automatically executes any tools the LLM requested and formats the results as messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n\ntool_node = ToolNode(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the agent node returns a message with tool\\_calls, the graph routes to this node, which runs the tools and appends their outputs to the message list.\n\n## Step 6: Define Routing Logic\n\nAfter the agent node runs, the graph needs to decide: should it call tools, or is the agent done?\n\nThis function checks if the last message contains tool calls. If yes, route to the tool node. If no, end the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n\ndef should_continue(state: State):\n    \"\"\"Determine whether to call tools or finish.\"\"\"\n    last_message = state[\"messages\"][-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Assemble the Graph\n\nNow combine the nodes and edges into a state graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START\n\nworkflow = StateGraph(State)\n\nworkflow.add_node(\"agent\", call_agent)\nworkflow.add_node(\"tools\", tool_node)\n\nworkflow.add_edge(START, \"agent\")\nworkflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\nworkflow.add_edge(\"tools\", \"agent\")\n\ngraph = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's what each line does:\n\n* **add\\_node(\"agent\", call\\_agent)** â€“ Registers the agent node.\n* **add\\_node(\"tools\", tool\\_node)** â€“ Registers the tool execution node.\n* **add\\_edge(START, \"agent\")** â€“ The graph always starts at the agent node.\n* **add\\_conditional\\_edges(\"agent\", should\\_continue, ...)** â€“ After the agent runs, route to tools or end based on should\\_continue.\n* **add\\_edge(\"tools\", \"agent\")** â€“ After tools run, return to the agent so it can synthesize results.\n\n## Step 8: Run the Agent\n\nInvoke the graph with a user message. The agent will decide which tools to call and return a final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n\nuser_input = \"Find me flights from San Francisco to Tokyo on March 15th and tell me the weather in Tokyo.\"\nresult = graph.invoke({\"messages\": [HumanMessage(content=user_input)]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result dictionary contains the full message history, including the user's input, tool calls, tool results, and the agent's final response.\n\n## Step 9: Display Results and Trace\n\nTo see the final answer and understand what happened at each step, print the last AI message and trace all messages.\n\nThis helper function extracts the final AI response and prints a numbered trace showing each message type, content, and metadata like tool calls or tool names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, ToolMessage\n\ndef print_message_trace(result):\n    final = [m for m in result[\"messages\"] if isinstance(m, AIMessage)][-1]\n    print(final.content)\n\n    print(\"\\nFull Trace:\")\n    for i, m in enumerate(result[\"messages\"], 1):\n        role = type(m).__name__\n        meta = \"\"\n        if isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None):\n            meta = f\" tool_calls={m.tool_calls}\"\n        if isinstance(m, ToolMessage):\n            meta = f\" tool_name={m.name}\"\n        print(f\"{i:02d}. {role}: {m.content}{meta}\")\n\nprint_message_trace(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This trace shows the agent called both tools in parallel, received results, and synthesized a final answer.\n\n## Run and Validate\n\nTest the agent with different inputs to confirm it routes correctly:\n\n**Single tool call:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = graph.invoke({\"messages\": [HumanMessage(content=\"What's the weather in Paris?\")]})\nprint_message_trace(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**No tool call (direct answer):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = graph.invoke({\"messages\": [HumanMessage(content=\"What is LangGraph?\")]})\nprint_message_trace(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Multi\\-turn conversation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = graph.invoke({\"messages\": [HumanMessage(content=\"Find flights to Berlin on April 10th.\")]})\nresult = graph.invoke({\"messages\": result[\"messages\"] + [HumanMessage(content=\"What about the weather there?\")]})\nprint_message_trace(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the multi\\-turn example, the agent maintains context from the first turn and knows \"there\" refers to Berlin.\n\n## Conclusion\n\nYou've built a stateful LangGraph agent that orchestrates tool calls and maintains conversation context. The key takeaways:\n\n* **State graphs make agent logic explicit** â€“ You control exactly when the LLM is called, when tools run, and how results flow back.\n* **Tool binding is straightforward** â€“ Decorate functions with @tool and bind them to the model. The LLM handles the rest.\n* **Tracing is built\\-in** â€“ Every message and tool call is logged, making debugging and optimization easier.\n\nFor readers working on data extraction challenges, our guide on building a structured data extraction pipeline with LLMs offers complementary strategies for handling unstructured inputs.\n\nIf you encounter unexpected model behavior, subtle bugs often stem from tokenization issuesâ€”see our article on tokenization pitfalls and invisible characters for actionable solutions.\n\nWhen scaling to long\\-context applications, be aware of memory limitations. Our analysis of context rot and memory management in LLMs explains why models sometimes lose track of earlier information and how to mitigate it.\n\n## Next Steps\n\n* **Add real APIs** â€“ Replace mock data with live calls to flight search APIs (e.g., Amadeus) and weather services (e.g., OpenWeatherMap).\n* **Persist state** â€“ Use LangGraph's checkpointing to save conversation history to a database, enabling multi\\-session continuity.\n* **Add error handling** â€“ Wrap tool calls in try\\-except blocks and return user\\-friendly error messages when APIs fail.\n* **Deploy as an API** â€“ Serve the graph via FastAPI or Flask so users can interact with it through a web interface or chat app."
      ]
    }
  ],
  "metadata": {
    "title": "How to Build a Stateful AI Agent with LangGraph Step-by-Step",
    "description": "Build reliable, stateful AI agents with LangGraph using step-by-step patterns, visual debugging, and persistenceâ€”ready for tool use and production today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}