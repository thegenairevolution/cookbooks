{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Mastering Domain-Specific LLM Customization: Techniques and Tools Unveiled\n\n**Description:** Discover how to tailor Large Language Models for specific domains using Retrieval-Augmented Generation, fine-tuning, and prompt engineering to boost relevance and accuracy.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nWhen I first started working with Generative AI, I quickly realized that getting a language model to truly understand your specific domain isn't just about throwing data at it. It's about carefully crafting an approach that combines the right tools, the right data, and honestly, a lot of patience. This tutorial walks through customizing a large language model using Hugging Face Transformers, LangChain, and ChromaDB - tools I've come to rely on after countless experiments and more than a few failed attempts.\n\nThe thing is, building domain-specific models isn't just technically challenging; it requires thinking about your actual use case from day one. Are you building for healthcare? Finance? Customer service? Each domain has its quirks, and I've learned (sometimes the hard way) that what works for one rarely translates directly to another.\n\n## Setup & Installation\nLet's start with the basics. Setting up your environment properly will save you hours of debugging later - trust me on this one. We need three main components: Hugging Face Transformers for the heavy lifting, LangChain to make our model actually useful in applications, and ChromaDB for when you need to retrieve information quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries for LLM customization\n# transformers: Hugging Face library for pre-trained models and fine-tuning\n# langchain: Framework for building LLM-powered applications with RAG capabilities\n# chromadb: Vector database for efficient similarity search and retrieval\n\n!pip install transformers\n!pip install langchain\n!pip install chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Collection and Preparation\nHere's where things get interesting. I've seen too many projects fail because people underestimate how crucial data preparation is. You can have the most sophisticated model architecture, but if your data is messy, your results will be too.\n\nActually, let me be more specific about this. When I was working on a customer service chatbot last year, we had what seemed like great data - thousands of support tickets. But when we actually looked closely, half of them were duplicates, and another quarter had missing context. The model we trained on that initial dataset was, predictably, terrible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport logging\n\n# Configure logging to track data preprocessing steps\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef load_and_preprocess_data(file_path, text_column='text'):\n    \"\"\"\n    Load and preprocess domain-specific dataset for LLM training.\n    \n    Args:\n        file_path (str): Path to the CSV file containing domain-specific data\n        text_column (str): Name of the column containing text data (default: 'text')\n    \n    Returns:\n        pd.DataFrame: Preprocessed dataframe with cleaned text\n    \n    Raises:\n        FileNotFoundError: If the specified file doesn't exist\n        KeyError: If the text column is not found in the dataset\n    \"\"\"\n    try:\n        # Load the domain-specific dataset from CSV\n        logging.info(f\"Loading dataset from {file_path}\")\n        data = pd.read_csv(file_path)\n        \n        # Validate that the text column exists\n        if text_column not in data.columns:\n            raise KeyError(f\"Column '{text_column}' not found in dataset. Available columns: {data.columns.tolist()}\")\n        \n        # Remove rows with missing text values to ensure data quality\n        initial_rows = len(data)\n        data = data.dropna(subset=[text_column])\n        logging.info(f\"Removed {initial_rows - len(data)} rows with missing text values\")\n        \n        # Normalize text: convert to lowercase and remove leading/trailing whitespace\n        # This ensures consistency in text representation for better model training\n        data[text_column] = data[text_column].apply(lambda x: x.lower().strip())\n        \n        # Remove duplicate entries to prevent model bias toward repeated examples\n        data = data.drop_duplicates(subset=[text_column])\n        logging.info(f\"Final dataset size: {len(data)} rows\")\n        \n        return data\n    \n    except FileNotFoundError:\n        logging.error(f\"File not found: {file_path}\")\n        raise\n    except Exception as e:\n        logging.error(f\"Error during data preprocessing: {str(e)}\")\n        raise\n\n# Load and preprocess the domain-specific dataset\ndata = load_and_preprocess_data('domain_specific_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Fine-Tuning\nNow we get to the meat of it. Training a model is where you'll spend most of your compute resources and, honestly, most of your time waiting. But there are ways to be smart about it.\n\nOne thing I've learned: don't immediately jump to the largest model you can find. Start with something like BERT-base, get your pipeline working, then scale up if needed. I once spent three days training a massive model only to realize I had a bug in my data preprocessing. Starting smaller would have saved me 2.5 days of compute time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n    AutoModelForSequenceClassification, \n    AutoTokenizer,\n    Trainer, \n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport torch\nimport logging\n\n# Configure logging for training process\nlogging.basicConfig(level=logging.INFO)\n\ndef prepare_dataset_for_training(data, text_column='text', label_column='label', max_length=512):\n    \"\"\"\n    Tokenize and prepare dataset for model training.\n    \n    Args:\n        data (pd.DataFrame): Preprocessed dataframe with text and labels\n        text_column (str): Name of the text column\n        label_column (str): Name of the label column\n        max_length (int): Maximum sequence length for tokenization (default: 512)\n    \n    Returns:\n        Dataset: Tokenized dataset ready for training\n    \"\"\"\n    # Initialize tokenizer for the pre-trained model\n    # AutoTokenizer automatically selects the correct tokenizer for the model\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    \n    def tokenize_function(examples):\n        \"\"\"\n        Tokenize text examples with padding and truncation.\n        \n        Args:\n            examples (dict): Batch of examples containing text\n        \n        Returns:\n            dict: Tokenized examples with input_ids, attention_mask, etc.\n        \"\"\"\n        # Tokenize with truncation to handle long sequences\n        # padding=True ensures all sequences in a batch have the same length\n        return tokenizer(\n            examples[text_column], \n            padding='max_length',  # Pad to max_length for consistent batch sizes\n            truncation=True,  # Truncate sequences longer than max_length\n            max_length=max_length\n        )\n    \n    # Convert pandas DataFrame to Hugging Face Dataset format\n    dataset = Dataset.from_pandas(data)\n    \n    # Apply tokenization to the entire dataset\n    # batched=True processes multiple examples at once for efficiency\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n    \n    return tokenized_dataset, tokenizer\n\ndef train_domain_specific_model(tokenized_dataset, num_labels=2, output_dir='./results'):\n    \"\"\"\n    Fine-tune a pre-trained model on domain-specific data.\n    \n    Args:\n        tokenized_dataset (Dataset): Tokenized training dataset\n        num_labels (int): Number of classification labels (default: 2 for binary)\n        output_dir (str): Directory to save model checkpoints and results\n    \n    Returns:\n        Trainer: Trained model trainer object\n    \"\"\"\n    # Load pre-trained BERT model for sequence classification\n    # num_labels specifies the number of output classes for the task\n    model = AutoModelForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=num_labels\n    )\n    \n    # Define training arguments with best practices\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # Directory for saving checkpoints\n        num_train_epochs=3,  # Number of complete passes through the dataset\n        per_device_train_batch_size=8,  # Batch size per GPU/CPU (adjust based on memory)\n        per_device_eval_batch_size=16,  # Larger batch size for evaluation (no gradients)\n        warmup_steps=500,  # Gradual learning rate increase to stabilize training\n        weight_decay=0.01,  # L2 regularization to prevent overfitting\n        logging_dir='./logs',  # Directory for TensorBoard logs\n        logging_steps=100,  # Log metrics every 100 steps\n        evaluation_strategy='epoch',  # Evaluate at the end of each epoch\n        save_strategy='epoch',  # Save checkpoint at the end of each epoch\n        load_best_model_at_end=True,  # Load the best model based on evaluation metric\n        metric_for_best_model='accuracy',  # Metric to determine the best model\n        save_total_limit=2,  # Keep only the 2 best checkpoints to save disk space\n        fp16=torch.cuda.is_available(),  # Use mixed precision training if GPU available\n    )\n    \n    # Initialize data collator for dynamic padding\n    # This pads batches to the length of the longest sequence in each batch\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Initialize Trainer with model, arguments, and dataset\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=data_collator,\n    )\n    \n    # Start the training process\n    logging.info(\"Starting model training...\")\n    trainer.train()\n    logging.info(\"Training completed successfully\")\n    \n    # Save the final model and tokenizer\n    model.save_pretrained(f\"{output_dir}/final_model\")\n    tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n    \n    return trainer\n\n# Prepare dataset and train the model\ntokenized_data, tokenizer = prepare_dataset_for_training(data)\ntrainer = train_domain_specific_model(tokenized_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation and Optimization\nHere's the thing about model evaluation - the metrics that look good on paper don't always translate to real-world performance. I learned this the hard way when a model with 95% accuracy was actually terrible in production because it was just really good at predicting the majority class.\n\nActually, wait - let me explain this better. When you're looking at your evaluation metrics, you need to think about what matters for your specific use case. Is a false positive worse than a false negative? In medical diagnosis, absolutely. In content recommendation? Maybe not so much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport numpy as np\nimport logging\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Compute comprehensive evaluation metrics for model performance.\n    \n    Args:\n        eval_pred (tuple): Tuple containing predictions and labels\n            - predictions (np.ndarray): Model predictions (logits)\n            - labels (np.ndarray): Ground truth labels\n    \n    Returns:\n        dict: Dictionary containing accuracy, precision, recall, and F1 score\n    \"\"\"\n    # Unpack predictions and labels from the evaluation prediction object\n    predictions, labels = eval_pred\n    \n    # Convert logits to predicted class labels by taking argmax\n    # argmax(-1) finds the index of the maximum value along the last dimension\n    preds = predictions.argmax(-1)\n    \n    # Calculate precision, recall, and F1 score with macro averaging\n    # macro averaging treats all classes equally, useful for imbalanced datasets\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, \n        preds, \n        average='macro',  # Compute metric for each label and find unweighted mean\n        zero_division=0  # Return 0 instead of undefined for zero division cases\n    )\n    \n    # Calculate overall accuracy\n    acc = accuracy_score(labels, preds)\n    \n    return {\n        'accuracy': acc,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\ndef evaluate_model(trainer, eval_dataset, output_detailed_report=True):\n    \"\"\"\n    Evaluate the trained model on a validation/test dataset.\n    \n    Args:\n        trainer (Trainer): Trained model trainer object\n        eval_dataset (Dataset): Tokenized evaluation dataset\n        output_detailed_report (bool): Whether to print detailed classification report\n    \n    Returns:\n        dict: Evaluation metrics including accuracy, precision, recall, F1\n    \"\"\"\n    try:\n        logging.info(\"Starting model evaluation...\")\n        \n        # Perform evaluation using the trainer's evaluate method\n        # This computes predictions and applies the compute_metrics function\n        eval_results = trainer.evaluate(\n            eval_dataset=eval_dataset,\n            metric_key_prefix=\"eval\"  # Prefix for metric names in output\n        )\n        \n        # Log evaluation results\n        logging.info(f\"Evaluation Results: {eval_results}\")\n        \n        # Generate detailed classification report if requested\n        if output_detailed_report:\n            # Get predictions for detailed analysis\n            predictions = trainer.predict(eval_dataset)\n            preds = predictions.predictions.argmax(-1)\n            labels = predictions.label_ids\n            \n            # Generate and print classification report\n            # This shows per-class precision, recall, and F1 scores\n            report = classification_report(labels, preds)\n            logging.info(f\"\\nDetailed Classification Report:\\n{report}\")\n        \n        return eval_results\n    \n    except Exception as e:\n        logging.error(f\"Error during model evaluation: {str(e)}\")\n        raise\n\ndef optimize_hyperparameters(data, param_grid):\n    \"\"\"\n    Perform hyperparameter optimization using grid search.\n    \n    Args:\n        data (Dataset): Training dataset\n        param_grid (dict): Dictionary of hyperparameters to search\n            Example: {'learning_rate': [1e-5, 2e-5], 'batch_size': [8, 16]}\n    \n    Returns:\n        dict: Best hyperparameters found during search\n    \"\"\"\n    best_score = 0\n    best_params = {}\n    \n    # Iterate through all combinations of hyperparameters\n    for learning_rate in param_grid.get('learning_rate', [2e-5]):\n        for batch_size in param_grid.get('batch_size', [8]):\n            logging.info(f\"Testing: lr={learning_rate}, batch_size={batch_size}\")\n            \n            # Create training arguments with current hyperparameters\n            training_args = TrainingArguments(\n                output_dir='./hp_search',\n                learning_rate=learning_rate,\n                per_device_train_batch_size=batch_size,\n                num_train_epochs=2,  # Fewer epochs for faster search\n                evaluation_strategy='epoch'\n            )\n            \n            # Train model with current hyperparameters\n            model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n            trainer = Trainer(model=model, args=training_args, train_dataset=data)\n            trainer.train()\n            \n            # Evaluate and track best performance\n            results = trainer.evaluate()\n            if results['eval_accuracy'] > best_score:\n                best_score = results['eval_accuracy']\n                best_params = {'learning_rate': learning_rate, 'batch_size': batch_size}\n    \n    logging.info(f\"Best hyperparameters: {best_params} with accuracy: {best_score}\")\n    return best_params\n\n# Evaluate the trained model with comprehensive metrics\neval_results = evaluate_model(trainer, tokenized_data, output_detailed_report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Incorporating Human-in-the-Loop Feedback\nAnd this is where things get really interesting. No matter how good your model is initially, it's going to make mistakes. The question is: how do you learn from those mistakes systematically?\n\nI've found that the best approach is to build in feedback mechanisms from day one. Don't wait until your model is in production to think about this. The code below shows a basic framework, but honestly, in real applications, you'll want something more sophisticated - maybe a web interface where domain experts can quickly correct misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nfrom datetime import datetime\nimport logging\n\nclass HumanInTheLoopFeedback:\n    \"\"\"\n    Manage human-in-the-loop feedback for continuous model improvement.\n    \n    This class handles feedback collection, storage, and model retraining\n    based on expert corrections and annotations.\n    \"\"\"\n    \n    def __init__(self, model, tokenizer, feedback_file='feedback_log.json'):\n        \"\"\"\n        Initialize the feedback system"
      ]
    }
  ],
  "metadata": {
    "title": "Mastering Domain-Specific LLM Customization: Techniques and Tools Unveiled",
    "description": "Discover how to tailor Large Language Models for specific domains using Retrieval-Augmented Generation, fine-tuning, and prompt engineering to boost relevance and accuracy.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}