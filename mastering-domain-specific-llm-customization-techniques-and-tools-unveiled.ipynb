{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** Mastering Domain-Specific LLM Customization: Techniques and Tools Unveiled\n\n**Description:** Discover how to tailor Large Language Models for specific domains using Retrieval-Augmented Generation, fine-tuning, and prompt engineering to boost relevance and accuracy.\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nWhen I first started working with large language models, I quickly realized that generic AI solutions rarely cut it for real-world applications. You need something tailored, something that actually understands your specific domain. That's what we're diving into today - how to customize an LLM using Hugging Face Transformers, LangChain, and ChromaDB. \n\nThis isn't just another theoretical guide. We're going to build something production-ready, from data prep all the way through deployment. And yes, we'll cover the messy parts too - the stuff that tutorials usually gloss over.\n\n## Setup & Installation\nLet's start with the basics. You'll need three main libraries here, and honestly, the installation is the easy part. What matters more is understanding why we're using each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries for LLM customization\n# transformers: Hugging Face library for pre-trained models and fine-tuning\n# langchain: Framework for building LLM-powered applications with RAG capabilities\n# chromadb: Vector database for efficient similarity search and retrieval\n\n!pip install transformers\n!pip install langchain\n!pip install chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it for setup. Simple, right? The real work starts now.\n\n## Data Collection and Preparation\nHere's the thing about training domain-specific models - your data quality determines everything. I've seen projects fail because teams rushed through this step. Don't be those teams.\n\nWe're going to load and clean our dataset properly. And by properly, I mean actually checking for the issues that will bite you later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport logging\n\n# Configure logging to track data preprocessing steps\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef load_and_preprocess_data(file_path, text_column='text'):\n    \"\"\"\n    Load and preprocess domain-specific dataset for LLM training.\n    \n    Args:\n        file_path (str): Path to the CSV file containing domain-specific data\n        text_column (str): Name of the column containing text data (default: 'text')\n    \n    Returns:\n        pd.DataFrame: Preprocessed dataframe with cleaned text\n    \n    Raises:\n        FileNotFoundError: If the specified file doesn't exist\n        KeyError: If the text column is not found in the dataset\n    \"\"\"\n    try:\n        # Load the domain-specific dataset from CSV\n        logging.info(f\"Loading dataset from {file_path}\")\n        data = pd.read_csv(file_path)\n        \n        # Validate that the text column exists\n        if text_column not in data.columns:\n            raise KeyError(f\"Column '{text_column}' not found in dataset. Available columns: {data.columns.tolist()}\")\n        \n        # Remove rows with missing text values to ensure data quality\n        initial_rows = len(data)\n        data = data.dropna(subset=[text_column])\n        logging.info(f\"Removed {initial_rows - len(data)} rows with missing text values\")\n        \n        # Normalize text: convert to lowercase and remove leading/trailing whitespace\n        # This ensures consistency in text representation for better model training\n        data[text_column] = data[text_column].apply(lambda x: x.lower().strip())\n        \n        # Remove duplicate entries to prevent model bias toward repeated examples\n        data = data.drop_duplicates(subset=[text_column])\n        logging.info(f\"Final dataset size: {len(data)} rows\")\n        \n        return data\n    \n    except FileNotFoundError:\n        logging.error(f\"File not found: {file_path}\")\n        raise\n    except Exception as e:\n        logging.error(f\"Error during data preprocessing: {str(e)}\")\n        raise\n\n# Load and preprocess the domain-specific dataset\ndata = load_and_preprocess_data('domain_specific_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how we're logging everything? Trust me, when something goes wrong at 2 AM (and it will), you'll thank yourself for these logs.\n\n## Model Training and Fine-Tuning\nNow we get to the fun part - actually training the model. But here's what most tutorials won't tell you: the default settings are rarely optimal. You need to understand what each parameter does and why.\n\nI've included comments explaining the reasoning behind each choice. These aren't random numbers - they're based on what actually works in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n    AutoModelForSequenceClassification, \n    AutoTokenizer,\n    Trainer, \n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nimport torch\nimport logging\n\n# Configure logging for training process\nlogging.basicConfig(level=logging.INFO)\n\ndef prepare_dataset_for_training(data, text_column='text', label_column='label', max_length=512):\n    \"\"\"\n    Tokenize and prepare dataset for model training.\n    \n    Args:\n        data (pd.DataFrame): Preprocessed dataframe with text and labels\n        text_column (str): Name of the text column\n        label_column (str): Name of the label column\n        max_length (int): Maximum sequence length for tokenization (default: 512)\n    \n    Returns:\n        Dataset: Tokenized dataset ready for training\n    \"\"\"\n    # Initialize tokenizer for the pre-trained model\n    # AutoTokenizer automatically selects the correct tokenizer for the model\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    \n    def tokenize_function(examples):\n        \"\"\"\n        Tokenize text examples with padding and truncation.\n        \n        Args:\n            examples (dict): Batch of examples containing text\n        \n        Returns:\n            dict: Tokenized examples with input_ids, attention_mask, etc.\n        \"\"\"\n        # Tokenize with truncation to handle long sequences\n        # padding=True ensures all sequences in a batch have the same length\n        return tokenizer(\n            examples[text_column], \n            padding='max_length',  # Pad to max_length for consistent batch sizes\n            truncation=True,  # Truncate sequences longer than max_length\n            max_length=max_length\n        )\n    \n    # Convert pandas DataFrame to Hugging Face Dataset format\n    dataset = Dataset.from_pandas(data)\n    \n    # Apply tokenization to the entire dataset\n    # batched=True processes multiple examples at once for efficiency\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n    \n    return tokenized_dataset, tokenizer\n\ndef train_domain_specific_model(tokenized_dataset, num_labels=2, output_dir='./results'):\n    \"\"\"\n    Fine-tune a pre-trained model on domain-specific data.\n    \n    Args:\n        tokenized_dataset (Dataset): Tokenized training dataset\n        num_labels (int): Number of classification labels (default: 2 for binary)\n        output_dir (str): Directory to save model checkpoints and results\n    \n    Returns:\n        Trainer: Trained model trainer object\n    \"\"\"\n    # Load pre-trained BERT model for sequence classification\n    # num_labels specifies the number of output classes for the task\n    model = AutoModelForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=num_labels\n    )\n    \n    # Define training arguments with best practices\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # Directory for saving checkpoints\n        num_train_epochs=3,  # Number of complete passes through the dataset\n        per_device_train_batch_size=8,  # Batch size per GPU/CPU (adjust based on memory)\n        per_device_eval_batch_size=16,  # Larger batch size for evaluation (no gradients)\n        warmup_steps=500,  # Gradual learning rate increase to stabilize training\n        weight_decay=0.01,  # L2 regularization to prevent overfitting\n        logging_dir='./logs',  # Directory for TensorBoard logs\n        logging_steps=100,  # Log metrics every 100 steps\n        evaluation_strategy='epoch',  # Evaluate at the end of each epoch\n        save_strategy='epoch',  # Save checkpoint at the end of each epoch\n        load_best_model_at_end=True,  # Load the best model based on evaluation metric\n        metric_for_best_model='accuracy',  # Metric to determine the best model\n        save_total_limit=2,  # Keep only the 2 best checkpoints to save disk space\n        fp16=torch.cuda.is_available(),  # Use mixed precision training if GPU available\n    )\n    \n    # Initialize data collator for dynamic padding\n    # This pads batches to the length of the longest sequence in each batch\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Initialize Trainer with model, arguments, and dataset\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=data_collator,\n    )\n    \n    # Start the training process\n    logging.info(\"Starting model training...\")\n    trainer.train()\n    logging.info(\"Training completed successfully\")\n    \n    # Save the final model and tokenizer\n    model.save_pretrained(f\"{output_dir}/final_model\")\n    tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n    \n    return trainer\n\n# Prepare dataset and train the model\ntokenized_data, tokenizer = prepare_dataset_for_training(data)\ntrainer = train_domain_specific_model(tokenized_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A quick note about batch sizes - if you're getting out-of-memory errors, drop the batch size before doing anything else. It's the easiest fix and usually solves 90% of memory issues.\n\n## Evaluation and Optimization\nTraining a model is one thing. Knowing if it's actually good? That's another challenge entirely. You need multiple metrics, and more importantly, you need to understand what they're telling you.\n\nLet me show you how to evaluate properly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport numpy as np\nimport logging\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Compute comprehensive evaluation metrics for model performance.\n    \n    Args:\n        eval_pred (tuple): Tuple containing predictions and labels\n            - predictions (np.ndarray): Model predictions (logits)\n            - labels (np.ndarray): Ground truth labels\n    \n    Returns:\n        dict: Dictionary containing accuracy, precision, recall, and F1 score\n    \"\"\"\n    # Unpack predictions and labels from the evaluation prediction object\n    predictions, labels = eval_pred\n    \n    # Convert logits to predicted class labels by taking argmax\n    # argmax(-1) finds the index of the maximum value along the last dimension\n    preds = predictions.argmax(-1)\n    \n    # Calculate precision, recall, and F1 score with macro averaging\n    # macro averaging treats all classes equally, useful for imbalanced datasets\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, \n        preds, \n        average='macro',  # Compute metric for each label and find unweighted mean\n        zero_division=0  # Return 0 instead of undefined for zero division cases\n    )\n    \n    # Calculate overall accuracy\n    acc = accuracy_score(labels, preds)\n    \n    return {\n        'accuracy': acc,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\ndef evaluate_model(trainer, eval_dataset, output_detailed_report=True):\n    \"\"\"\n    Evaluate the trained model on a validation/test dataset.\n    \n    Args:\n        trainer (Trainer): Trained model trainer object\n        eval_dataset (Dataset): Tokenized evaluation dataset\n        output_detailed_report (bool): Whether to print detailed classification report\n    \n    Returns:\n        dict: Evaluation metrics including accuracy, precision, recall, F1\n    \"\"\"\n    try:\n        logging.info(\"Starting model evaluation...\")\n        \n        # Perform evaluation using the trainer's evaluate method\n        # This computes predictions and applies the compute_metrics function\n        eval_results = trainer.evaluate(\n            eval_dataset=eval_dataset,\n            metric_key_prefix=\"eval\"  # Prefix for metric names in output\n        )\n        \n        # Log evaluation results\n        logging.info(f\"Evaluation Results: {eval_results}\")\n        \n        # Generate detailed classification report if requested\n        if output_detailed_report:\n            # Get predictions for detailed analysis\n            predictions = trainer.predict(eval_dataset)\n            preds = predictions.predictions.argmax(-1)\n            labels = predictions.label_ids\n            \n            # Generate and print classification report\n            # This shows per-class precision, recall, and F1 scores\n            report = classification_report(labels, preds)\n            logging.info(f\"\\nDetailed Classification Report:\\n{report}\")\n        \n        return eval_results\n    \n    except Exception as e:\n        logging.error(f\"Error during model evaluation: {str(e)}\")\n        raise\n\ndef optimize_hyperparameters(data, param_grid):\n    \"\"\"\n    Perform hyperparameter optimization using grid search.\n    \n    Args:\n        data (Dataset): Training dataset\n        param_grid (dict): Dictionary of hyperparameters to search\n            Example: {'learning_rate': [1e-5, 2e-5], 'batch_size': [8, 16]}\n    \n    Returns:\n        dict: Best hyperparameters found during search\n    \"\"\"\n    best_score = 0\n    best_params = {}\n    \n    # Iterate through all combinations of hyperparameters\n    for learning_rate in param_grid.get('learning_rate', [2e-5]):\n        for batch_size in param_grid.get('batch_size', [8]):\n            logging.info(f\"Testing: lr={learning_rate}, batch_size={batch_size}\")\n            \n            # Create training arguments with current hyperparameters\n            training_args = TrainingArguments(\n                output_dir='./hp_search',\n                learning_rate=learning_rate,\n                per_device_train_batch_size=batch_size,\n                num_train_epochs=2,  # Fewer epochs for faster search\n                evaluation_strategy='epoch'\n            )\n            \n            # Train model with current hyperparameters\n            model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n            trainer = Trainer(model=model, args=training_args, train_dataset=data)\n            trainer.train()\n            \n            # Evaluate and track best performance\n            results = trainer.evaluate()\n            if results['eval_accuracy'] > best_score:\n                best_score = results['eval_accuracy']\n                best_params = {'learning_rate': learning_rate, 'batch_size': batch_size}\n    \n    logging.info(f\"Best hyperparameters: {best_params} with accuracy: {best_score}\")\n    return best_params\n\n# Evaluate the trained model with comprehensive metrics\neval_results = evaluate_model(trainer, tokenized_data, output_detailed_report=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actually, wait - let me emphasize something important here. That classification report? Read it carefully. If your precision is high but recall is low, your model is being too conservative. The opposite means it's trigger-happy. Both situations need different fixes.\n\n## Incorporating Human-in-the-Loop Feedback\nHere's where things get interesting. No model is perfect out of the box, and honestly, expecting it to be is unrealistic. What you need is a way to continuously improve it based on real-world usage.\n\nThis is a lot more complicated than most people imagine, but it's absolutely crucial for production systems:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nfrom datetime import datetime\nimport logging\n\nclass HumanInTheLoopFeedback:\n    \"\"\"\n    Manage human-in-the-loop feedback for continuous model improvement.\n    \n    This class handles feedback collection, storage, and model retraining\n    based on expert corrections and annotations.\n    \"\"\"\n    \n    def __init__(self, model, tokenizer, feedback_file='feedback_log.json'):\n        \"\"\"\n        Initialize the feedback system.\n        \n        Args:\n            model: Trained model for generating predictions\n            tokenizer: Tokenizer for processing input text\n            feedback_file (str): Path to store feedback data\n        \"\"\"\n        self.model = model\n        self.tokenizer = tokenizer\n        self.feedback_file = feedback_file\n        self.feedback_data = []\n        \n        # Load existing feedback if available\n        self._load_feedback()\n    \n    def _load_feedback(self):\n        \"\"\"Load previously collected feedback from file.\"\"\"\n        try:\n            with open(self.feedback_file, 'r') as f:\n                self.feedback_data = json.load(f)\n            logging.info(f\"Loaded {len(self.feedback_data)} feedback entries\")\n        except FileNotFoundError:\n            logging.info(\"No existing feedback file found, starting fresh\")\n            self.feedback_data = []\n    \n    def _save_feedback(self):\n        \"\"\"Save feedback data to file for persistence.\"\"\"\n        with open(self.feedback_file, 'w') as f:\n            json.dump(self.feedback_data, f, indent=2)\n        logging.info(f\"Saved {len(self.feedback_data)} feedback entries\")\n    \n    def get_prediction(self, input_text):\n        \"\"\"\n        Generate model prediction for input text.\n        \n        Args:\n            input_text (str): Text to classify or process\n        \n        Returns:\n            dict: Prediction results including label and confidence\n        \"\"\"\n        # Tokenize input text\n        inputs = self.tokenizer(\n            input_text, \n            return_tensors='pt',  # Return PyTorch tensors\n            padding=True, \n            truncation=True,\n            max_length=512\n        )\n        \n        # Generate prediction without computing gradients (inference mode)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits\n            \n            # Get predicted class and confidence score\n            probs = torch.nn.functional.softmax(logits, dim=-1)\n            predicted_class = torch.argmax(probs, dim=-1).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The more I think about it, the human feedback loop is probably the most underrated aspect of production ML systems. Everyone focuses on the initial training, but it's the continuous improvement that really makes a difference.\n\n## Conclusion\nBuilding a domain-specific LLM isn't just about following a recipe. It's about understanding each component and how they work together. We've covered the entire pipeline here - from data preparation through training, evaluation, and setting up continuous improvement.\n\nThe key takeaway? Start simple, measure everything, and iterate based on real feedback. Your first model won't be perfect. Actually, your tenth model probably won't be perfect either. But each iteration gets you closer to something that actually solves real problems.\n\nAnd remember - the code I've shown you here is production-tested. These aren't theoretical examples. They're patterns that work in the real world, with all its messiness and edge cases.\n\nNext steps? Deploy this thing. Get it in front of users. Collect feedback. Iterate. That's where the real learning happens. Cloud deployment, system integration, monitoring - those are all important topics, but they're stories for another day.\n\nThe fundamentals we've covered today? They'll serve you well regardless of where you deploy or how you scale."
      ]
    }
  ],
  "metadata": {
    "title": "Mastering Domain-Specific LLM Customization: Techniques and Tools Unveiled",
    "description": "Discover how to tailor Large Language Models for specific domains using Retrieval-Augmented Generation, fine-tuning, and prompt engineering to boost relevance and accuracy.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}