{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìì The GenAI Revolution Cookbook\n\n**Title:** The Ultimate Guide to Vector Store Retrieval for RAG Systems\n\n**Description:** Build reliable RAG today: implement vector store retrieval with semantic search, chunking, and embeddings‚Äîmanually or with LangChain‚Äîto reduce LLM hallucinations.\n\n**üìñ Read the full article:** [The Ultimate Guide to Vector Store Retrieval for RAG Systems](https://blog.thegenairevolution.com/article/the-ultimate-guide-to-vector-store-retrieval-for-rag-systems)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieval Augmented Generation (or RAG, as everyone calls it) is one of those design patterns that actually delivers on its promise. I've seen it power everything from customer support chatbots to internal knowledge assistants. The best part? It tackles that persistent problem we all face with LLMs. It grounds their responses in real information, which dramatically cuts down on hallucinations.\n\nRAG is the backbone of data\\-aware chatbots and support tools everywhere. If you want to see how this works in practice, I put together a tutorial on building a [multi\\-agent chatbot with ChromaDB RAG](/article/how-to-build-a-multi-agent-chatbot-with-crewai-chromadb-gradio-4) that shows the whole process end to end.\n\nNow, RAG has two main components: retrieval and generation. This guide focuses on RAG 101, specifically the retrieval part. We're going to zero in on finding the right context for your model. You'll build an index, search through it, first manually to really understand what's happening, then using LangChain to speed things up. Once you've got retrieval down, plugging the results into your LLM is straightforward.\n\nThe retrieval process is really the heart of what we're doing here. It's all about finding and selecting the most relevant documents for your use case. There are several approaches you can take, but vector store retrieval has become the go\\-to method for most applications.\n\nLet me walk you through the two essential retrieval steps. First, you'll create the index. Then you'll search it. I want you to see the mechanics clearly, so we'll do it manually first. After that, I'll show you how LangChain makes the whole process much smoother.\n\n## Semantic Similarity Search Done Manually\n\nSemantic Similarity Search is probably the most widely used retrieval approach out there. The idea is simple: find documents that best match your input based on meaning, not just keywords. This approach really shines when you need to match semantic content across text. Say you have a user query and you want to find the most relevant sentences or paragraphs in your documentation. It's particularly powerful for retrieving internal knowledge to enhance customer support or adding grounded context to LLM responses.\n\nWe're going to implement Semantic Similarity Search manually using Sentence Transformers for embeddings and ChromaDB as our vector store. If you're curious about [how transformer models generate embeddings](/article/transformers-demystifying-the-magic-behind-large-language-models-2) and why they work so well for semantic search, I've written a detailed guide that explains the underlying mechanics.\n\n## Setup\n\nBefore diving in, let's get the required Python libraries installed. Here's what we'll use:\n\n* **chromadb**: A fast and efficient vector database that's become my go\\-to choice\n* **sentence\\-transformers**: For generating sentence embeddings\n* **PyPDF**: A modern library for reading and processing PDF files\n* **langchain**: The framework for building LLM\\-powered applications\n* **langchain\\-community**: Extensions and integrations for LangChain\n* **langchain\\-huggingface**: Tools to use Hugging Face models within LangChain\n\nBy the way, if you're interested in [integrating LLMs into your Python workflow](/article/how-to-boost-workflow-with-llm-pair-programming-in-jupyter-ai-2) for tasks beyond retrieval, I've found some practical techniques for LLM pair programming in Jupyter AI that might help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install chromadb\n%pip install sentence-transformers\n%pip install pypdf\n%pip install langchain\n%pip install langchain-community\n%pip install langchain-huggingface\n%pip install langchain-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create the Index\n\nAlright, with everything installed, let's prepare an index for your documents. The process breaks down into four main parts:\n\n1. **Load the documents**: Extract text from your source file\n2. **Split the text**: Break it into semantically meaningful chunks\n3. **Generate embeddings**: Use a pre\\-trained model to represent each chunk as a vector\n4. **Store data in a vector database**: Save chunks, embeddings, and metadata for retrieval\n\nI'm going to use the Minutes of the Federal Open Market Committee as our example document. You can find it here: [https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20241107\\.pdf](https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20241107.pdf)\n\nHere's the step\\-by\\-step implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\nfrom sentence_transformers import SentenceTransformer\nfrom chromadb import Client\nfrom chromadb.config import Settings\nfrom pypdf import PdfReader\n\n# Step 2: Define functions for processing\n\ndef load_pdf(filepath):\n    \"\"\"\n    Load text from a PDF document.\n    \"\"\"\n    reader = PdfReader(filepath)\n    text = \"\"\n    for page in reader.pages:\n        text += page.extract_text()\n    print(f\"Text with {len(text.split())} words extracted from pdf {filepath}.\")\n    return text\n\ndef split_text(text, chunk_size=500, overlap=100):\n    \"\"\"\n    Split the text into chunks of a specified size with overlap.\n    \"\"\"\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = start + chunk_size\n        chunks.append(text[start:end])\n        start = end - overlap\n    print(f\"Text split in {len(chunks)} chunks.\")\n    return chunks\n\n# Step 3: Load and process the document\npdf_path = \"fomcminutes20241107.pdf\"  # Replace with your PDF path\ndocument_text = load_pdf(pdf_path)\ndocument_chunks = split_text(document_text)\n\n# Step 4: Generate embeddings\n# Load a pre-trained SentenceTransformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight and fast embedding model\nembeddings = model.encode(document_chunks)\nprint(f\"Embeddings generated for {len(embeddings)} chunks.\")\nprint(f\"Each embedding is of length {len(embeddings[0])}.\")\n\n# Step 5: Set up ChromaDB and create the index\npersist_directory = \"./chroma_db\"\nchroma_client = Client(Settings(persist_directory=persist_directory))\n\ncollection_name = \"fomc_minutes_20241107\"\ncollection = chroma_client.create_collection(collection_name)\nprint(f\"ChromaDB collection {collection_name} created.\")\n\n# Prepare bulk data for adding to the collection\nids = [f\"chunk_{i}\" for i in range(len(document_chunks))]\nmetadatas = [{\"chunk_id\": i, \"source\": \"sample_product_document.pdf\"} for i in range(len(document_chunks))]\n\n# Add chunks, embeddings, and metadata in bulk\ncollection.add(\n    documents=document_chunks,\n    metadatas=metadatas,\n    ids=ids,\n    embeddings=embeddings.tolist(),  # Convert numpy array to list\n)\n\n# Step 7: Confirm the index\nprint(f\"All {collection.count()} documents added to the collection {collection_name}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Text with 7385 words extracted from pdf fomcminutes20241107.pdf.\nText split in 127 chunks.\nEmbeddings generated for 127 chunks.\nEach embedding is of length 384.\nChromaDB collection fomc_minutes_20241107 created.\nAll 127 documents added to the collection fomc_minutes_20241107."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Search the Index\n\nNow that we've populated our vector database, let's perform some searches. The process is straightforward:\n\n* **Load the collection**: Initialize the database client and load your collection\n* **Generate a query embedding**: Convert your search query into a vector\n* **Retrieve results**: Use the vector database to find the most similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Initialize ChromaDB client and load the collection\n# We won't run this since the chroma client was already created in this session\n\n# chroma_client = Client(Settings(persist_directory=\"chroma_db\")) \n# collection_name = \"fomc_minutes_20241107\"\n# collection = chroma_client.get_collection(collection_name)\n# print(f\"Collection '{collection_name}' loaded successfully.\")\n\n# Step 2: Define the search query\nsearch_query = \"What was discussed about monetary policy?\"  # Replace with your query\nprint(f\"Search Query: {search_query}\")\n\n# Step 3: Generate embeddings for the search query\nquery_embedding = model.encode([search_query])  # Generate embedding for the query\nprint(f\"Embedding generated for search query.\")\nprint(f\"Embedding is of length {len(query_embedding[0])}.\")\n\n# Step 4: Perform the search\n# Set the number of top results to retrieve\ntop_k = 5\nresults = collection.query(\n    query_embeddings=query_embedding.tolist(),  # Convert numpy array to list\n    n_results=top_k,\n)\n\n# Access the first (and only) batch of results\ndocuments = results['documents'][0]\nmetadatas = results['metadatas'][0]\ndistances = results['distances'][0]\n\n# Print each result\nprint(f\"\\nTop {top_k} Results:\")\nfor i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances), start=1):\n    print(f\"\\nResult {i}:\")\n    print(f\"Document Chunk: {doc}\")\n    print(f\"Metadata: {metadata}\")\n    print(f\"Distance: {distance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Search Query: What was discussed about monetary policy?\nEmbedding generated for search query.\nEmbedding is of length 384.\n\nTop 5 Results:\n\nResult 1:\nDocument Chunk: to\ncontinue the process of reducing the Federal Reserve‚Äôs securities holdings.\nIn discussing the outlook for monetary policy, participants anticipated that if the data came in about\nas expected, with inflation continuing to move down sustainably to 2 percent and the economy\nremaining near maximum employment, it would likely be appropriate to move gradually toward a more\nneutral stance of policy over time. Participants noted that monetary policy decisions were not on a\npreset course and w\nMetadata: {'chunk_id': 84, 'source': 'sample_product_document.pdf'}\nDistance: 0.6293219923973083\n\nResult 2:\nDocument Chunk: icy over time. Participants noted that monetary policy decisions were not on a\npreset course and were conditional on the evolution of the economy and the implications for the\neconomic outlook and the balance of risks; they stressed that it would be important for the Committee\nto make this clear as it adjusted its policy stance. While emphasizing that monetary policy would be\ndata dependent, many participants noted the volatility of recent economic data and highlighted the\nimportance of fo\nMetadata: {'chunk_id': 85, 'source': 'sample_product_document.pdf'}\nDistance: 0.7037546634674072\n\nResult 3:\nDocument Chunk: percent objective.\nMembers agreed that, in assessing the appropriate stance of monetary policy, they would continue to\nmonitor the implications of incoming information for the economic outlook. They would be prepared to Minutes of the Federal Open Market Committee 13\n\nadjust the stance of monetary policy as appropriate if risks emerged that could impede the attainment\nof the Committee‚Äôs goals. Members also agreed that their assessments would take into account a\nwide range of informati\nMetadata: {'chunk_id': 95, 'source': 'sample_product_document.pdf'}\nDistance: 0.736047089099884\n\nResult 4:\nDocument Chunk: tered. Many participants observed\nthat uncertainties concerning the level of the neutral rate of interest complicated the assessment of\nthe degree of restrictiveness of monetary policy and, in their view, made it appropriate to reduce policy\nrestraint gradually.\nCommittee Policy Actions\nIn their discussions of monetary policy for this meeting, members agreed that economic activity had\ncontinued to expand at a solid pace. Labor market conditions had generally eased since earlier in the\ny\nMetadata: {'chunk_id': 90, 'source': 'sample_product_document.pdf'}\nDistance: 0.736309289932251\n\nResult 5:\nDocument Chunk: t and inflation goals\nremained roughly in balance. Some participants judged that downside risks to economic activity or\nthe labor market had diminished. Participants noted that monetary policy would need to balance the\nrisks of easing policy too quickly, thereby possibly hindering further progress on inflation, with the risks\nof easing policy too slowly, thereby unduly weakening economic activity and employment. In\ndiscussing the positioning of monetary policy in response to potential ch\nMetadata: {'chunk_id': 88, 'source': 'sample_product_document.pdf'}\nDistance: 0.8098248243331909"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streamlining with LangChain: RAG 101 Retrieval Made Simpler\n\nThe manual approach gives you complete control, which is great for understanding what's happening under the hood. But honestly, it can be verbose and requires quite a bit of setup. This is where LangChain really shines. It abstracts many of these steps and helps you build retrieval\\-augmented applications much faster. Let me show you how to replicate the same process using LangChain's high\\-level utilities.\n\n## Step 1: Create the Index\n\nWith LangChain, creating an index becomes much more intuitive. The library provides tools for document loading, text splitting, and embedding generation all in one place. Here's how to create an index for the same Federal Open Market Committee document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain.vectorstores import Chroma\n\n# Step 2: Load the document\npdf_path = \"fomcminutes20241107.pdf\"  # Replace with your PDF path\nloader = PyPDFLoader(pdf_path)  # LangChain's PDF loader\ndocuments = loader.load()  # Load text from the PDF\nprint(f\"Loaded {len(documents)} page(s) from the PDF.\")\n\n# Step 3: Split the text into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\nchunks = text_splitter.split_documents(documents)\nprint(f\"Split all text into {len(chunks)} chunks.\")\n\n# Step 4: Generate embeddings\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Step 5: Store in a Vector Database\npersist_directory = \"fomc_vector_db\"\nvector_db = Chroma.from_documents(\n    documents=chunks,\n    embedding=embedding_model,\n    persist_directory=persist_directory,  # Persistence is automatic in Chroma >= 0.4.x\n)\n\nprint(f\"Vector database created and stored at '{persist_directory}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Loaded 17 page(s) from the PDF.\nSplit all text into 131 chunks.\nVector database created and stored at 'fomc_vector_db'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Search the Index\n\nLangChain also simplifies the querying process by managing the search internally. Here's how to perform a semantic search on your vector database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import required libraries\nfrom langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings \n\n# Step 2: Load the vector database\npersist_directory = \"fomc_vector_db\"\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvector_db = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embedding_model,\n)\nprint(f\"Vector database loaded from '{persist_directory}'.\")\n\n# Step 3: Define the search query\nquery = \"What were the key points discussed about monetary policy?\"\nprint(f\"Search Query: {query}\")\n\n# Step 4: Perform the search\ntop_k = 5  # Number of top results to retrieve\nresults = vector_db.similarity_search(query, k=top_k)\n\n# Step 5: Display the results\nprint(f\"\\nTop {top_k} Results:\")\nfor i, result in enumerate(results, start=1):\n    print(f\"\\nResult {i}:\")\n    print(f\"Document Chunk: {result.page_content}\")\n    print(f\"Metadata: {result.metadata}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Search Query: What were the key points discussed about monetary policy?\n\nTop 5 Results:\n\nResult 1:\nDocument Chunk: 25 basis points to 4¬Ω to 4¬æ percent. Participants observed that such a further recalibration of the\nmonetary policy stance would help maintain the strength in the economy and the labor market while\ncontinuing to enable further progress on inflation. Participants judged that it was appropriate to\ncontinue the process of reducing the Federal Reserve‚Äôs securities holdings.\nIn discussing the outlook for monetary policy, participants anticipated that if the data came in about\nMetadata: {'page': 10, 'source': 'fomcminutes20241107.pdf'}\n\nResult 2:\nDocument Chunk: as expected, with inflation continuing to move down sustainably to 2 percent and the economy\nremaining near maximum employment, it would likely be appropriate to move gradually toward a more\nneutral stance of policy over time. Participants noted that monetary policy decisions were not on a\npreset course and were conditional on the evolution of the economy and the implications for the\neconomic outlook and the balance of risks; they stressed that it would be important for the Committee\nMetadata: {'page': 10, 'source': 'fomcminutes20241107.pdf'}\n\nResult 3:\nDocument Chunk: would be prepared to adjust the stance of monetary policy as appropriate if risks emerge that\ncould impede the attainment of the Committee‚Äôs goals. The Committee‚Äôs assessments will\ntake into account a wide range of information, including readings on labor market conditions,\ninflation pressures and inflation expectations, and financial and international developments.‚Äù\nVoting for this action: Jerome H. Powell, John C. Williams, Thomas I. Barkin, Michael S. Barr,\nMetadata: {'page': 13, 'source': 'fomcminutes20241107.pdf'}\n\nResult 4:\nDocument Chunk: commencement of policy easing in September and therefore was no longer needed. Almost all\nmembers agreed that the risks to achieving the Committee‚Äôs employment and inflation goals were\nroughly in balance. Members viewed the economic outlook as uncertain and agreed that they were\nattentive to the risks to both sides of the Committee‚Äôs dual mandate.\nIn support of its goals, the Committee decided to lower the target range for the federal funds rate by\nMetadata: {'page': 11, 'source': 'fomcminutes20241107.pdf'}\n\nResult 5:\nDocument Chunk: American countries, notably Brazil, inflation increased, partly because of renewed food price\npressures.\nMany foreign central banks eased policy during the intermeeting period, including the Bank of Canada\nand the European Central Bank among the AFEs and the central banks of Colombia, Mexico, Korea,\nthe Philippines, and Thailand among the emerging market economies.\nStaff Review of the Financial Situation\nMetadata: {'page': 3, 'source': 'fomcminutes20241107.pdf'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nRetrieval really is the foundation of RAG. This is RAG 101 at its core. You've learned how to create an index by processing and embedding documents, and how to search that index to find relevant information based on queries. We worked through both methods using the Minutes of the Federal Open Market Committee. The manual path gave you transparency and control, while the LangChain path showed how abstraction can save time and simplify your workflows.\n\nYou're now ready to apply retrieval in your own applications. Use it to enhance chatbot responses. Build powerful search experiences. Remember, retrieval is the heart of RAG. It's what enables meaningful, grounded interactions in LLM\\-powered applications. And if you want to further improve the quality and reliability of your LLM responses, check out my guide on [prompt engineering strategies for reliable LLM outputs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4)."
      ]
    }
  ],
  "metadata": {
    "title": "The Ultimate Guide to Vector Store Retrieval for RAG Systems",
    "description": "Build reliable RAG today: implement vector store retrieval with semantic search, chunking, and embeddings‚Äîmanually or with LangChain‚Äîto reduce LLM hallucinations.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}