{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** RAG Application: 7 Retrieval Tricks to Boost Answer Accuracy\n\n**Description:** Upgrade your RAG pipeline with proven chunking, MMR, metadata filters, and BM25/TFâ€‘IDF choices to deliver sharper, trustworthy answers consistently today.\n\n**ðŸ“– Read the full article:** [RAG Application: 7 Retrieval Tricks to Boost Answer Accuracy](https://blog.thegenairevolution.com/article/rag-application-7-retrieval-tricks-to-boost-answer-accuracy)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most RAG pipelines fail on retrieval, not generation. Here's the thing: in this hands\\-on build, you'll implement seven proven retrieval techniques. We're talking chunking, semantic search, MMR, metadata filters, BM25, TF\\-IDF\\+FAISS, and a hybrid router. The goal? Deliver sharper, more trustworthy answers from large document sets. This guide focuses on the retrieval layer only. You'll build a complete context\\-fetching system that returns top\\-k chunks with reduced redundancy, normalized scores, and measured latency. By the end, you'll have a working hybrid retrieval function ready to integrate with any LLM generation step. For those interested in exploring retrieval pipelines that leverage structured relationships, our [guide to building a Knowledge Graph RAG pipeline with Neo4j and embeddings](/article/how-to-build-a-knowledge-graph-rag-pipeline-with-neo4j-embeddings-2) offers a step\\-by\\-step alternative.\n\n## Why This Approach Works\n\nSingle\\-method retrieval misses too much. Let me be clear about this. Semantic search alone ignores exact keywords. BM25 alone misses conceptual matches. Combining multiple strategies, each tuned for different query types, ensures you capture the right evidence every time.\n\nThis build uses LangChain for retriever adapters and splitters, Chroma for fast local vector search, FAISS for scalable similarity, rank\\-bm25 for exact matches, and sklearn TF\\-IDF for keyword baselines. Now, alternatives like Elasticsearch, Qdrant, or Weaviate offer production\\-grade scale and persistence. But this stack prioritizes speed, simplicity, and Colab compatibility for rapid prototyping.\n\n## How It Works (High\\-Level Overview)\n\nYou'll split documents into chunks using three strategies, embed them with SentenceTransformers, and build indexes for semantic (Chroma), keyword (BM25\\), and TF\\-IDF\\+FAISS retrieval. Then you'll add MMR for diversity, metadata filters for precision, and a hybrid router that selects the best retrieval path based on query heuristics.\n\nEach technique addresses a specific failure mode. Semantic drift, keyword blindness, redundancy, or irrelevant context. The final router merges results, deduplicates, and returns top\\-k chunks ready for LLM consumption.\n\n## Project Overview\n\n**The concrete app:** A retrieval layer that accepts a query and returns the top\\-k most relevant document chunks from a corpus, using a hybrid of semantic, keyword, and metadata\\-driven strategies.\n\n**The real problem:** Single\\-method retrieval fails on diverse queries. Semantic search misses exact codes, BM25 misses paraphrases, and both return redundant or off\\-topic chunks.\n\n**The core challenge:** Combining multiple retrieval methods without score confusion, redundancy, or misrouting, while keeping latency low and results interpretable.\n\n## Setup and Dependencies\n\nThis cell securely loads API keys from Colab userdata. You'll need OpenAI and Anthropic keys for downstream LLM integration. Actually, they're not used in this retrieval\\-only build, but required for full RAG pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom google.colab import userdata\n\nkeys = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\nmissing = [k for k in keys if not (os.environ[k] := userdata.get(k) or None)]\n\nif missing:\n    raise EnvironmentError(f\"Missing keys: {', '.join(missing)}. Add them in Colab â†’ Settings â†’ Secrets.\")\n\nprint(\"All keys loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install all required dependencies. This includes LangChain core and text splitters, Chroma for vector storage, FAISS for similarity search, BM25 for keyword retrieval, and sklearn for TF\\-IDF. Expected runtime is about 30 seconds in Colab. Memory footprint for TF\\-IDF dense conversion is manageable for small to medium corpora, up to around 10k chunks. For larger datasets, consider sparse backends like Elasticsearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install -U langchain langchain-core langchain-community langchain-huggingface langchain-text-splitters chromadb sentence-transformers faiss-cpu rank-bm25 nltk tiktoken scikit-learn\n\nimport os\nimport time\nimport math\nimport numpy as np\nfrom pprint import pprint\nimport logging\n\nimport nltk\ntry:\n    nltk.download('punkt', quiet=True)\nexcept:\n    nltk.download('punkt_tab', quiet=True)\n\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import (\n    RecursiveCharacterTextSplitter,\n    TokenTextSplitter,\n    SentenceTransformersTokenTextSplitter,\n)\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma, FAISS\n\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport faiss\n\nEMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the embedding model and environment setup. This health check ensures the model loads correctly and returns the expected 384\\-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Embeddings model:\", EMBED_MODEL)\nvec = embeddings.embed_query(\"health check\")\nprint(\"Embedding dim:\", len(vec))\nprint(\"FAISS available:\", faiss is not None)\nprint(\"Chroma available:\", Chroma is not None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: Chunking Strategy First\n\nChunk documents using three strategies to match different content. Natural boundaries for prose, token\\-based for LLM limits, and embedding\\-compatible splits for SentenceTransformers. Good chunks are the single biggest lever for retrieval quality.\n\nStart with 256 to 400 tokens and 10 to 20% overlap. Reduce overlap if you see duplicates. If you want to avoid common issues that can undermine chunking and retrieval, see our breakdown of [tokenization pitfalls and invisible characters that break prompts and RAG](/article/tokenization-pitfalls-invisible-characters-that-break-prompts-and-rag-2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_docs = [\n    Document(page_content=\"\"\"Product OrionX v2.3 Release Notes:\n- Added support for TLS 1.3\n- Deprecated config flag net.legacy_mode\n- Fixed CVE-2024-12345 affecting auth handshake\nFor migrations, see Section 4.2.\"\"\",\n            metadata={\"source\": \"release_notes\", \"version\": \"2.3\", \"date\": \"2024-06-01\", \"section\": \"overview\"}),\n\n    Document(page_content=\"\"\"OrionX Admin Manual â€“ Networking:\nTo enable TLS 1.3, set security.tls_version=1.3 in orionx.conf.\nFor FIPS mode, enable crypto.fips=true and restart.\nAvoid using net.legacy_mode in production.\"\"\",\n            metadata={\"source\": \"manual\", \"section\": \"networking\", \"date\": \"2024-05-20\"}),\n\n    Document(page_content=\"\"\"Troubleshooting:\nHandshake failures with error code OX-AUTH-902 typically indicate clock skew.\nVerify NTP and ensure the client presents ECDSA certificates.\nSee Appendix A for certificate chains and sample configs.\"\"\",\n            metadata={\"source\": \"manual\", \"section\": \"troubleshooting\", \"date\": \"2024-05-20\"}),\n\n    Document(page_content=\"\"\"Support KB #KB-7782:\nHow to resolve OX-AUTH-902 during federated login with Azure AD.\nRoot cause: invalid audience in JWT.\nMitigation: set auth.saml.audience=orionx-prod in IdP config.\"\"\",\n            metadata={\"source\": \"kb\", \"id\": \"KB-7782\", \"date\": \"2024-07-15\"}),\n]\n\nrc_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n)\nrc_chunks = rc_splitter.split_documents(raw_docs)\n\ntok_splitter = TokenTextSplitter(chunk_size=256, chunk_overlap=32)\ntok_chunks = tok_splitter.split_documents(raw_docs)\n\nst_splitter = SentenceTransformersTokenTextSplitter(chunk_size=256, chunk_overlap=32)\nst_chunks = st_splitter.split_documents(raw_docs)\n\nprint(\"Recursive chunks:\", len(rc_chunks))\nprint(\"Token-based chunks:\", len(tok_chunks))\nprint(\"ST token chunks:\", len(st_chunks))\nprint(\"--- Sample chunk ---\\n\", rc_chunks[0].page_content, \"\\n\", rc_chunks[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Semantic Search with Chroma\n\nBuild a Chroma vector store for semantic retrieval. This indexes all chunks with SentenceTransformers embeddings and returns the top\\-k most similar chunks for a given query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = st_chunks\n\nchroma_store = Chroma.from_documents(documents=chunks, embedding=embeddings, collection_name=\"orionx_demo\")\n\ndef semantic_search(query, k=4):\n    \"\"\"\n    Perform semantic similarity search using Chroma vector store.\n\n    Args:\n        query (str): The user query.\n        k (int): Number of top results to return.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, relevance_score) tuples.\n    \"\"\"\n    return chroma_store.similarity_search_with_relevance_scores(query, k=k)\n\nresults = semantic_search(\"How do I enable TLS 1.3?\")\nfor doc, score in results:\n    print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:80]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 3: Max Marginal Relevance (MMR)\n\nControl redundancy with MMR, which balances relevance and diversity. Lambda controls the trade\\-off. 1\\.0 is pure relevance, 0\\.0 is pure diversity. Use 0\\.3 to 0\\.7 for most cases. Lower for exploration, higher for precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mmr_search(query, k=4, fetch_k=20, lambda_mult=0.5):\n    \"\"\"\n    Perform Max Marginal Relevance (MMR) search to balance relevance and diversity.\n\n    Args:\n        query (str): The user query.\n        k (int): Number of results to return.\n        fetch_k (int): Number of candidates to fetch before MMR selection.\n        lambda_mult (float): Trade-off between relevance (1.0) and diversity (0.0).\n\n    Returns:\n        List[Tuple[Document, None]]: List of (Document, None) tuples for compatibility.\n    \"\"\"\n    docs = chroma_store.max_marginal_relevance_search(\n        query=query, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult\n    )\n    return [(d, None) for d in docs]\n\nq = \"Troubleshooting ORIONX handshake failures\"\nprint(\"\\n-- Similarity Search --\")\nfor doc, score in semantic_search(q, k=4):\n    print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:80]}\")\n\nprint(\"\\n-- MMR (lambda=0.5) --\")\nfor doc, _ in mmr_search(q, k=4, lambda_mult=0.5):\n    print(f\"     | {doc.metadata} | {doc.page_content.splitlines()[0][:80]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 4: Metadata Filtering\n\nAdd precision with metadata filtering. This restricts retrieval to chunks matching specific metadata keys, such as source type, section, or date range. Normalize metadata keys at ingestion to avoid silent filter misses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filtered_search(query, meta_filter: dict, k=4):\n    \"\"\"\n    Perform semantic search with metadata filtering.\n\n    Args:\n        query (str): The user query.\n        meta_filter (dict): Metadata key-value pairs to filter on.\n        k (int): Number of results to return.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, relevance_score) tuples.\n    \"\"\"\n    return chroma_store.similarity_search_with_relevance_scores(query, k=k, filter=meta_filter)\n\nprint(\"\\n-- Filter: source=manual, section=networking --\")\nfor doc, score in filtered_search(\"Enable TLS 1.3\", {\"source\": \"manual\", \"section\": \"networking\"}, k=3):\n    print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:80]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 5: BM25 for Exact Matches\n\nAdd BM25 for exact\\-match and jargon\\-heavy queries. BM25 excels at keyword precision, especially for error codes, product names, and technical identifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text):\n    \"\"\"\n    Tokenize text for BM25 using NLTK's word_tokenize.\n\n    Args:\n        text (str): Input text.\n\n    Returns:\n        List[str]: List of lowercased tokens.\n    \"\"\"\n    return nltk.word_tokenize(text.lower())\n\nbm25_corpus = [c.page_content for c in chunks]\nbm25_tokens = [tokenize(t) for t in bm25_corpus]\nbm25 = BM25Okapi(bm25_tokens)\n\ndef bm25_search(query, k=5):\n    \"\"\"\n    Perform BM25 keyword search.\n\n    Args:\n        query (str): The user query.\n        k (int): Number of top results to return.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, BM25_score) tuples.\n    \"\"\"\n    tokens = tokenize(query)\n    scores = bm25.get_scores(tokens)\n    idxs = np.argsort(scores)[::-1][:k]\n    out = []\n    for i in idxs:\n        out.append((chunks[i], float(scores[i])))\n    return out\n\nprint(\"\\n-- BM25: technical query with specific token --\")\nfor doc, score in bm25_search(\"Resolve OX-AUTH-902 handshake failures\", k=4):\n    print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 6: TF\\-IDF \\+ FAISS for Keyword Retrieval\n\nAdd TF\\-IDF \\+ FAISS for scalable keyword retrieval with cosine similarity. This approach is fast for small to medium corpora, up to about 10k chunks. For larger datasets, consider sparse backends like Elasticsearch or Pyserini to avoid dense conversion overhead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\ntfidf_matrix = vectorizer.fit_transform(bm25_corpus)\ntfidf_dense = tfidf_matrix.astype(np.float32).toarray()\n\ndef l2_normalize(mat):\n    \"\"\"\n    L2-normalize a matrix for cosine similarity via inner product.\n\n    Args:\n        mat (np.ndarray): Input matrix.\n\n    Returns:\n        np.ndarray: L2-normalized matrix.\n    \"\"\"\n    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12\n    return mat / norms\n\ntfidf_norm = l2_normalize(tfidf_dense)\n\ndim = tfidf_norm.shape[1]\nfaiss_index = faiss.IndexFlatIP(dim)\nfaiss_index.add(tfidf_norm)\n\ndef tfidf_faiss_search(query, k=5):\n    \"\"\"\n    Perform TF-IDF + FAISS search for keyword/phrase queries.\n\n    Args:\n        query (str): The user query.\n        k (int): Number of top results to return.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, cosine_score) tuples.\n    \"\"\"\n    q_vec = vectorizer.transform([query]).astype(np.float32).toarray()\n    q_vec = l2_normalize(q_vec)\n    D, I = faiss_index.search(q_vec, k)\n    out = []\n    for score, idx in zip(D[0], I[0]):\n        out.append((chunks[int(idx)], float(score)))\n    return out\n\nprint(\"\\n-- TF-IDF+FAISS: phrase query --\")\nfor doc, score in tfidf_faiss_search(\"enable TLS 1.3 in config\", k=4):\n    print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 7: Hybrid Retrieval Router\n\nBuild a hybrid retrieval router that combines all retrieval strategies. The router uses query heuristics to select the best path. Semantic for conceptual queries, MMR for long queries, BM25/TF\\-IDF for keyword\\-heavy queries, and metadata filters for time\\-bounded or source\\-specific queries. Results are merged, deduplicated, and normalized to ensure consistent ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n\ndef is_keyword_heavy(query):\n    \"\"\"\n    Heuristic to detect if a query is keyword-heavy (IDs, codes, symbols).\n\n    Args:\n        query (str): The user query.\n\n    Returns:\n        bool: True if keyword-heavy, else False.\n    \"\"\"\n    version_pattern = r'\\bv?\\d+(.\\d+)*\\b'\n    error_code_pattern = r'\\b[A-Z]{2,}-[A-Z0-9-]+\\b'\n    date_pattern = r'\\b20\\d{2}\\b'\n    \n    versions = len(re.findall(version_pattern, query))\n    codes = len(re.findall(error_code_pattern, query))\n    dates = len(re.findall(date_pattern, query))\n    \n    return (versions + codes + dates) >= 1\n\ndef is_time_bounded(query):\n    \"\"\"\n    Heuristic to detect if a query is time-bounded.\n\n    Args:\n        query (str): The user query.\n\n    Returns:\n        bool: True if time-bounded, else False.\n    \"\"\"\n    triggers = [\"before\", \"after\", \"since\", \"between\", \"version\", r\"\\bv\\d+\"]\n    pattern = \"|\".join(triggers)\n    return bool(re.search(pattern, query.lower()))\n\ndef pick_semantic_mode(query):\n    \"\"\"\n    Choose between MMR and similarity search based on query length.\n\n    Args:\n        query (str): The user query.\n\n    Returns:\n        str: \"mmr\" or \"similarity\"\n    \"\"\"\n    return \"mmr\" if len(query.split()) >= 6 else \"similarity\"\n\ndef normalize_scores(results, method_name):\n    \"\"\"\n    Normalize scores within a method using min-max scaling.\n\n    Args:\n        results (List[Tuple[Document, float]]): List of (Document, score) tuples.\n        method_name (str): Name of the retrieval method (for logging).\n\n    Returns:\n        List[Tuple[Document, float]]: Normalized results.\n    \"\"\"\n    if not results or all(s is None for _, s in results):\n        return results\n    \n    scores = [s for _, s in results if s is not None]\n    if not scores:\n        return results\n    \n    min_s, max_s = min(scores), max(scores)\n    if max_s == min_s:\n        return [(doc, 1.0) for doc, _ in results]\n    \n    normalized = []\n    for doc, score in results:\n        if score is not None:\n            norm_score = (score - min_s) / (max_s - min_s)\n            normalized.append((doc, norm_score))\n        else:\n            normalized.append((doc, 0.0))\n    return normalized\n\ndef merge_results(*lists, max_k=6):\n    \"\"\"\n    Merge and deduplicate results from multiple retrieval strategies.\n    Uses content hash for deduplication and sorts by normalized score.\n\n    Args:\n        *lists: Lists of (Document, score) tuples.\n        max_k (int): Max number of results to return.\n\n    Returns:\n        List[Tuple[Document, float]]: Merged, deduplicated, sorted results.\n    \"\"\"\n    seen = set()\n    merged = []\n    for lst in lists:\n        for doc, score in lst:\n            key = hash(doc.page_content.strip())\n            if key not in seen:\n                merged.append((doc, score if score is not None else 0.0))\n                seen.add(key)\n    \n    merged.sort(key=lambda x: x[1], reverse=True)\n    return merged[:max_k]\n\ndef hybrid_retrieve(query, k=6, meta_filter=None):\n    \"\"\"\n    Hybrid retrieval combining semantic, MMR, metadata, BM25, and TF-IDF+FAISS.\n    Normalizes scores per method before merging to ensure fair ranking.\n\n    Args:\n        query (str): The user query.\n        k (int): Number of results to return.\n        meta_filter (dict, optional): Metadata filter.\n\n    Returns:\n        List[Tuple[Document, float]]: Top-k merged results.\n    \"\"\"\n    candidates = []\n\n    if meta_filter:\n        filt_results = filtered_search(query, meta_filter, k=min(4, k))\n        candidates += normalize_scores(filt_results, \"filtered\")\n\n    mode = pick_semantic_mode(query)\n    if mode == \"mmr\":\n        mmr_results = mmr_search(query, k=min(4, k), lambda_mult=0.5)\n        candidates += [(doc, 0.5) for doc, _ in mmr_results]\n    else:\n        sem_results = semantic_search(query, k=min(4, k))\n        candidates += normalize_scores(sem_results, \"semantic\")\n\n    if is_keyword_heavy(query):\n        bm_results = bm25_search(query, k=min(4, k))\n        candidates += normalize_scores(bm_results, \"bm25\")\n        tf_results = tfidf_faiss_search(query, k=min(4, k))\n        candidates += normalize_scores(tf_results, \"tfidf\")\n\n    if is_time_bounded(query):\n        time_results = filtered_search(query, {\"source\": \"release_notes\"}, k=min(3, k))\n        candidates += normalize_scores(time_results, \"time_bounded\")\n\n    merged = merge_results(candidates, max_k=k)\n    \n    if len(merged) < k:\n        fallback = bm25_search(query, k=k)\n        merged = merge_results(merged, normalize_scores(fallback, \"fallback\"), max_k=k)\n    \n    return merged\n\nqueries = [\n    \"How do I enable TLS 1.3?\",\n    \"Resolve OX-AUTH-902 handshake failures\",\n    \"What changed in version 2.3 regarding legacy mode?\",\n    \"Troubleshooting steps for federated login with Azure AD\",\n    \"Only show networking config details from the manual\",\n]\nfor q in queries:\n    print(f\"\\n=== {q} ===\")\n    for doc, score in hybrid_retrieve(q, k=5, meta_filter={\"section\": \"networking\"} if \"networking\" in q.lower() else None):\n        print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Validate\n\nRun the complete pipeline across all retrieval techniques. This cell times each method and prints results for a set of test queries, allowing you to compare performance and quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_queries = [\n    \"Enable TLS 1.3 in configuration\",\n    \"What is KB-7782 about?\",\n    \"Fix CVE-2024-12345 handshake issue\",\n    \"Why does OX-AUTH-902 occur?\",\n    \"Show only manual networking guidance\",\n]\n\ndef time_call(fn, *args, **kwargs):\n    \"\"\"\n    Time a function call and return its output and elapsed time in ms.\n\n    Args:\n        fn (callable): Function to call.\n        *args: Positional arguments.\n        **kwargs: Keyword arguments.\n\n    Returns:\n        Tuple[Any, float]: (Function output, elapsed time in ms)\n    \"\"\"\n    t0 = time.time()\n    out = fn(*args, **kwargs)\n    return out, (time.time() - t0) * 1000\n\nfor q in test_queries:\n    print(f\"\\n\\n### Query: {q}\")\n\n    ss_out, ss_ms = time_call(semantic_search, q, 4)\n    print(f\"\\n-- Similarity Search ({ss_ms:.1f} ms) --\")\n    for doc, score in ss_out:\n        print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")\n\n    mmr_out, mmr_ms = time_call(mmr_search, q, 4, 20, 0.5)\n    print(f\"\\n-- MMR ({mmr_ms:.1f} ms) --\")\n    for doc, _ in mmr_out:\n        print(f\"     | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")\n\n    filt = {\"source\": \"manual\"} if \"manual\" in q.lower() or \"networking\" in q.lower() else None\n    if filt:\n        fs_out, fs_ms = time_call(filtered_search, q, filt, 4)\n        print(f\"\\n-- Metadata-Filtered ({fs_ms:.1f} ms), filter={filt} --\")\n        for doc, score in fs_out:\n            print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")\n\n    bm_out, bm_ms = time_call(bm25_search, q, 4)\n    print(f\"\\n-- BM25 ({bm_ms:.1f} ms) --\")\n    for doc, score in bm_out:\n        print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")\n\n    tf_out, tf_ms = time_call(tfidf_faiss_search, q, 4)\n    print(f\"\\n-- TF-IDF+FAISS ({tf_ms:.1f} ms) --\")\n    for doc, score in tf_out:\n        print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")\n\n    hy_out, hy_ms = time_call(hybrid_retrieve, q, 6, meta_filter={\"section\": \"networking\"} if \"networking\" in q.lower() else None)\n    print(f\"\\n-- Hybrid Router ({hy_ms:.1f} ms) --\")\n    for doc, score in hy_out:\n        print(f\"{score:.3f} | {doc.metadata} | {doc.page_content.splitlines()[0][:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Tuning Guidelines\n\n* **Chunking:** Start with 256 to 400 tokens, 10 to 20% overlap. Reduce overlap if you see duplicates.\n* **MMR:** Lambda 0\\.3 to 0\\.7 is a good range. Lower for exploration, higher for precision.\n* **k:** 3 to 6 is a sweet spot for most prompts. More raises token cost.\n* **BM25/TF\\-IDF:** Keep both. BM25 for short keywords. TF\\-IDF\\+FAISS for fast cosine search on small to medium corpora.\n* **Metadata:** Normalize keys at ingestion. Avoid free\\-form fields that don't filter cleanly.\n\nFor more on crafting robust prompts and ensuring reliable outputs from LLMs in production, check out our [guide to prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-3).\n\n## Integration and Next Steps\n\nExpose a clean retrieval function for downstream use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_context(query: str, k: int = 6, filters: dict = None) -> list:\n    \"\"\"\n    Retrieve top-k context chunks for a given query.\n\n    Args:\n        query (str): User query.\n        k (int): Number of chunks to return.\n        filters (dict, optional): Metadata filters.\n\n    Returns:\n        list: List of Document objects.\n    \"\"\"\n    results = hybrid_retrieve(query, k=k, meta_filter=filters)\n    return [doc for doc, _ in results]\n\ncontext = retrieve_context(\"Enable TLS 1.3\", k=5)\nfor doc in context:\n    print(doc.page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To persist Chroma indexes across sessions, use persist\\_directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chroma_store = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    collection_name=\"orionx_demo\",\n    persist_directory=\"./chroma_db\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To cache embeddings and avoid recomputation, store the embedding model and vectors locally or use a persistent vector store like Qdrant or Weaviate."
      ]
    }
  ],
  "metadata": {
    "title": "RAG Application: 7 Retrieval Tricks to Boost Answer Accuracy",
    "description": "Upgrade your RAG pipeline with proven chunking, MMR, metadata filters, and BM25/TFâ€‘IDF choices to deliver sharper, trustworthy answers consistently today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}