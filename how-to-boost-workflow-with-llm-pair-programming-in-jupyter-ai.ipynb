{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Boost Workflow with LLM Pair Programming in Jupyter AI\n\n**Description:** Install Jupyter AI, configure LLM providers, leverage %ai/%%ai to write Python, debug faster, and accelerate data science notebooks dramatically today.\n\n**ðŸ“– Read the full article:** [How to Boost Workflow with LLM Pair Programming in Jupyter AI](https://blog.thegenairevolution.com/article/how-to-boost-workflow-with-llm-pair-programming-in-jupyter-ai-2)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'll be honest, when I first discovered Jupyter AI, it felt like someone finally understood what I actually needed. You know how it goes, you're deep in a notebook, debugging some pandas transformation, and you have to switch tabs to ChatGPT or copy\\-paste error messages into Claude. It's annoying. Jupyter AI just... stays where you are. It brings LLM\\-powered code generation directly into your notebook cells. No browser tabs, no context switching. You can ask it to write functions, explain that cryptic error you've been staring at for 20 minutes, or clean up that messy code you wrote at 2 AM. This tutorial will walk you through getting it set up, configuring a provider (I use OpenAI mostly, but Anthropic works great too), and actually using those %ai and %%ai magics to generate and debug Python code without leaving your workflow. If you're curious about the tech behind these models, check out [how transformer models power modern LLMs](/article/transformers-demystifying-the-magic-behind-large-language-models-2).\n\n## Prerequisites\n\nOkay, before we jump in, let me tell you what you'll need:\n\n* Python 3\\.8 or later on your machine. Nothing fancy, just a regular install\n* JupyterLab 3\\.x or Jupyter Notebook 7\\.x. And here's the thing, it doesn't work with Google Colab. I tried. Trust me.\n* An API key from at least one provider. OpenAI, Anthropic, Google, Mistral, they all work fine\n* You should know your way around Jupyter notebooks and basic Python. Nothing crazy.\n\n## Install Jupyter AI and Dependencies\n\nSo Jupyter AI plays nice with JupyterLab 3\\.x and Notebook 7\\.x. Here's what you need to run in your terminal to get the magics working, plus some common data science stuff you'll probably want anyway. Oh, and if you're using JupyterLab and want that chat UI (which is actually pretty cool), grab the optional package too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or activate your environment first if needed\n\n# Core magics and helpful packages\npip install --upgrade pip\npip install jupyter-ai-magics python-dotenv pandas matplotlib\n\n# Optional. Install the JupyterLab chat UI extension if you use JupyterLab.\npip install jupyter-ai\n\n# Optional. Install provider SDKs so you can use their latest models.\n# Install only what you plan to use.\npip install openai anthropic google-generativeai mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once that's done, fire up JupyterLab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jupyter lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open up a fresh notebook and let's keep going.\n\n## Configure API Keys Securely\n\nHere's something I learned the hard way. Jupyter AI needs to read your provider API keys from environment variables. Create a .env file in your project folder and drop your keys in there:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .env\nOPENAI_API_KEY=your_openai_key_here\nANTHROPIC_API_KEY=your_anthropic_key_here\nGOOGLE_API_KEY=your_google_key_here\nMISTRAL_API_KEY=your_mistral_key_here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then at the very start of your notebook, run this cell to load them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n_ = load_dotenv()  # Loads variables from .env into the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This way your keys are ready before the Jupyter AI extension even loads. Simple but important.\n\n## Load Jupyter AI Magics\n\nNow let's actually load the extension so we can use those %ai and %%ai magics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext jupyter_ai_magics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick sanity check, let's make sure it's working:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%ai openai/gpt-4o-mini Say hello in one short sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you see a response from the model, you're golden.\n\n## Define a Default Model\n\nThis is just a quality of life thing, but set a default model so you don't have to type it every single time. I usually create a Python variable and then interpolate it into my prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a model you have access to.\n# Examples: \"openai/gpt-4o-mini\", \"anthropic/claude-3-5-sonnet\", \"google/gemini-1.5-pro\", \"mistral/mistral-large\"\nDEFAULT_MODEL = \"openai/gpt-4o-mini\"\nDEFAULT_MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can just use {DEFAULT\\_MODEL} in your prompts. Saves typing, keeps things consistent.\n\n## Generate a Data Cleaning Function\n\nAlright, let's actually do something useful. We'll use the %%ai cell magic to generate a function that cleans up a pandas DataFrame. Remember, the magic has to be the very first line of the cell or it won't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou are a Python expert. Write a function named clean_dataframe(df, inplace=False) that performs these steps:\n- Strip whitespace from column names.\n- Drop exact duplicate rows.\n- Trim leading and trailing whitespace in string columns.\n- Convert obvious numeric-like columns to numeric where safe.\n- Fill missing values in numeric columns with the column median.\n- If inplace is True, modify df in place and return df. Otherwise, return a new cleaned DataFrame.\nReturn only valid Python code for the function definition. Do not include any extra text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy whatever it generates into a new cell and run it. Now you've got that function available in your notebook.\n\n## Refine the Function with Additional Requirements\n\nActually, wait. Let's make that function better. We should add some error handling and maybe support for inplace modifications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou previously wrote clean_dataframe(df, inplace=False).\nRefine it with:\n- Defensive checks for non-DataFrame inputs. Raise a clear TypeError.\n- More careful numeric conversion using errors='ignore'.\n- A parameter columns_to_trim that accepts a list of column names to trim. Default trims all string columns.\n- Docstring with args, returns, and examples.\nReturn only the updated Python function definition. No extra commentary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the updated version and run it to replace what we had before.\n\n## Use Prompt Interpolation for Context\\-Aware Code\n\nThis is where it gets really interesting. And honestly, this feature alone makes Jupyter AI worth it. Prompt interpolation lets you shove live data, error messages, schema info, whatever, directly into your %%ai prompts. The model gets way more context and generates much better code. It's like the difference between asking someone to cook dinner versus showing them what's in your fridge. If you want to understand why this works so well, check out our explainer on the magic of in\\-context learning. For more practical stuff, look at [techniques for prompting reasoning models to get clear, accurate answers](/article/how-to-prompt-reasoning-models-for-clear-accurate-answers-techniques-examples-2).\n\nLet's load some sample data and pass its schema to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\n\n# Create a small, reproducible dataset\nrng = np.random.default_rng(42)\ndf = pd.DataFrame({\n    \"total_bill\": rng.normal(20, 8, 200).round(2),\n    \"tip\": rng.normal(3, 1, 200).round(2),\n    \"size\": rng.integers(1, 6, 200)\n}).clip(lower=0)\n\nschema = df.dtypes.to_string()\nschema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now generate a transformation function using that schema as context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou are given this pandas DataFrame schema:\n{schema}\n\nWrite a function transform_data(df) that:\n- Adds a tip_pct column as tip / total_bill. Handle division by zero safely.\n- Buckets size into small (1-2), medium (3-4), large (5+).\n- Returns a new DataFrame with the new columns.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the function it generates into a new cell and run it to apply your transformation.\n\n## Debug Errors with AI Assistance\n\nLet me show you something that's saved me countless hours. We'll deliberately break something to demonstrate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deliberate typo in the column name to trigger a KeyError\nbad_df = df.copy()\nbad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_billl\"]  # incorrect column name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now feed that traceback to the model for a fix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import traceback\n\ntry:\n    # Re-run to capture the traceback\n    bad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_billl\"]\nexcept Exception:\n    error_trace = traceback.format_exc()\n\nerror_trace[:600]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou are a Python debugging assistant.\nHere is the traceback:\n{error_trace}\n\nGiven this code that caused the error:\nbad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_billl\"]\n\nExplain the root cause in one sentence, then provide a single corrected line of code.\nReturn only the fixed line of Python code without extra text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply whatever fix it suggests and check if it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the correct code. If the model suggested something equivalent, use that suggestion.\nbad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_bill\"]\n\n# Quick validation\nbad_df[\"tip_pct\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If this kind of AI orchestration gets you excited, you might enjoy [building advanced multi\\-agent chatbots in Python notebooks](/article/how-to-build-a-multi-agent-chatbot-with-crewai-chromadb-gradio-4).\n\n## Generate a Plotting Helper\n\nLet's have it write us a reusable plotting function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nWrite a function plot_histogram(df, column, bins=30, title=None, figsize=(6, 4)):\n- Use matplotlib only.\n- Validate inputs and raise a ValueError if column is missing or non-numeric.\n- Show grid lines and a tight layout.\n- Return the matplotlib Axes object.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy that function into a new cell and use it to visualize your data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nax = plot_histogram(df, \"total_bill\", bins=25, title=\"Total Bill\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Generated Code\n\nLook, I've learned not to trust generated code blindly. Always add some basic sanity checks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity checks for clean_dataframe\nimport inspect\nassert \"clean_dataframe\" in globals() and inspect.isfunction(clean_dataframe)\n\ntoy = pd.DataFrame({\"A\": [1, 1, None], \"B\": [\" x \", \" y\", \" z \"]})\nout = clean_dataframe(toy)\nassert isinstance(out, pd.DataFrame)\nassert \"A\" in out.columns and \"B\" in out.columns\nassert out.shape[0] <= toy.shape[0]\nprint(\"clean_dataframe sanity checks passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These simple checks catch the obvious issues and give you more confidence in what the model generated.\n\n## Handle Provider Errors Gracefully\n\nAPI calls fail. Rate limits, expired keys, network issues, it happens. Wrap your magic calls in try\\-except blocks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n\ntry:\n    body = \"Reply with 'ok' if you received this request.\"\n    get_ipython().run_cell_magic(\"ai\", DEFAULT_MODEL, body)\nexcept Exception as e:\n    import logging, time\n    logging.exception(\"AI request failed\")\n    # Simple retry strategy\n    time.sleep(1.5)\n    try:\n        get_ipython().run_cell_magic(\"ai\", DEFAULT_MODEL, body)\n    except Exception as e2:\n        logging.exception(\"Second attempt failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For anything production\\-ish, you'll want proper logging and maybe exponential backoff for retries. But this gets you started.\n\n## Avoid Leaking Sensitive Data\n\nThis is important. When you're interpolating data into prompts, be careful about sensitive information. Redact or truncate columns with PII:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_sample(df, cols_to_redact=None, max_rows=5, truncate=4):\n    \"\"\"\n    Return a safe preview of df for prompts.\n    Redact specified columns and truncate long strings.\n    \"\"\"\n    import pandas as pd\n\n    preview = df.sample(min(len(df), max_rows), random_state=42).copy()\n    if cols_to_redact:\n        for c in cols_to_redact:\n            if c in preview.columns:\n                preview[c] = \"[REDACTED]\"\n    # Truncate long string values\n    def _truncate(x):\n        if isinstance(x, str) and len(x) > truncate:\n            return x[:truncate] + \"...\"\n        return x\n    return preview.applymap(_truncate)\n\n# Example usage\nsafe_preview = safe_sample(df, cols_to_redact=[\"email\", \"ssn\"] if {\"email\", \"ssn\"}.issubset(df.columns) else [], max_rows=5)\nsafe_preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use that safe\\_sample in your prompts instead of the full dataset. Better safe than sorry.\n\n## End\\-to\\-End Runnable Example\n\nHere's a complete workflow you can actually run from start to finish. I use something like this as a template for new projects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment and setup\nfrom dotenv import load_dotenv\n_ = load_dotenv()\n\n%load_ext jupyter_ai_magics\n\nimport pandas as pd\nimport numpy as np\n\n# Choose a model you have access to\nDEFAULT_MODEL = \"openai/gpt-4o-mini\"\n\n# Create a simple dataset\nrng = np.random.default_rng(0)\ndf = pd.DataFrame({\n    \"total_bill\": rng.normal(20, 7, 120).round(2),\n    \"tip\": rng.normal(3, 1, 120).round(2),\n    \"size\": rng.integers(1, 6, 120)\n}).clip(lower=0)\n\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate your cleaning function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nWrite a function clean_dataframe(df, inplace=False) that:\n- Validates df is a pandas DataFrame.\n- Strips whitespace from column names.\n- Drops duplicate rows.\n- Trims whitespace in string columns.\n- Converts numeric-like columns with errors='ignore'.\n- Fills NaNs in numeric columns with the column median.\n- If inplace is True, modify df in place. Otherwise, return a new DataFrame.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy it, run it, make sure it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage after you paste the generated function\ncleaned = clean_dataframe(df)\ncleaned.info()\n\n# Basic checks\nassert not cleaned.isna().sum().sum()\nassert cleaned.shape[0] <= df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nWrite a function plot_histogram(df, column, bins=30, title=None, figsize=(6, 4)):\n- Use matplotlib to plot a histogram of df[column].\n- Validate the column exists and is numeric.\n- Label axes and add a title if provided.\n- Return the Axes object.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy and run that too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nax = plot_histogram(cleaned, \"total_bill\", bins=25, title=\"Total Bill Distribution\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n\nThe thing about %ai and %%ai magics is that your prompts really matter. Bad prompt, bad code. It's that simple. If you want to get better at this, check out [our guide on prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4).\n\nAnd if you're looking to level up beyond just using these tools, if you want to actually build with AI, [our practical roadmap for aspiring GenAI developers](/article/practical-roadmap-for-aspiring-genai-developers) lays out the skills and projects that'll get you there. It's what I wish I had when I started."
      ]
    }
  ],
  "metadata": {
    "title": "How to Boost Workflow with LLM Pair Programming in Jupyter AI",
    "description": "Install Jupyter AI, configure LLM providers, leverage %ai/%%ai to write Python, debug faster, and accelerate data science notebooks dramatically today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}