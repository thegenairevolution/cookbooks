{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Boost Workflow with LLM Pair Programming in Jupyter AI\n\n**Description:** Install Jupyter AI, configure LLM providers, leverage %ai/%%ai to write Python, debug faster, and accelerate data science notebooks dramatically today.\n\n**ðŸ“– Read the full article:** [How to Boost Workflow with LLM Pair Programming in Jupyter AI](https://blog.thegenairevolution.com/article/how-to-boost-workflow-with-llm-pair-programming-in-jupyter-ai)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Jupyter AI is a JupyterLab extension that brings LLM\\-powered code generation and debugging right into your notebook cells. No more switching to a browser or IDE plugin. You can ask an LLM to scaffold functions, explain errors, or refactor code without ever leaving your analysis environment. This tutorial walks you through installing Jupyter AI, configuring a provider, and using %ai and %%ai magics to generate, debug, and refine Python code in a reproducible notebook workflow. If you want a quick primer on the foundations of these models, check out [how transformer models power modern LLMs](/article/transformers-demystifying-the-magic-behind-large-language-models-2).\n\n## Prerequisites\n\nBefore diving in, you'll need:\n\n* Python 3\\.8 or later installed locally\n* JupyterLab 3\\.x or Jupyter Notebook 7\\.x. Unfortunately, Jupyter AI doesn't support Google Colab\n* An API key for at least one supported provider. OpenAI, Anthropic, Google, or Mistral all work\n* Basic familiarity with Jupyter notebooks and Python\n\n## Install Jupyter AI and Dependencies\n\nJupyter AI works with JupyterLab 3\\.x and Notebook 7\\.x. Run this in a terminal to install the magics and common data science dependencies. If you're using JupyterLab and want the chat UI, go ahead and install the optional package too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or activate your environment first if needed\n\n# Core magics and helpful packages\npip install --upgrade pip\npip install jupyter-ai-magics python-dotenv pandas matplotlib\n\n# Optional. Install the JupyterLab chat UI extension if you use JupyterLab.\npip install jupyter-ai\n\n# Optional. Install provider SDKs so you can use their latest models.\n# Install only what you plan to use.\npip install openai anthropic google-generativeai mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After installation, launch JupyterLab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jupyter lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open a new notebook to continue.\n\n## Configure API Keys Securely\n\nJupyter AI reads provider API keys from environment variables. Create a .env file in your project directory and add your keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .env\nOPENAI_API_KEY=your_openai_key_here\nANTHROPIC_API_KEY=your_anthropic_key_here\nGOOGLE_API_KEY=your_google_key_here\nMISTRAL_API_KEY=your_mistral_key_here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the keys at the start of your notebook with this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n_ = load_dotenv()  # Loads variables from .env into the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This makes sure your keys are available before loading the Jupyter AI extension.\n\n## Load Jupyter AI Magics\n\nLoad the Jupyter AI extension to enable %ai and %%ai magics in your notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext jupyter_ai_magics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify the extension is active by running a simple query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%ai openai/gpt-4o-mini Say hello in one short sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If everything's loaded correctly, you'll see a response from the model.\n\n## Define a Default Model\n\nSet a default model identifier so you don't have to repeat it in every magic call. You can create a Python variable and interpolate it in prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a model you have access to.\n# Examples: \"openai/gpt-4o-mini\", \"anthropic/claude-3-5-sonnet\", \"google/gemini-1.5-pro\", \"mistral/mistral-large\"\nDEFAULT_MODEL = \"openai/gpt-4o-mini\"\nDEFAULT_MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can use {DEFAULT\\_MODEL} in your prompts for consistency.\n\n## Generate a Data Cleaning Function\n\nUse the %%ai cell magic to generate a function that cleans a pandas DataFrame. The magic has to be the first line of the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou are a Python expert. Write a function named clean_dataframe(df, inplace=False) that performs these steps:\n- Strip whitespace from column names.\n- Drop exact duplicate rows.\n- Trim leading and trailing whitespace in string columns.\n- Convert obvious numeric-like columns to numeric where safe.\n- Fill missing values in numeric columns with the column median.\n- If inplace is True, modify df in place and return df. Otherwise, return a new cleaned DataFrame.\nReturn only valid Python code for the function definition. Do not include any extra text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the generated function into a new cell and execute it to make it available in your notebook.\n\n## Refine the Function with Additional Requirements\n\nLet's ask the model to add error handling and inplace modification support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou previously wrote clean_dataframe(df, inplace=False).\nRefine it with:\n- Defensive checks for non-DataFrame inputs. Raise a clear TypeError.\n- More careful numeric conversion using errors='ignore'.\n- A parameter columns_to_trim that accepts a list of column names to trim. Default trims all string columns.\n- Docstring with args, returns, and examples.\nReturn only the updated Python function definition. No extra commentary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy and run the updated function to replace the previous version.\n\n## Use Prompt Interpolation for Context\\-Aware Code\n\nHere's where things get interesting. Prompt interpolation lets you embed live data, error traces, or schema details directly into your %%ai prompts. This gives the model much richer context for more accurate code generation. To really understand this technique and its impact on LLM accuracy, take a look at our explainer on the magic of in\\-context learning. For practical examples that build on these ideas, explore [techniques for prompting reasoning models to get clear, accurate answers](/article/how-to-prompt-reasoning-models-for-clear-accurate-answers-techniques-examples-2).\n\nLoad a sample dataset and pass its schema to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\n\n# Create a small, reproducible dataset\nrng = np.random.default_rng(42)\ndf = pd.DataFrame({\n    \"total_bill\": rng.normal(20, 8, 200).round(2),\n    \"tip\": rng.normal(3, 1, 200).round(2),\n    \"size\": rng.integers(1, 6, 200)\n}).clip(lower=0)\n\nschema = df.dtypes.to_string()\nschema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a transformation function using the schema as context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou are given this pandas DataFrame schema:\n{schema}\n\nWrite a function transform_data(df) that:\n- Adds a tip_pct column as tip / total_bill. Handle division by zero safely.\n- Buckets size into small (1-2), medium (3-4), large (5+).\n- Returns a new DataFrame with the new columns.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the generated function into a new cell and run it to apply the transformation.\n\n## Debug Errors with AI Assistance\n\nLet's introduce a deliberate error to demonstrate debugging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deliberate typo in the column name to trigger a KeyError\nbad_df = df.copy()\nbad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_billl\"]  # incorrect column name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pass the traceback to the model for a fix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import traceback\n\ntry:\n    # Re-run to capture the traceback\n    bad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_billl\"]\nexcept Exception:\n    error_trace = traceback.format_exc()\n\nerror_trace[:600]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nYou are a Python debugging assistant.\nHere is the traceback:\n{error_trace}\n\nGiven this code that caused the error:\nbad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_billl\"]\n\nExplain the root cause in one sentence, then provide a single corrected line of code.\nReturn only the fixed line of Python code without extra text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the suggested fix and validate the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the correct code. If the model suggested something equivalent, use that suggestion.\nbad_df[\"tip_pct\"] = bad_df[\"tip\"] / bad_df[\"total_bill\"]\n\n# Quick validation\nbad_df[\"tip_pct\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If this gets you excited about orchestrating more complex AI workflows, you might want to try [building advanced multi\\-agent chatbots in Python notebooks](/article/how-to-build-a-multi-agent-chatbot-with-crewai-chromadb-gradio-4).\n\n## Generate a Plotting Helper\n\nUse the model to scaffold a reusable plotting function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nWrite a function plot_histogram(df, column, bins=30, title=None, figsize=(6, 4)):\n- Use matplotlib only.\n- Validate inputs and raise a ValueError if column is missing or non-numeric.\n- Show grid lines and a tight layout.\n- Return the matplotlib Axes object.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the function into a new cell and run it to visualize the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nax = plot_histogram(df, \"total_bill\", bins=25, title=\"Total Bill\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Generated Code\n\nAfter generating a function, add some minimal sanity checks to ensure correctness:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity checks for clean_dataframe\nimport inspect\nassert \"clean_dataframe\" in globals() and inspect.isfunction(clean_dataframe)\n\ntoy = pd.DataFrame({\"A\": [1, 1, None], \"B\": [\" x \", \" y\", \" z \"]})\nout = clean_dataframe(toy)\nassert isinstance(out, pd.DataFrame)\nassert \"A\" in out.columns and \"B\" in out.columns\nassert out.shape[0] <= toy.shape[0]\nprint(\"clean_dataframe sanity checks passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These checks catch common issues and help you trust the generated code.\n\n## Handle Provider Errors Gracefully\n\nAPI calls can fail due to rate limits or invalid keys. Wrap magic calls in a try\\-except block to handle errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n\ntry:\n    body = \"Reply with 'ok' if you received this request.\"\n    get_ipython().run_cell_magic(\"ai\", DEFAULT_MODEL, body)\nexcept Exception as e:\n    import logging, time\n    logging.exception(\"AI request failed\")\n    # Simple retry strategy\n    time.sleep(1.5)\n    try:\n        get_ipython().run_cell_magic(\"ai\", DEFAULT_MODEL, body)\n    except Exception as e2:\n        logging.exception(\"Second attempt failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For production workflows, you'll want to log errors and retry with exponential backoff.\n\n## Avoid Leaking Sensitive Data\n\nWhen interpolating data into prompts, redact or truncate sensitive columns to prevent PII leakage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_sample(df, cols_to_redact=None, max_rows=5, truncate=4):\n    \"\"\"\n    Return a safe preview of df for prompts.\n    Redact specified columns and truncate long strings.\n    \"\"\"\n    import pandas as pd\n\n    preview = df.sample(min(len(df), max_rows), random_state=42).copy()\n    if cols_to_redact:\n        for c in cols_to_redact:\n            if c in preview.columns:\n                preview[c] = \"[REDACTED]\"\n    # Truncate long string values\n    def _truncate(x):\n        if isinstance(x, str) and len(x) > truncate:\n            return x[:truncate] + \"...\"\n        return x\n    return preview.applymap(_truncate)\n\n# Example usage\nsafe_preview = safe_sample(df, cols_to_redact=[\"email\", \"ssn\"] if {\"email\", \"ssn\"}.issubset(df.columns) else [], max_rows=5)\nsafe_preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use safe\\_sample instead of the full dataset in your prompts.\n\n## End\\-to\\-End Runnable Example\n\nHere's a complete, minimal workflow you can run from top to bottom:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment and setup\nfrom dotenv import load_dotenv\n_ = load_dotenv()\n\n%load_ext jupyter_ai_magics\n\nimport pandas as pd\nimport numpy as np\n\n# Choose a model you have access to\nDEFAULT_MODEL = \"openai/gpt-4o-mini\"\n\n# Create a simple dataset\nrng = np.random.default_rng(0)\ndf = pd.DataFrame({\n    \"total_bill\": rng.normal(20, 7, 120).round(2),\n    \"tip\": rng.normal(3, 1, 120).round(2),\n    \"size\": rng.integers(1, 6, 120)\n}).clip(lower=0)\n\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a cleaning function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nWrite a function clean_dataframe(df, inplace=False) that:\n- Validates df is a pandas DataFrame.\n- Strips whitespace from column names.\n- Drops duplicate rows.\n- Trims whitespace in string columns.\n- Converts numeric-like columns with errors='ignore'.\n- Fills NaNs in numeric columns with the column median.\n- If inplace is True, modify df in place. Otherwise, return a new DataFrame.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the function, run it, and validate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage after you paste the generated function\ncleaned = clean_dataframe(df)\ncleaned.info()\n\n# Basic checks\nassert not cleaned.isna().sum().sum()\nassert cleaned.shape[0] <= df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ai {DEFAULT_MODEL}\nWrite a function plot_histogram(df, column, bins=30, title=None, figsize=(6, 4)):\n- Use matplotlib to plot a histogram of df[column].\n- Validate the column exists and is numeric.\n- Label axes and add a title if provided.\n- Return the Axes object.\nReturn only valid Python code for the function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the function and run it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nax = plot_histogram(cleaned, \"total_bill\", bins=25, title=\"Total Bill Distribution\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n\nWhen using %ai and %%ai magics, the quality of your prompt directly impacts how useful the generated code or explanations will be. For a deeper understanding of designing prompts that yield reliable and accurate outputs, see [our guide on prompt engineering with LLM APIs](/article/prompt-engineering-with-llm-apis-how-to-get-reliable-outputs-4).\n\nIf you want to expand your skills beyond this workflow and become more proficient in AI\\-assisted development, [our practical roadmap for aspiring GenAI developers](/article/practical-roadmap-for-aspiring-genai-developers) outlines the essential skills and projects to accelerate your growth in this field."
      ]
    }
  ],
  "metadata": {
    "title": "How to Boost Workflow with LLM Pair Programming in Jupyter AI",
    "description": "Install Jupyter AI, configure LLM providers, leverage %ai/%%ai to write Python, debug faster, and accelerate data science notebooks dramatically today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}