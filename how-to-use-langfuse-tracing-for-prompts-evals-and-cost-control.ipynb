{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Use Langfuse Tracing for Prompts, Evals, and Cost Control\n\n**Description:** Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.\n\n**ðŸ“– Read the full article:** [How to Use Langfuse Tracing for Prompts, Evals, and Cost Control](https://blog.thegenairevolution.com/article/how-to-use-langfuse-tracing-for-prompts-evals-and-cost-control-3)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you're building LLM-powered applications, you quickly realize you're flying blind without proper observability. You need to see what's happening with model calls, how your prompts are performing, and what users actually think of the responses. I've tried cobbling together solutions with OpenTelemetry and custom logging, and honestly? Total mess. Langfuse just makes sense for LLM apps:\n<ul><li>**Unified model of traces, generations, and prompts** â€“ Unlike trying to stitch together OpenTelemetry with custom logging, Langfuse actually gets LLM concepts. Generations, prompt versions, scores - they're all first-class citizens. No more glue code.\n</li><li>**Prompt version pinning and A/B testing** â€“ Sure, LangSmith offers tracing, but Langfuse's Prompt Library lets you version and A/B test prompts directly through the SDK. This is huge.\n</li><li>**Dataset-driven evaluation workflows** â€“ Here's where it gets interesting. You can run experiments on datasets and log scores per trace. CI-gated quality checks with minimal custom code.\n</li></ul>This guide walks you through instrumenting a QA endpoint with Langfuse. By the end, you'll have:\n<ul><li>A fully traced QA flow\n</li><li>Prompt versioning via the Prompt Library\n</li><li>A/B experiments on two prompt versions\n</li><li>Dataset evaluation with automated scoring\n</li><li>CI gating to prevent regressions\n</li></ul>You'll track p95 latency, tokens per request, user acceptance. Gives you a baseline for quality and lets you roll out improvements without breaking things.\n## Core Concepts for This Use CaseLangfuse organizes observability around four main primitives:\n<ul><li>**Trace** â€“ A single user request (like \"What is the warranty period?\"). Contains metadata and aggregates all nested activity.\n</li><li>**Span** â€“ A logical step within a trace. Use these to measure latency and find bottlenecks.\n</li><li>**Generation** â€“ The actual model call. Langfuse logs everything automatically.\n</li><li>**Score** â€“ Quality metrics attached to traces. Could be user acceptance, groundedness, whatever.\n</li><li>**Prompt (via Prompt Library)** â€“ Versioned templates you fetch by label. Pin versions in production or fetch latest in dev.\n</li></ul>These map directly to what we're building: trace per request, spans for parsing/retrieval, generation for model calls, scores for feedback, prompts by version for experiments.\n## SetupRun this to install dependencies and configure environment variables. If you're in Colab without a `.env` file, just set the keys inline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langfuse openai python-dotenv\n\nimport os\nfrom dotenv import load_dotenv\n\n# Load from .env if present (local), otherwise set inline (Colab)\nload_dotenv()\n\nif \"LANGFUSE_PUBLIC_KEY\" not in os.environ:\n    os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"  # Replace with your key\nif \"LANGFUSE_SECRET_KEY\" not in os.environ:\n    os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"  # Replace with your key\nif \"LANGFUSE_HOST\" not in os.environ:\n    os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # Or your self-hosted URL\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your OpenAI key\n\n# Optional: pin a specific environment and model\nos.environ[\"ENV\"] = os.getenv(\"ENV\", \"dev\")\nos.environ[\"OPENAI_MODEL\"] = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You'll need a Langfuse account (free at <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://langfuse.com\">langfuse.com</a>) and an OpenAI API key. If `gpt-4o-mini` isn't available, swap it for whatever you have.\nNext, ensure the `qa-answer` prompt exists in your Langfuse Prompt Library. Run this once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n\nlangfuse = Langfuse()\n\ntry:\n    # Check if prompt exists\n    langfuse.get_prompt(\"qa-answer\")\n    print(\"Prompt 'qa-answer' already exists.\")\nexcept Exception:\n    # Create and publish a minimal prompt\n    prompt_text = \"\"\"Answer the question using only the context below. Be concise.\n\nContext:\n{{context}}\n\nQuestion: {{question}}\n\nAnswer:\"\"\"\n    \n    langfuse.create_prompt(\n        name=\"qa-answer\",\n        prompt=prompt_text,\n        is_active=True,  # Publish immediately\n    )\n    print(\"Created and published prompt 'qa-answer'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If it exists, this skips creation. Now you can fetch it by label.\n## Using the Tool in Practice### Step 1: Initialize Clients and Configure Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\nfrom openai import OpenAI\nfrom time import perf_counter\n\nlangfuse = Langfuse()\nopenai_client = OpenAI()\n\nENV = os.getenv(\"ENV\", \"dev\")\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define a Retrieval FunctionReplace this stub with your actual retriever. Keep it deterministic for now so tests are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_docs(query: str) -> list[str]:\n    \"\"\"Retrieve documents related to the query.\"\"\"\n    return [f\"Doc about: {query}\", \"Another relevant doc.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Start a Trace with Parsing and Retrieval Spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_trace(user_id: str, question: str):\n    \"\"\"Create a trace and log input parsing + retrieval spans.\"\"\"\n    trace = langfuse.trace(\n        name=\"qa_request\",\n        user_id=user_id,\n        input={\"question\": question},\n        metadata={\"env\": ENV, \"component\": \"qa-service\"},\n    )\n    \n    # Span: parse input\n    parse = trace.span(name=\"parse_input\", input={\"raw\": question})\n    normalized = question.strip()\n    parse.end(output={\"normalized\": normalized})\n    \n    # Span: retrieve documents\n    ret = trace.span(name=\"retrieval\", input={\"query\": normalized})\n    docs = retrieve_docs(normalized)\n    ret.end(output={\"docs_count\": len(docs)})\n    \n    return trace, normalized, docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Generate an Answer and Log the Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(trace, model: str, prompt_text: str):\n    \"\"\"Call the model and log generation details.\"\"\"\n    gen_span = trace.span(name=\"generation.prepare_prompt\", input={\"prompt_preview\": prompt_text[:200]})\n    \n    t0 = perf_counter()\n    response = openai_client.chat.completions.create(\n        model=model,\n        temperature=0.2,\n        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n    )\n    latency_ms = (perf_counter() - t0) * 1000\n    gen_span.end(output={\"latency_ms\": latency_ms})\n    \n    content = response.choices[0].message.content\n    usage = getattr(response, \"usage\", None)\n    \n    generation = trace.generation(\n        name=\"answer_generation\",\n        model=response.model,\n        input=prompt_text,\n        output=content,\n        metadata={\"provider\": \"openai\", \"latency_ms\": latency_ms},\n        usage={\n            \"input_tokens\": getattr(usage, \"prompt_tokens\", None),\n            \"output_tokens\": getattr(usage, \"completion_tokens\", None),\n            \"total_tokens\": getattr(usage, \"total_tokens\", None),\n        },\n    )\n    generation.end()\n    trace.update(output={\"answer\": content})\n    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note on token costs:** If `usage` is missing, token counts will be `None`. For cost estimation, maybe use `tiktoken` or just track latency.\n### Step 5: Record Feedback and Heuristic Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_feedback(trace, accepted: bool, question: str, answer: str, docs: list[str]):\n    \"\"\"Attach user feedback and heuristic scores to the trace.\"\"\"\n    trace.score(name=\"acceptance\", value=1.0 if accepted else 0.0, comment=\"user_feedback\")\n    \n    # Heuristic: does answer cite a document term?\n    hit = any(term.lower() in answer.lower() for term in set(\" \".join(docs).split()) if len(term) > 5)\n    trace.score(name=\"groundedness_heuristic\", value=1.0 if hit else 0.0)\n    \n    # Heuristic: brevity penalty\n    brevity = 1.0 if len(answer) < 800 else 0.0\n    trace.score(name=\"brevity_ok\", value=brevity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Fetch and Compile Prompts from the Prompt Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_compiled_prompt(label: str, variables: dict, pinned_version: str | None = None) -> str:\n    \"\"\"Fetch a prompt by label and version, then compile with variables.\"\"\"\n    if pinned_version:\n        prompt = langfuse.get_prompt(label, version=pinned_version)\n    else:\n        prompt = langfuse.get_prompt(label)  # Latest published\n    return prompt.compile(variables)\n\ndef build_prompt(question: str, docs: list[str]) -> str:\n    \"\"\"Build the QA prompt, optionally pinning a version.\"\"\"\n    version_pin = os.getenv(\"PROMPT_VERSION_PIN\") or None\n    return get_compiled_prompt(\n        \"qa-answer\",\n        {\"question\": question, \"context\": \"\\n\\n\".join(docs)},\n        pinned_version=version_pin,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Add LLM-as-Judge Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def judge_helpfulness(trace, question: str, answer: str):\n    \"\"\"Use an LLM to score answer helpfulness.\"\"\"\n    judge_prompt = f\"Rate helpfulness 0-1 for the answer given the question.\\nQuestion: {question}\\nAnswer: {answer}\\nRespond with a number.\"\n    \n    t0 = perf_counter()\n    judge_resp = openai_client.chat.completions.create(\n        model=DEFAULT_MODEL,\n        temperature=0,\n        messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n    )\n    latency_ms = (perf_counter() - t0) * 1000\n    judge_text = judge_resp.choices[0].message.content.strip()\n    \n    try:\n        score_val = float(judge_text.split()[0])\n    except Exception:\n        score_val = 0.0\n    \n    judge_gen = trace.generation(\n        name=\"judge_helpfulness\",\n        model=judge_resp.model,\n        input=judge_prompt,\n        output=judge_text,\n        metadata={\"provider\": \"openai\", \"latency_ms\": latency_ms},\n        usage={\n            \"input_tokens\": getattr(judge_resp.usage, \"prompt_tokens\", None),\n            \"output_tokens\": getattr(judge_resp.usage, \"completion_tokens\", None),\n        },\n    )\n    judge_gen.end()\n    trace.score(name=\"helpfulness_llm\", value=score_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Combine Steps into a Single Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_and_score(user_id: str, question: str, accept: bool | None = None):\n    \"\"\"Generate an answer, record feedback, and judge helpfulness.\"\"\"\n    trace, normalized, docs = start_trace(user_id, question)\n    prompt_text = build_prompt(normalized, docs)\n    answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n    record_feedback(trace, accepted=bool(accept), question=normalized, answer=answer, docs=docs)\n    judge_helpfulness(trace, question=normalized, answer=answer)\n    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Evaluate### Run the QA FlowLet's execute the flow a few times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(answer_and_score(\"u1\", \"What is the warranty period for the Pro plan?\", accept=True))\nprint(answer_and_score(\"u2\", \"How do I reset my password?\", accept=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Langfuse - you'll see everything organized nicely.\n### A/B Test Two Prompt Versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_ab(user_id: str, question: str, version_a: str, version_b: str):\n    \"\"\"Run an A/B test on two prompt versions.\"\"\"\n    results = []\n    for variant, v in [(\"A\", version_a), (\"B\", version_b)]:\n        trace, normalized, docs = start_trace(user_id, question)\n        trace.update(metadata={\"experiment\": \"qa_prompt_ab:v1\", \"variant\": variant})\n        prompt_text = get_compiled_prompt(\"qa-answer\", {\"question\": normalized, \"context\": \"\\n\\n\".join(docs)}, v)\n        answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n        \n        # Simple auto-score: does answer contain first question keyword?\n        score = 1.0 if normalized.lower().split()[0] in answer.lower() else 0.0\n        trace.score(name=\"keyword_hit\", value=score)\n        results.append((variant, answer, trace))\n    return results\n\n# Run A/B test (replace \"3\" and \"4\" with actual prompt version numbers from your Prompt Library)\nab = run_ab(\"u3\", \"Do you support SSO?\", version_a=\"1\", version_b=\"1\")\nfor variant, ans, _ in ab:\n    print(f\"{variant}: {ans[:120]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** To actually test different versions, create version 2 of `qa-answer` in the Langfuse UI. Then update the version numbers.\n### Evaluate on a DatasetHere's where things get powerful. Run your QA system against a dataset with automatic scoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exact_match(answer: str, ground_truth: str) -> float:\n    \"\"\"Check if answer exactly matches ground truth (case-insensitive).\"\"\"\n    return 1.0 if answer.strip().lower() == ground_truth.strip().lower() else 0.0\n\ndef contains_any(answer: str, ground_truth: str) -> float:\n    \"\"\"Check if answer contains any keyword from ground truth.\"\"\"\n    keywords = ground_truth.lower().split()\n    return 1.0 if any(kw in answer.lower() for kw in keywords) else 0.0\n\ndef evaluate_dataset(dataset: list[dict], version_a: str, version_b: str):\n    \"\"\"Evaluate a dataset using two prompt versions.\"\"\"\n    for item in dataset:\n        for variant, v in [(\"A\", version_a), (\"B\", version_b)]:\n            trace, normalized, docs = start_trace(user_id=item[\"id\"], question=item[\"question\"])\n            trace.update(metadata={\"experiment\": \"qa_dataset_eval:v1\", \"variant\": variant, \"dataset_id\": \"qa_sanity\"})\n            prompt_text = get_compiled_prompt(\"qa-answer\", {\"question\": normalized, \"context\": \"\\n\\n\".join(docs)}, v)\n            answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n            trace.score(name=\"accuracy_exact\", value=exact_match(answer, item[\"ground_truth\"]))\n            trace.score(name=\"relevancy_contains\", value=contains_any(answer, item[\"ground_truth\"]))\n\n# Example dataset\ndataset = [\n    {\"id\": \"q1\", \"question\": \"What is the warranty period?\", \"ground_truth\": \"one year\"},\n    {\"id\": \"q2\", \"question\": \"How do I reset my password?\", \"ground_truth\": \"click forgot password\"},\n]\n\nevaluate_dataset(dataset, version_a=\"1\", version_b=\"1\")\nprint(\"Dataset evaluation complete. Check Langfuse for scores.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetch and Aggregate Scores Locally (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flush pending traces to Langfuse\nlangfuse.flush()\n\n# Note: Fetching traces via API requires additional setup (API client, filtering by metadata).\n# For now, inspect aggregated scores in the Langfuse dashboard under Experiments or Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With trace data in hand, you can baseline quality, A/B test prompts, and roll out changes confidently. The iteration loop becomes tight: diagnose, tweak, re-run, compare. Smoother rollouts, lower p95 latency, controlled token spend.\nActually, if you're interested in diving deeper into RAG systems, check out our guide on <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"/article/44830763/5-essential-steps-to-building-agentic-rag-systems-with-langchain-and-chromadb\">building agentic RAG systems with LangChain and ChromaDB</a>.\n## Production Considerations### Overhead and Sampling StrategyLangfuse batches and flushes asynchronously - minimal latency, usually under 10ms per trace. But for high-throughput production, be smart about sampling:\n<ul><li>**Dev/staging:** Sample everything (`sample_rate=1.0`). Catch all issues here.\n</li><li>**Production:** Sample 5â€“10% (`sample_rate=0.05`) to balance cost and visibility.\n</li><li>**Flush on shutdown:** Always call `langfuse.flush()` before process exit.\n</li></ul>### Error Handling and RetriesWrap model calls in try/except and log errors as spans:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n    response = openai_client.chat.completions.create(...)\nexcept Exception as e:\n    error_span = trace.span(name=\"generation_error\", metadata={\"error\": str(e)})\n    error_span.end()\n    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For transient failures, implement exponential backoff:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\nfor attempt in range(3):\n    try:\n        response = openai_client.chat.completions.create(...)\n        break\n    except Exception as e:\n        trace.event(name=\"retry\", metadata={\"attempt\": attempt, \"error\": str(e)})\n        time.sleep(2 ** attempt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CI Gating with Score ThresholdsPrevent regressions by gating deployments on evaluation scores. Here's a CI snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation and fetch mean acceptance score via Langfuse API\npython evaluate.py --dataset qa_sanity --version $NEW_VERSION\nSCORE=$(curl -X GET \"https://cloud.langfuse.com/api/public/scores?dataset=qa_sanity&version=$NEW_VERSION\" \\\n  -H \"Authorization: Bearer $LANGFUSE_SECRET_KEY\" | jq '.mean_acceptance')\n\nif (( $(echo \"$SCORE < 0.8\" | bc -l) )); then\n  echo \"Score $SCORE below threshold 0.8. Failing build.\"\n  exit 1\nfi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I've found 0.8 works well as a starting point. Adjust based on your needs.\n## ConclusionYou've built a fully instrumented QA endpoint with Langfuse. You can now:\n<ul><li>Track p95 latency, tokens, and user acceptance in real time\n</li><li>Version and A/B test prompts\n</li><li>Evaluate on datasets with automated scoring\n</li><li>Gate CI/CD on quality thresholds\n</li></ul>This scales from prototype to production. Faster iteration, confident shipping, maintained quality. The difference between having this observability and not? Night and day - trust me.\n**Next steps:**\n<ul><li>Explore advanced prompt engineering and version multiple prompts\n</li><li>Integrate with LangChain or LlamaIndex for deeper instrumentation\n</li><li>Set up alerts when latency or acceptance drops\n</li></ul>"
      ]
    }
  ],
  "metadata": {
    "title": "How to Use Langfuse Tracing for Prompts, Evals, and Cost Control",
    "description": "Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}