{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** How to Use Langfuse Tracing for Prompts, Evals, and Cost Control\n\n**Description:** Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.\n\n**ðŸ“– Read the full article:** [How to Use Langfuse Tracing for Prompts, Evals, and Cost Control](https://blog.thegenairevolution.com/article/how-to-use-langfuse-tracing-for-prompts-evals-and-cost-control)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Use Langfuse for This Problem\nWhen you're building LLM-powered applications, you quickly realize you're flying blind without proper observability. You need to see what's happening with model calls, how your prompts are performing, and what users actually think of the responses. I've tried cobbling together solutions with OpenTelemetry and custom logging, and honestly, it's a mess. Langfuse provides a unified workflow that just makes sense for LLM apps:\n\n<ul>\n- **Unified model of traces, generations, and prompts** â€“ Unlike trying to stitch together OpenTelemetry with custom logging or using something like Helicone, Langfuse actually understands LLM concepts natively. Generations, prompt versions, scores - they're all first-class citizens in one platform. No more glue code.\n- **Prompt version pinning and A/B testing** â€“ Sure, LangSmith and Phoenix offer tracing, but Langfuse's Prompt Library lets you version, pin, and A/B test prompts directly through the SDK. This is huge - you can experiment without managing some external config system.\n- **Dataset-driven evaluation workflows** â€“ Here's where it gets really interesting. Langfuse supports running experiments on datasets and logging scores per trace. You can set up CI-gated quality checks and prevent regressions with minimal custom code.\n</ul>\nThis guide walks through instrumenting a question-answering (QA) endpoint end-to-end with Langfuse. We'll capture traces, spans, generations, scores, and prompt versions. By the end, you'll have:\n\n<ul>\n- A fully traced QA flow (input parsing, retrieval, generation, feedback)\n- Prompt versioning via Langfuse's Prompt Library\n- A/B experiments on two prompt versions\n- A dataset evaluation loop with automated scoring\n- A CI gating snippet to prevent regressions\n</ul>\nYou'll be able to track p95 latency, tokens per request, and user acceptance. This gives you a baseline for quality, lets you A/B test changes confidently, and helps you roll out improvements without breaking things.\n\n## Core Concepts for This Use Case\nLangfuse organizes everything around four main primitives. Each one maps to a specific step in your QA workflow:\n\n<ul>\n- **Trace** â€“ This is a single user request (like \"What is the warranty period?\"). It contains metadata (user ID, environment) and aggregates all the nested activity.\n- **Span** â€“ A logical step within a trace (input parsing, retrieval, that sort of thing). Use spans to measure latency and figure out where your bottlenecks are.\n- **Generation** â€“ A model call (your OpenAI completion, for instance). Langfuse logs input, output, model name, token usage, and latency automatically.\n- **Score** â€“ A quality or performance metric attached to a trace. Could be user acceptance, a groundedness heuristic, or LLM-as-judge helpfulness. Scores enable dataset evaluation and A/B comparison.\n- **Prompt (via Prompt Library)** â€“ A versioned, reusable template you fetch by label and version. Pin a version in production or fetch the latest in dev to control rollout and enable A/B tests.\n</ul>\nThese primitives map directly to what we're building: you'll create a trace per request, add spans for parsing and retrieval, log a generation for the model call, attach scores for feedback and heuristics, and fetch prompts by version to run experiments.\n\n## Setup\nRun the following cell to install dependencies and configure environment variables. If you're in Colab without a `.env` file, just set the keys inline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langfuse openai python-dotenv\n\nimport os\nfrom dotenv import load_dotenv\n\n# Load from .env if present (local), otherwise set inline (Colab)\nload_dotenv()\n\nif \"LANGFUSE_PUBLIC_KEY\" not in os.environ:\n    os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"  # Replace with your key\nif \"LANGFUSE_SECRET_KEY\" not in os.environ:\n    os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"  # Replace with your key\nif \"LANGFUSE_HOST\" not in os.environ:\n    os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # Or your self-hosted URL\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your OpenAI key\n\n# Optional: pin a specific environment and model\nos.environ[\"ENV\"] = os.getenv(\"ENV\", \"dev\")\nos.environ[\"OPENAI_MODEL\"] = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You'll need a Langfuse account (free at <a href=\"https://langfuse.com\">langfuse.com</a>) and an OpenAI API key. If `gpt-4o-mini` isn't available for you, just swap in `gpt-3.5-turbo` or whatever model you have access to.\n\nNext, we need to ensure the `qa-answer` prompt exists in your Langfuse Prompt Library. Run this cell once to create and publish it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n\nlangfuse = Langfuse()\n\ntry:\n    # Check if prompt exists\n    langfuse.get_prompt(\"qa-answer\")\n    print(\"Prompt 'qa-answer' already exists.\")\nexcept Exception:\n    # Create and publish a minimal prompt\n    prompt_text = \"\"\"Answer the question using only the context below. Be concise.\n\nContext:\n{{context}}\n\nQuestion: {{question}}\n\nAnswer:\"\"\"\n    \n    langfuse.create_prompt(\n        name=\"qa-answer\",\n        prompt=prompt_text,\n        is_active=True,  # Publish immediately\n    )\n    print(\"Created and published prompt 'qa-answer'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the prompt already exists, this cell will skip creation. Now you can fetch it by label in the tutorial code.\n\n## Using the Tool in Practice\n### Step 1: Initialize Clients and Configure Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\nfrom openai import OpenAI\nfrom time import perf_counter\n\nlangfuse = Langfuse()\nopenai_client = OpenAI()\n\nENV = os.getenv(\"ENV\", \"dev\")\nDEFAULT_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define a Retrieval Function\nReplace this stub with your actual retriever (vector DB, keyword search, whatever you're using). Keep it deterministic for reproducible tests though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_docs(query: str) -> list[str]:\n    \"\"\"Retrieve documents related to the query.\"\"\"\n    return [f\"Doc about: {query}\", \"Another relevant doc.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Start a Trace with Parsing and Retrieval Spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_trace(user_id: str, question: str):\n    \"\"\"Create a trace and log input parsing + retrieval spans.\"\"\"\n    trace = langfuse.trace(\n        name=\"qa_request\",\n        user_id=user_id,\n        input={\"question\": question},\n        metadata={\"env\": ENV, \"component\": \"qa-service\"},\n    )\n    \n    # Span: parse input\n    parse = trace.span(name=\"parse_input\", input={\"raw\": question})\n    normalized = question.strip()\n    parse.end(output={\"normalized\": normalized})\n    \n    # Span: retrieve documents\n    ret = trace.span(name=\"retrieval\", input={\"query\": normalized})\n    docs = retrieve_docs(normalized)\n    ret.end(output={\"docs_count\": len(docs)})\n    \n    return trace, normalized, docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Generate an Answer and Log the Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(trace, model: str, prompt_text: str):\n    \"\"\"Call the model and log generation details.\"\"\"\n    gen_span = trace.span(name=\"generation.prepare_prompt\", input={\"prompt_preview\": prompt_text[:200]})\n    \n    t0 = perf_counter()\n    response = openai_client.chat.completions.create(\n        model=model,\n        temperature=0.2,\n        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n    )\n    latency_ms = (perf_counter() - t0) * 1000\n    gen_span.end(output={\"latency_ms\": latency_ms})\n    \n    content = response.choices[0].message.content\n    usage = getattr(response, \"usage\", None)\n    \n    generation = trace.generation(\n        name=\"answer_generation\",\n        model=response.model,\n        input=prompt_text,\n        output=content,\n        metadata={\"provider\": \"openai\", \"latency_ms\": latency_ms},\n        usage={\n            \"input_tokens\": getattr(usage, \"prompt_tokens\", None),\n            \"output_tokens\": getattr(usage, \"completion_tokens\", None),\n            \"total_tokens\": getattr(usage, \"total_tokens\", None),\n        },\n    )\n    generation.end()\n    trace.update(output={\"answer\": content})\n    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Quick note on token costs:** If `usage` is missing (some providers don't return it), the token counts will be `None`. For cost estimation, you might want to use a fallback tokenizer like `tiktoken`, or just log a warning and track latency only.\n\n### Step 5: Record Feedback and Heuristic Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_feedback(trace, accepted: bool, question: str, answer: str, docs: list[str]):\n    \"\"\"Attach user feedback and heuristic scores to the trace.\"\"\"\n    trace.score(name=\"acceptance\", value=1.0 if accepted else 0.0, comment=\"user_feedback\")\n    \n    # Heuristic: does answer cite a document term?\n    hit = any(term.lower() in answer.lower() for term in set(\" \".join(docs).split()) if len(term) > 5)\n    trace.score(name=\"groundedness_heuristic\", value=1.0 if hit else 0.0)\n    \n    # Heuristic: brevity penalty\n    brevity = 1.0 if len(answer) < 800 else 0.0\n    trace.score(name=\"brevity_ok\", value=brevity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Fetch and Compile Prompts from the Prompt Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_compiled_prompt(label: str, variables: dict, pinned_version: str | None = None) -> str:\n    \"\"\"Fetch a prompt by label and version, then compile with variables.\"\"\"\n    if pinned_version:\n        prompt = langfuse.get_prompt(label, version=pinned_version)\n    else:\n        prompt = langfuse.get_prompt(label)  # Latest published\n    return prompt.compile(variables)\n\ndef build_prompt(question: str, docs: list[str]) -> str:\n    \"\"\"Build the QA prompt, optionally pinning a version.\"\"\"\n    version_pin = os.getenv(\"PROMPT_VERSION_PIN\") or None\n    return get_compiled_prompt(\n        \"qa-answer\",\n        {\"question\": question, \"context\": \"\\n\\n\".join(docs)},\n        pinned_version=version_pin,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Add LLM-as-Judge Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def judge_helpfulness(trace, question: str, answer: str):\n    \"\"\"Use an LLM to score answer helpfulness.\"\"\"\n    judge_prompt = f\"Rate helpfulness 0-1 for the answer given the question.\\nQuestion: {question}\\nAnswer: {answer}\\nRespond with a number.\"\n    \n    t0 = perf_counter()\n    judge_resp = openai_client.chat.completions.create(\n        model=DEFAULT_MODEL,\n        temperature=0,\n        messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n    )\n    latency_ms = (perf_counter() - t0) * 1000\n    judge_text = judge_resp.choices[0].message.content.strip()\n    \n    try:\n        score_val = float(judge_text.split()[0])\n    except Exception:\n        score_val = 0.0\n    \n    judge_gen = trace.generation(\n        name=\"judge_helpfulness\",\n        model=judge_resp.model,\n        input=judge_prompt,\n        output=judge_text,\n        metadata={\"provider\": \"openai\", \"latency_ms\": latency_ms},\n        usage={\n            \"input_tokens\": getattr(judge_resp.usage, \"prompt_tokens\", None),\n            \"output_tokens\": getattr(judge_resp.usage, \"completion_tokens\", None),\n        },\n    )\n    judge_gen.end()\n    trace.score(name=\"helpfulness_llm\", value=score_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Combine Steps into a Single Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_and_score(user_id: str, question: str, accept: bool | None = None):\n    \"\"\"Generate an answer, record feedback, and judge helpfulness.\"\"\"\n    trace, normalized, docs = start_trace(user_id, question)\n    prompt_text = build_prompt(normalized, docs)\n    answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n    record_feedback(trace, accepted=bool(accept), question=normalized, answer=answer, docs=docs)\n    judge_helpfulness(trace, question=normalized, answer=answer)\n    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run and Evaluate\n### Run the QA Flow\nLet's execute the flow a few times and see what we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(answer_and_score(\"u1\", \"What is the warranty period for the Pro plan?\", accept=True))\nprint(answer_and_score(\"u2\", \"How do I reset my password?\", accept=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the Langfuse dashboard to see your traces, spans, generations, and scores. It's pretty satisfying to see everything laid out.\n\n### A/B Test Two Prompt Versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_ab(user_id: str, question: str, version_a: str, version_b: str):\n    \"\"\"Run an A/B test on two prompt versions.\"\"\"\n    results = []\n    for variant, v in [(\"A\", version_a), (\"B\", version_b)]:\n        trace, normalized, docs = start_trace(user_id, question)\n        trace.update(metadata={\"experiment\": \"qa_prompt_ab:v1\", \"variant\": variant})\n        prompt_text = get_compiled_prompt(\"qa-answer\", {\"question\": normalized, \"context\": \"\\n\\n\".join(docs)}, v)\n        answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n        \n        # Simple auto-score: does answer contain first question keyword?\n        score = 1.0 if normalized.lower().split()[0] in answer.lower() else 0.0\n        trace.score(name=\"keyword_hit\", value=score)\n        results.append((variant, answer, trace))\n    return results\n\n# Run A/B test (replace \"3\" and \"4\" with actual prompt version numbers from your Prompt Library)\nab = run_ab(\"u3\", \"Do you support SSO?\", version_a=\"1\", version_b=\"1\")\nfor variant, ans, _ in ab:\n    print(f\"{variant}: {ans[:120]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You'll want to create version 2 of the `qa-answer` prompt in the Langfuse UI. Just edit and publish a new version, then update `version_a` and `version_b` accordingly.\n\n### Evaluate on a Dataset\nHere's where things get really powerful. You can run your entire dataset through different prompt versions and compare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def exact_match(answer: str, ground_truth: str) -> float:\n    \"\"\"Check if answer exactly matches ground truth (case-insensitive).\"\"\"\n    return 1.0 if answer.strip().lower() == ground_truth.strip().lower() else 0.0\n\ndef contains_any(answer: str, ground_truth: str) -> float:\n    \"\"\"Check if answer contains any keyword from ground truth.\"\"\"\n    keywords = ground_truth.lower().split()\n    return 1.0 if any(kw in answer.lower() for kw in keywords) else 0.0\n\ndef evaluate_dataset(dataset: list[dict], version_a: str, version_b: str):\n    \"\"\"Evaluate a dataset using two prompt versions.\"\"\"\n    for item in dataset:\n        for variant, v in [(\"A\", version_a), (\"B\", version_b)]:\n            trace, normalized, docs = start_trace(user_id=item[\"id\"], question=item[\"question\"])\n            trace.update(metadata={\"experiment\": \"qa_dataset_eval:v1\", \"variant\": variant, \"dataset_id\": \"qa_sanity\"})\n            prompt_text = get_compiled_prompt(\"qa-answer\", {\"question\": normalized, \"context\": \"\\n\\n\".join(docs)}, v)\n            answer = generate_answer(trace, DEFAULT_MODEL, prompt_text)\n            trace.score(name=\"accuracy_exact\", value=exact_match(answer, item[\"ground_truth\"]))\n            trace.score(name=\"relevancy_contains\", value=contains_any(answer, item[\"ground_truth\"]))\n\n# Example dataset\ndataset = [\n    {\"id\": \"q1\", \"question\": \"What is the warranty period?\", \"ground_truth\": \"one year\"},\n    {\"id\": \"q2\", \"question\": \"How do I reset my password?\", \"ground_truth\": \"click forgot password\"},\n]\n\nevaluate_dataset(dataset, version_a=\"1\", version_b=\"1\")\nprint(\"Dataset evaluation complete. Check Langfuse for scores.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetch and Aggregate Scores Locally (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flush pending traces to Langfuse\nlangfuse.flush()\n\n# Note: Fetching traces via API requires additional setup (API client, filtering by metadata).\n# For now, inspect aggregated scores in the Langfuse dashboard under Experiments or Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With all this trace data, you can baseline quality and cost, A/B test prompt versions on the same inputs, and roll out changes with confidence. The iteration loop becomes so much tighter: diagnose, tweak, re-run, compare. You'll see smoother rollouts, lower p95 latency, and controlled token spend as you tighten the feedback loop. \n\nActually, if you're interested in diving deeper into building Retrieval-Augmented Generation systems, you might find our guide on <a href=\"/article/44830763/5-essential-steps-to-building-agentic-rag-systems-with-langchain-and-chromadb\">building agentic RAG systems with LangChain and ChromaDB</a> helpful.\n\n## Production Considerations\n### Overhead and Sampling Strategy\nOne thing I've learned is that Langfuse batches trace data and flushes asynchronously, which adds minimal latency (we're talking less than 10ms per trace in most cases). But for high-throughput production services, you need a strategy:\n\n<ul>\n- **Dev/staging:** Sample 100% of traces (`sample_rate=1.0`) to catch everything.\n- **Production:** Sample 5â€“10% (`sample_rate=0.05`) to balance cost and visibility. Adjust based on your traffic volume and budget.\n- **Flush on shutdown:** Always call `langfuse.flush()` before process exit to ensure all traces are sent. I've lost data by forgetting this more times than I'd like to admit.\n</ul>\n### Error Handling and Retries\nWrap your model calls in try/except blocks and log errors as spans:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n    response = openai_client.chat.completions.create(...)\nexcept Exception as e:\n    error_span = trace.span(name=\"generation_error\", metadata={\"error\": str(e)})\n    error_span.end()\n    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And for transient failures (rate limits, timeouts), implement exponential backoff and log retry attempts as events:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\nfor attempt in range(3):\n    try:\n        response = openai_client.chat.completions.create(...)\n        break\n    except Exception as e:\n        trace.event(name=\"retry\", metadata={\"attempt\": attempt, \"error\": str(e)})\n        time.sleep(2 ** attempt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CI Gating with Score Thresholds\nThis is where you can really prevent regressions. Gate your deployments on dataset evaluation scores. Here's an example CI snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation and fetch mean acceptance score via Langfuse API\npython evaluate.py --dataset qa_sanity --version $NEW_VERSION\nSCORE=$(curl -X GET \"https://cloud.langfuse.com/api/public/scores?dataset=qa_sanity&version=$NEW_VERSION\" \\\n  -H \"Authorization: Bearer $LANGFUSE_SECRET_KEY\" | jq '.mean_acceptance')\n\nif (( $(echo \"$SCORE < 0.8\" | bc -l) )); then\n  echo \"Score $SCORE below threshold 0.8. Failing build.\"\n  exit 1\nfi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adjust the thresholds based on your quality requirements. I've found 0.8 is a good starting point, but your mileage may vary.\n\n## Conclusion\nSo there you have it - a fully instrumented QA endpoint with Langfuse, capturing traces, spans, generations, and scores at every step. You can now:\n\n<ul>\n- Track p95 latency, token usage, and user acceptance in real time\n- Version and A/B test prompts via the Prompt Library\n- Evaluate changes on datasets with automated scoring\n- Gate CI/CD pipelines on quality thresholds to prevent regressions\n</ul>\nThis workflow scales from prototyping to production. It enables you to iterate faster, ship with confidence, and maintain quality as your LLM app grows. The visibility you get is honestly game-changing - no more guessing why users are unhappy or where your latency is coming from.\n\n**Next steps:**\n\n<ul>\n- Explore advanced prompt engineering techniques and version multiple prompts for different use cases\n- Integrate Langfuse with LangChain or LlamaIndex for deeper callback instrumentation (we cover this in separate guides)\n- Set up alerts in Langfuse to notify your team when latency or acceptance drops below thresholds\n</ul>"
      ]
    }
  ],
  "metadata": {
    "title": "How to Use Langfuse Tracing for Prompts, Evals, and Cost Control",
    "description": "Instrument calls with Langfuse tracing to see traces/spans, version prompts safely, A/B on datasets, and tie evaluations to p95 latency, token cost, and acceptance-rate improvements.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}