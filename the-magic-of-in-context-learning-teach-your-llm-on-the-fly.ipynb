{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** The Magic of In-Context Learning: Teach Your LLM on the Fly\n\n**Description:** Master in-context learning to boost LLM accuracy and control using zero/one/few-shot prompts, clear examples, and proven prompt engineeringâ€”avoid pitfalls today.\n\n**ðŸ“– Read the full article:** [The Magic of In-Context Learning: Teach Your LLM on the Fly](https://blog.thegenairevolution.com/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-2)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you're starting out with large language models (LLMs), there's this one technique that keeps coming up as both incredibly effective and surprisingly simple: in\\-context learning. It basically lets you \"teach\" an LLM a new task it wasn't specifically trained for by providing explanations and examples right there in your prompt. And here's what really gets me, you don't need to change the model at all.\n\nThis honestly saves you so much time. Way faster and more flexible than fine\\-tuning or pre\\-training an LLM. The reason it works is actually pretty fascinating. Traditional machine learning always needed retraining with new data, but LLMs can use the information and examples you give them in the prompt to \"learn\" new tasks on the spot.\n\nLet me walk you through how this works, along with best practices and where it falls short. If you're serious about working with LLMs, understanding this concept is crucial for getting the most out of these models.\n\n![Uploaded image](/public-objects/user_insert_44830763_1763429816021.png \"Uploaded image\")\n\n## What is In\\-Context Learning?\n\nIn\\-context learning allows LLMs to \"learn\" how to perform a new task using only the information in the prompt, even if they weren't originally trained for that specific task. This dramatically speeds up development compared to what we dealt with before LLMs came along. Traditional models always needed retraining to handle new tasks. They relied entirely on their pre\\-trained data and couldn't adapt from user input alone. But LLMs? They can quickly adapt to new tasks just by looking at examples you give them in the prompt.\n\nThere are three main variations you should know about:\n\n* **Zero\\-shot Learning**: The model tackles a task without any examples, relying completely on its pre\\-trained knowledge to figure out what you want. Even though you're not giving examples, the model still uses the prompt's context to understand the task, which is why we still call it in\\-context learning.\n* **One\\-shot Learning**: You provide just one example to guide the model, using a single input\\-output pair to show the format or reasoning you're looking for.\n* **Few\\-shot Learning**: You give the model a handful of examples, which allows it to generalize and produce output based on the patterns it sees.\n\nNow, in\\-context learning isn't magic. It has limits. If the model still isn't producing what you expect after five examples, it's probably time to consider fine\\-tuning. Adding more examples at that point usually won't help. Also, the model's context window limits how much information you can include, which restricts the number of examples you can actually pass to the model.\n\n## How Does In\\-Context Learning Work?\n\n### What We Understand\n\nThe transformer architecture makes in\\-context learning possible, specifically through its self\\-attention mechanism, which helps models recognize and process patterns in input data. During pretraining, large language models learn statistical relationships between tokens by processing massive amounts of text. After this pretraining phase, something interesting happens. The model gains this emergent ability to generalize and respond to new tasks by recognizing patterns from examples within prompts. This happens immediately, without updating the model's internal weights, which makes it fundamentally different from traditional machine learning approaches.\n\n### What We Don't Fully Understand\n\nBut here's where things get mysterious. Despite understanding the mechanism, some deeper aspects remain puzzling. One key mystery is how models can perform tasks they haven't been explicitly trained for, just by seeing examples in a prompt. This ability to infer what you want and generalize from context without specific training really challenges our conventional understanding of how neural networks work. The model's capacity to handle tasks involving abstraction or reasoning, even with minimal examples, is something we can observe but can't fully explain.\n\nAnother puzzling thing is that emergent abilities like in\\-context learning only start appearing as models grow larger. These capabilities typically surface once models reach a certain size, which raises questions about what's actually going on under the hood. We don't fully understand why these behaviors emerge or how the model decides which parts of the context deserve the most attention.\n\nThe bottom line is this: while we understand how in\\-context learning works in practice, we don't fully grasp why it works so effectively or what deeper cognitive\\-like processes drive it. But honestly, the good news is we don't need to fully understand why it works to take full advantage of what it can do.\n\n## Zero\\-shot, One\\-shot, and Few\\-shot Learning\n\nIn\\-context learning breaks down into three main approaches: zero\\-shot, one\\-shot, and few\\-shot learning. Each one serves a different purpose depending on how much guidance you need to give the model.\n\n### Zero\\-shot Learning\n\nIn zero\\-shot learning, the model needs to perform a task without any examples whatsoever. This approach works great for simpler tasks or when you want to test how well the model can generalize based purely on its pre\\-trained knowledge.\n\nThis works surprisingly well for large models. The more parameters a model has, the more sophisticated its understanding of language becomes. Large models can leverage this advanced understanding to infer and perform many tasks they weren't explicitly trained for. But even the largest models can struggle when you throw highly specific or unusual tasks at them.\n\nSmaller models, on the other hand, often have a hard time with zero\\-shot learning. They're typically limited to tasks that closely align with their training. Whether you need one or more examples really depends on the model's scale and how complex or unique your task is.\n\n**Example Prompt**: Here we ask a simple question to the model and see how it responds on its own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "What is the capital of Italy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Response**: The model will respond based on its general knowledge without needing any additional context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The capital of Italy is Rome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One\\-shot Learning\n\nOne\\-shot learning takes things a step further by including a single example in the prompt. This helps when you want to clarify the response format or provide a small hint to guide the model's approach, while still letting the model do most of the generalization work.\n\n**Example Prompt**: Let's continue with the same example, but this time the answer should follow a specific format: the name of the city, the population, and then the main landmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Answer the following geography question using the format shown in the context.\n\nQuestion: What is the capital of France?\nAnswer: The capital of France is Paris, it has a population of 2.1 million people, and the famous landmarks are the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and Arc de Triomphe.\n\nQuestion: What is the capital of Italy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Response**: You've provided one example, and the model uses that format to complete the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The capital of Italy is Rome, it has a population of 2.8 million people, and the famous landmarks are the Colosseum, Vatican City, Pantheon, and Roman Forum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few\\-shot Learning\n\nIf you're still not getting the output you want, or the model isn't approaching the task the way your example shows, you might need to add more examples. This is where few\\-shot learning comes in. By providing several examples, the model can better understand complex patterns and generalize across different tasks. This is especially helpful for tasks that require understanding specific structures or logic.\n\n**Example Prompt**: Here we'll use a more complex example involving mathematical reasoning with dependencies and intermediate steps, where smaller LLMs would likely struggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Solve the following word problems, showing all steps clearly:\n\nProblem: John has 5 apples. He gives 2 apples to Sarah and buys 3 more. How many apples does John have now?\nSteps:\n1. John starts with 5 apples.\n2. He gives 2 apples to Sarah, so 5 - 2 = 3 apples.\n3. He buys 3 more apples, so 3 + 3 = 6 apples.\nAnswer: 6 apples\n\nProblem: A car travels 60 miles per hour for 2 hours, then 50 miles per hour for another 3 hours. What is the total distance the car traveled?\nSteps:\n1. The car travels 60 miles per hour for 2 hours, so 60 * 2 = 120 miles.\n2. Then, it travels 50 miles per hour for 3 hours, so 50 * 3 = 150 miles.\n3. The total distance is 120 + 150 = 270 miles.\nAnswer: 270 miles\n\nProblem: Maria has 10 pencils. She gives 4 pencils to Tom, then buys 7 more pencils. How many pencils does Maria have now?\nSteps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Response**: With multiple examples, the model should now have a clearer understanding of the task and format, which increases the likelihood of getting accurate and consistent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Steps:\n1. Maria starts with 10 pencils.\n2. She gives 4 pencils to Tom, so 10 - 4 = 6 pencils.\n3. She buys 7 more pencils, so 6 + 7 = 13 pencils.\nAnswer: 13 pencils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for In\\-Context Learning Prompts\n\nCrafting your prompts thoughtfully is essential for getting the best results from in\\-context learning. How you structure examples and present information directly influences what the model outputs. Here are some best practices I've found particularly useful:\n\n### 1\\. Clarity in Examples\n\nThe clearer your examples, the better the model understands the task. Avoid vague or ambiguous input\\-output pairs. Each example should clearly demonstrate the pattern or structure you want the model to follow.\n\nExample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Question: What is the capital of France?\nAnswer: Paris\n\nQuestion: What is the capital of Germany?\nAnswer: Berlin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the model has a clear idea of the pattern. Questions about capitals with direct answers containing only the city name.\n\n### 2\\. Consistency in Format\n\nMaintaining a consistent input\\-output format across your examples helps the model generalize correctly. If the format varies too much, the model might struggle to understand what you're expecting.\n\nExample: If you use this structure for the first few examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Word: fast\nSynonym: quick\n\nWord: intelligent\nSynonym: smart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stick to it. Avoid mixing in other styles or unnecessary changes that could confuse the model, like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Word: fast\nAnswer: quick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keeping the format uniform ensures the model picks up on the correct pattern.\n\n### 3\\. Avoid Overloading with Information\n\nLess is often more when it comes to in\\-context learning. While giving examples is useful, overloading the model with too much information can create confusion or dilute the pattern you want to reinforce. Stick to concise, focused examples that highlight the specific behavior you're looking for. Instead of giving a complex paragraph as an example, break it down into smaller, simpler prompts.\n\n### 4\\. Context Matters\n\nThe context in which you give examples plays a key role in how the model interprets them. Make sure any supporting information in the prompt is relevant and aligns with the examples you provide. Context includes how you frame the question or task, the wording, and even the structure. The model uses all of this to guide its response.\n\nFor instance, if your task is technical, specifying a technical context and using technology\\-related examples gives the model a clearer focus. This helps it understand that you're looking for a technical response.\n\nLLMs are highly sensitive to the input they receive. They don't just look at the specific question but process the entire input as a whole to generate an appropriate response. If the context is unclear, inconsistent, or includes irrelevant details, the model might misunderstand the task completely.\n\n## Limitations of In\\-Context Learning\n\nIn\\-context learning is incredibly useful, offering unprecedented flexibility and adaptability. But it does have its limitations. Understanding where it falls short can help you manage expectations and know when it's time to move on to the next technique, like fine\\-tuning.\n\n### 1\\. Scaling Issues with Complex Tasks\n\nAs tasks become more complex, the effectiveness of in\\-context learning can decrease. The model might struggle to generalize from a limited set of examples, especially when deeper reasoning or intricate logic is involved.\n\nFor instance, if a task requires multi\\-step reasoning or domain\\-specific knowledge, providing a few examples might result in inconsistent or incorrect responses. While the model's ability to generalize is impressive, it can't replace the need for training on a specialized dataset for highly complex tasks.\n\n### 2\\. Limited Memory\n\nLarge language models have a fixed token limit, which means they can only process a certain amount of input in a single prompt. This restricts how much context you can provide through examples. If you need to include many examples for the model to fully grasp a task, you might quickly hit this limit. You'll need to balance the depth of examples with the model's capacity to handle them.\n\nThis token limitation can result in incomplete or misunderstood outputs, especially if the context gets cut off due to length.\n\n### 3\\. Variability in Performance\n\nIn\\-context learning can sometimes produce inconsistent results, particularly when examples are unclear or when the task has multiple valid outputs. Even with well\\-structured prompts, the model's responses can vary slightly due to the probabilistic nature of LLMs. This makes it difficult to rely on in\\-context learning for tasks that require high precision or reproducibility.\n\n### 4\\. Ambiguity in Examples\n\nWhen examples in a prompt are too ambiguous or open to multiple interpretations, the model might misinterpret the task completely. In\\-context learning depends on pattern recognition, so if the pattern is unclear or inconsistent, the model might focus on the wrong part of the input and generate irrelevant or incorrect responses.\n\n### 5\\. Lack of Long\\-term Learning\n\nUnlike fine\\-tuned models, LLMs using in\\-context learning don't retain information or patterns from previous prompts. Each new prompt is treated as a standalone instance. The model won't \"remember\" examples from one session to the next. For tasks that require ongoing learning or long\\-term knowledge retention, in\\-context learning just isn't sufficient.\n\nTo sum it up, while in\\-context learning is a powerful and flexible tool for guiding LLMs, it has limitations. Complex tasks, token limits, performance variability, and ambiguity in examples can all impact the quality of results. Being mindful of these limitations will help you understand when and how to use in\\-context learning effectively.\n\n## Conclusion\n\nIn\\-context learning is a powerful tool in prompt engineering. The ability to \"teach\" a model to perform tasks it wasn't explicitly trained for, directly from the input, is both remarkable and highly practical. It provides a faster, easier, and more flexible way to adapt large language models to handle a wide variety of tasks. And all you need is a few examples within a prompt.\n\nBut it's important to remember that in\\-context learning isn't a one\\-size\\-fits\\-all solution. While it handles many tasks well, it can struggle with complex challenges, limited memory, and inconsistent performance if you don't manage it carefully. By following best practices and being mindful of its limitations, you can make the most of its potential and produce more accurate, useful outputs from your LLM.\n\nIn the end, in\\-context learning represents a significant advancement in how we interact with AI. It's actually amazing that it works as well as it does, offering unparalleled flexibility and control with just a few carefully crafted examples."
      ]
    }
  ],
  "metadata": {
    "title": "The Magic of In-Context Learning: Teach Your LLM on the Fly",
    "description": "Master in-context learning to boost LLM accuracy and control using zero/one/few-shot prompts, clear examples, and proven prompt engineeringâ€”avoid pitfalls today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}