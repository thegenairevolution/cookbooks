{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** The Magic of In-Context Learning: Teach Your LLM on the Fly\n\n**Description:** Master in-context learning to boost LLM accuracy and control using zero/one/few-shot prompts, clear examples, and proven prompt engineeringâ€”avoid pitfalls today.\n\n**ðŸ“– Read the full article:** [The Magic of In-Context Learning: Teach Your LLM on the Fly](https://blog.thegenairevolution.com/article/the-magic-of-in-context-learning-teach-your-llm-on-the-fly-3)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you're starting with LLMs, there's this one technique that keeps popping up everywhere: in\\-context learning. It lets you basically \"teach\" an LLM something new just by giving it examples right in your prompt. No model changes needed. Nothing.\n\nThis saves you ridiculous amounts of time. Way faster than fine\\-tuning. The reason it works? Actually pretty wild. Traditional ML always needed retraining, but LLMs can just look at what you give them and figure out new tasks on the spot.\n\nLet me show you how this actually works, plus where it breaks down. If you're serious about LLMs, you need to get this.\n\n![Uploaded image](/public-objects/user_insert_44830763_1763429816021.png \"Uploaded image\")\n\n## What is In\\-Context Learning?\n\nIn\\-context learning lets LLMs \"learn\" new tasks using only what's in your prompt. Even if they weren't trained for that specific thing. This speeds everything up compared to the old days. Traditional models? They needed retraining for every new task. But LLMs adapt just by looking at your examples.\n\nThree main types to know:\n\n* **Zero\\-shot Learning**: Model tackles the task with zero examples. Just relies on what it already knows. Still counts as in\\-context learning since it uses the prompt's context.\n* **One\\-shot Learning**: You give one example. Single input\\-output pair shows the format you want.\n* **Few\\-shot Learning**: Give a handful of examples. Model generalizes from the patterns.\n\nNow, this isn't magic. Got limits. If five examples don't work, probably time for fine\\-tuning. More examples won't help. Plus the context window limits how much you can actually include.\n\n## How Does In\\-Context Learning Work?\n\n### What We Understand\n\nThe transformer architecture makes this possible. Specifically the self\\-attention mechanism. During pretraining, LLMs learn statistical relationships between tokens from massive text. Then something interesting happens. The model gains this ability to generalize from prompt examples. Immediately. No weight updates. Totally different from traditional ML.\n\n### What We Don't Fully Understand\n\nBut here's where it gets weird. We see it work but can't fully explain why. Models perform tasks they've never been trained for, just from seeing examples. This challenges everything we thought we knew about neural networks. The abstraction and reasoning abilities, even with minimal examples? We can observe it but can't explain it.\n\nAnother puzzle. These abilities only show up in larger models. Once models hit a certain size, boom, emergent capabilities. Why? No idea. We don't know why these behaviors emerge or how the model decides what to focus on.\n\nBottom line: we know how it works in practice. We don't know why it works so well. But honestly? We don't need to understand why to use it effectively.\n\n## Zero\\-shot, One\\-shot, and Few\\-shot Learning\n\nIn\\-context learning breaks into three approaches. Each serves different purposes depending on how much guidance you need.\n\n### Zero\\-shot Learning\n\nThe model performs tasks without any examples. Works great for simple stuff or testing how well the model generalizes from pre\\-training alone.\n\nLarge models handle this surprisingly well. More parameters, better language understanding. They can infer tasks they weren't explicitly trained for. But even huge models struggle with really specific or weird tasks.\n\nSmaller models? They have a hard time. Usually limited to tasks close to their training. Whether you need examples depends on model size and task complexity.\n\n**Example Prompt**: Ask a simple question, see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "What is the capital of Italy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Response**: Model responds based on general knowledge. No extra context needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The capital of Italy is Rome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One\\-shot Learning\n\nOne\\-shot learning adds a single example. Helps clarify response format or gives a hint about approach. Model still does most of the work.\n\n**Example Prompt**: Same example, but now the answer needs specific format: city name, population, main landmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Answer the following geography question using the format shown in the context.\n\nQuestion: What is the capital of France?\nAnswer: The capital of France is Paris, it has a population of 2.1 million people, and the famous landmarks are the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and Arc de Triomphe.\n\nQuestion: What is the capital of Italy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Response**: You provided one example. Model uses that format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The capital of Italy is Rome, it has a population of 2.8 million people, and the famous landmarks are the Colosseum, Vatican City, Pantheon, and Roman Forum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few\\-shot Learning\n\nStill not getting what you want? Model not following your example? Add more. Few\\-shot learning provides several examples so the model understands complex patterns better. Really helpful for tasks needing specific structures or logic.\n\n**Example Prompt**: More complex example with mathematical reasoning, dependencies, intermediate steps. Smaller LLMs would probably fail here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Solve the following word problems, showing all steps clearly:\n\nProblem: John has 5 apples. He gives 2 apples to Sarah and buys 3 more. How many apples does John have now?\nSteps:\n1. John starts with 5 apples.\n2. He gives 2 apples to Sarah, so 5 - 2 = 3 apples.\n3. He buys 3 more apples, so 3 + 3 = 6 apples.\nAnswer: 6 apples\n\nProblem: A car travels 60 miles per hour for 2 hours, then 50 miles per hour for another 3 hours. What is the total distance the car traveled?\nSteps:\n1. The car travels 60 miles per hour for 2 hours, so 60 * 2 = 120 miles.\n2. Then, it travels 50 miles per hour for 3 hours, so 50 * 3 = 150 miles.\n3. The total distance is 120 + 150 = 270 miles.\nAnswer: 270 miles\n\nProblem: Maria has 10 pencils. She gives 4 pencils to Tom, then buys 7 more pencils. How many pencils does Maria have now?\nSteps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Response**: Multiple examples give clearer understanding. Better chance of accurate, consistent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Steps:\n1. Maria starts with 10 pencils.\n2. She gives 4 pencils to Tom, so 10 - 4 = 6 pencils.\n3. She buys 7 more pencils, so 6 + 7 = 13 pencils.\nAnswer: 13 pencils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for In\\-Context Learning Prompts\n\nHow you structure prompts directly affects output. Here's what I've found works:\n\n### 1\\. Clarity in Examples\n\nClear examples, better understanding. Avoid vague input\\-output pairs. Each example should clearly show the pattern you want.\n\nExample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Question: What is the capital of France?\nAnswer: Paris\n\nQuestion: What is the capital of Germany?\nAnswer: Berlin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model gets it. Questions about capitals, answers with just city names.\n\n### 2\\. Consistency in Format\n\nKeep input\\-output format consistent. Too much variation confuses the model.\n\nExample: If you start with this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Word: fast\nSynonym: quick\n\nWord: intelligent\nSynonym: smart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stick with it. Don't mix in other styles like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Word: fast\nAnswer: quick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uniform format helps the model pick up patterns.\n\n### 3\\. Avoid Overloading with Information\n\nLess is more. Too much information creates confusion. Use concise, focused examples highlighting specific behaviors. Break complex paragraphs into smaller prompts.\n\n### 4\\. Context Matters\n\nContext shapes interpretation. Make sure supporting information aligns with examples. This includes framing, wording, structure. The model uses everything to guide responses.\n\nTechnical task? Use technical context and tech examples. Gives clearer focus. Helps the model understand you want technical responses.\n\nLLMs are super sensitive to input. They process everything as a whole. Unclear or inconsistent context? The model might completely misunderstand.\n\n## Limitations of In\\-Context Learning\n\nIn\\-context learning is useful. Really flexible. But it has limits. Knowing where it fails helps manage expectations and tells you when to try something else, like fine\\-tuning.\n\n### 1\\. Scaling Issues with Complex Tasks\n\nComplex tasks reduce effectiveness. Models struggle generalizing from limited examples when deep reasoning is involved.\n\nMulti\\-step reasoning or domain\\-specific knowledge? A few examples might give inconsistent responses. The model's generalization is impressive but can't replace specialized training for really complex stuff.\n\n### 2\\. Limited Memory\n\nFixed token limits mean limited input per prompt. This restricts how much context you can provide. Need many examples? You'll hit the limit fast. Got to balance example depth with model capacity.\n\nToken limits can cause incomplete outputs if context gets cut off.\n\n### 3\\. Variability in Performance\n\nSometimes produces inconsistent results. Especially with unclear examples or multiple valid outputs. Even well\\-structured prompts vary slightly. LLMs are probabilistic. Makes it hard for tasks needing high precision.\n\n### 4\\. Ambiguity in Examples\n\nAmbiguous examples lead to misinterpretation. In\\-context learning depends on pattern recognition. Unclear patterns? Model focuses on wrong parts, generates irrelevant responses.\n\n### 5\\. Lack of Long\\-term Learning\n\nUnlike fine\\-tuned models, LLMs don't retain information between prompts. Each prompt is standalone. Model won't \"remember\" examples from one session to another. For ongoing learning or long\\-term retention, in\\-context learning isn't enough.\n\nTo sum up: in\\-context learning is powerful and flexible. But complex tasks, token limits, performance variability, and ambiguity all impact results. Know these limitations to use it effectively.\n\n## Conclusion\n\nIn\\-context learning is powerful for prompt engineering. Teaching models new tasks directly from input? Remarkable and practical. Faster, easier, more flexible than traditional approaches. Just need a few examples.\n\nBut remember, it's not universal. Handles many tasks well but struggles with complex challenges, limited memory, inconsistent performance if not managed right. Follow best practices, know the limitations, and you'll get better outputs.\n\nIn the end, in\\-context learning represents a huge advancement in AI interaction. Actually amazing it works this well. Unparalleled flexibility with just a few examples."
      ]
    }
  ],
  "metadata": {
    "title": "The Magic of In-Context Learning: Teach Your LLM on the Fly",
    "description": "Master in-context learning to boost LLM accuracy and control using zero/one/few-shot prompts, clear examples, and proven prompt engineeringâ€”avoid pitfalls today.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}