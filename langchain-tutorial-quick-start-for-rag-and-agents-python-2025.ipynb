{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ The GenAI Revolution Cookbook\n\n**Title:** LangChain Tutorial: Quick-Start for RAG and Agents (Python) [2025]\n\n**Description:** Build a production-ready Python RAG and agent fast with LangChainâ€”step-by-step setup, copyable code, GitHub repo, testing, and deployment guidance included.\n\n**ðŸ“– Read the full article:** [LangChain Tutorial: Quick-Start for RAG and Agents (Python) [2025]](https://blog.thegenairevolution.com/article/langchain-tutorial-quick-start-for-rag-and-agents-python-2025)\n\n---\n\n*This jupyter notebook contains executable code examples. Run the cells below to try out the code yourself!*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LangChain is a really practical framework for getting things done with LLMs. It provides a unified interface for document loading, splitting, embedding, retrieval, and chainingâ€”which means you can build a working RAG pipeline without writing low\\-level vector store or LLM client code. The framework integrates with OpenAI, Chroma, and other providers out of the box, so you can focus on tuning retrieval and prompts rather than dealing with plumbing. For builders who want to prototype quickly and iterate on retrieval quality, LangChain's abstractions reduce boilerplate and make it easy to swap components (switching from Chroma to another vector store, for instance) without rewriting your chain logic.\n\n## Core Concepts for This Use Case\n\n**Document ingestion and splitting**: Load text from files or URLs, then split it into chunks that fit your embedding model's context window while preserving semantic boundaries.\n\n**Embeddings and vector storage**: Convert chunks into dense vectors using an embedding model, then persist them in a vector store (Chroma) for fast similarity search.\n\n**Retrieval and prompting**: Query the vector store to fetch relevant chunks, inject them into a prompt template, and pass the grounded prompt to an LLM for an answer.\n\n**Evaluation**: Measure retrieval quality and response accuracy using keyword checks, timing, and manual inspection of retrieved sources.\n\n## Setup\n\nRun this notebook in Google Colab or locally with Python 3\\.9\\+. Install dependencies first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU langchain== langchain-openai langchain-community chromadb requests==2.32.4 langchain-core>=1.0.0,<2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set your OpenAI API key securely. In Colab, use the Secrets panel (key name: OPENAI\\_API\\_KEY) or prompt for it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the key and model access with a quick sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nresp = llm.invoke(\"Respond with the single word: ready\")\nassert isinstance(resp.content, str), \"OpenAI client did not return a string response.\"\nprint(\"OpenAI client OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using LangChain for RAG in Practice\n\n### Ingest and Split Documents\n\nCreate a sample text file and load it with TextLoader. For web pages, use WebBaseLoader instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n\nsample_text = \"\"\"\nLangChain is a framework for developing applications powered by language models.\nIt provides integrations for document loading, splitting, embeddings, vector stores, retrieval, chains, tools, and agents.\nThis file serves as sample content for RAG demonstrations.\n\"\"\"\n\nwith open(\"docs.txt\", \"w\") as f:\n    f.write(sample_text)\n\nloader = TextLoader(\"docs.txt\")\ndocs = loader.load()\nprint(f\"Loaded {len(docs)} document(s). Sample content: {docs[0].page_content[:80]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split documents into chunks using RecursiveCharacterTextSplitter. This splitter respects paragraph and sentence boundaries better than naive character splits. You'll want to adjust chunk\\_size and overlap to balance recall (larger chunks capture more context) and token cost (smaller chunks reduce prompt size). If you encounter unexpected retrieval misses or prompt mismatches, it's worth reviewing [common tokenization pitfalls that can break prompts and RAG](/article/common-tokenization-pitfalls-that-can-break-prompts-and-rag)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\nsplits = splitter.split_documents(docs)\nprint(f\"Split into {len(splits)} chunk(s). First chunk preview:\\n{splits[0].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embed and Persist to Chroma\n\nEmbed the chunks using OpenAI's text\\-embedding\\-3\\-small model and store them in a Chroma vector database. Persist the store to disk so you can reuse it across sessions without re\\-embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\npersist_dir = \"chroma_rag_store\"\n\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory=persist_dir\n)\nvectorstore.persist()\nprint(f\"Vector store created and persisted at '{persist_dir}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid duplicating data on re\\-runs, either delete the persist directory before creating a new store or reopen the existing store and add documents incrementally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reopened = Chroma(\n    embedding_function=embeddings,\n    persist_directory=persist_dir\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Retrieval\n\nCreate a retriever using Maximal Marginal Relevance (MMR) to reduce redundancy in retrieved chunks. Start with k\\=4 (number of chunks returned) and fetch\\_k\\=20 (candidates to re\\-rank). Use search\\_type\\=\"similarity\" for pure cosine similarity if you prefer speed over diversity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\"k\": 4, \"fetch_k\": 20}\n)\nprint(\"Retriever initialized with MMR and k=4.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a Grounded Prompt\n\nWrite a prompt template that instructs the model to answer only from the provided context and to admit when it doesn't know. This reduces hallucination and keeps responses grounded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_template(\"\"\"\nYou are a helpful assistant. Use the context to answer the question. If the answer\nis not in the context, say you do not know.\n\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer concisely.\n\"\"\".strip())\nprint(\"Prompt template created for grounded RAG responses.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assemble the RAG Chain\n\nCombine the retriever, prompt, and LLM into a RetrievalQA chain. This chain fetches relevant chunks, stuffs them into the prompt, and sends the prompt to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\nrag_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": prompt}\n)\nprint(\"RAG chain assembled and ready for queries.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run and Evaluate\n\nTest the RAG pipeline with a sample query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What does LangChain provide for building LLM apps?\"\nresult = rag_chain({\"query\": query})\nprint(\"RAG result:\", result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Measure latency and check for keyword presence to validate retrieval quality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\ndef timed_call(fn, *args, **kwargs):\n    start = time.time()\n    out = fn(*args, **kwargs)\n    return out, time.time() - start\n\ndef contains_any(text, keywords):\n    text_l = text.lower()\n    return any(k.lower() in text_l for k in keywords)\n\neval_cases = [\n    {\n        \"query\": \"What components does LangChain offer?\",\n        \"keywords\": [\"document\", \"embeddings\", \"vector\", \"tools\", \"agents\"]\n    }\n]\n\nfor case in eval_cases:\n    res, dur = timed_call(lambda: rag_chain({\"query\": case[\"query\"]}))\n    answer = res[\"result\"]\n    ok = contains_any(answer, case[\"keywords\"])\n    print(f\"Eval: {ok} for '{case['query']}' in {dur:.2f}s (Result: {answer})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start with small k and compact prompts. Lower temperature for determinism. Use smaller models during iteration and reserve higher\\-end models for production paths that require them. Cache embeddings and responses where possible. For advanced optimization, consider adding [semantic caching with Redis Vector](/article/semantic-caching-with-redis-vector) to cut LLM costs.\n\n## Conclusion\n\nYou've built a working RAG pipeline with LangChain, Chroma, and OpenAI: ingesting documents, splitting them into chunks, embedding and persisting to a vector store, retrieving relevant context, and generating grounded answers. You've also added basic evaluation to measure latency and keyword coverage. This foundation lets you iterate on chunking strategy, retrieval parameters, and prompt design to improve accuracy and cost\\-efficiency. For next steps, explore [building a stateful AI agent with LangGraph](/article/building-a-stateful-ai-agent-with-langgraph) to add multi\\-turn reasoning and tool use on top of your RAG system."
      ]
    }
  ],
  "metadata": {
    "title": "LangChain Tutorial: Quick-Start for RAG and Agents (Python) [2025]",
    "description": "Build a production-ready Python RAG and agent fast with LangChainâ€”step-by-step setup, copyable code, GitHub repo, testing, and deployment guidance included.",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}